{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-01T00:25:29.607631Z",
     "start_time": "2019-11-01T00:25:29.008007Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-01T00:25:29.619425Z",
     "start_time": "2019-11-01T00:25:29.611914Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.nn import init\n",
    "from torch.nn import Parameter\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-01T00:25:34.436694Z",
     "start_time": "2019-11-01T00:25:29.623695Z"
    }
   },
   "outputs": [],
   "source": [
    "from Experiment import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-01T00:25:34.443242Z",
     "start_time": "2019-11-01T00:25:34.438958Z"
    }
   },
   "outputs": [],
   "source": [
    "def one_hot(y):\n",
    "    return np.concatenate([(y==0).reshape(-1, 1), (y==1).reshape(-1, 1)], axis=1).astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-01T00:25:37.499369Z",
     "start_time": "2019-11-01T00:25:34.447059Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-31 20:25:34,451 - utilities - INFO - Loading Stock [JPM]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tianyudu/Documents/UToronto/Course/ECO499/3_Replication_Market_Wisdom/src/PortfolioBasic/stockstats.py:387: FutureWarning: Currently, 'apply' passes the values as ndarrays to the applied function. In the future, this will change to passing it as Series objects. You need to specify 'raw=True' to keep the current behaviour, and you can pass 'raw=False' to silence this warning\n",
      "  lambda x: np.fabs(x - x.mean()).mean())\n"
     ]
    }
   ],
   "source": [
    "item = \"JPM\"\n",
    "price_source = \"quandl\"\n",
    "x_data, y_data = get_data(False, None, price_source, item)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.3, random_state=42)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.25, random_state=42)\n",
    "# y_train, y_test = map(one_hot, (y_train, y_test))\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "x_val = np.reshape(x_val, (x_val.shape[0], x_val.shape[1], 1))\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-01T00:25:37.507107Z",
     "start_time": "2019-11-01T00:25:37.501813Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train = torch.from_numpy(x_train).float()\n",
    "x_val = torch.from_numpy(x_val).float()\n",
    "x_test = torch.from_numpy(x_test).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-01T00:25:37.514511Z",
     "start_time": "2019-11-01T00:25:37.510238Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train = torch.from_numpy(y_train).long()\n",
    "y_val = torch.from_numpy(y_val).long()\n",
    "y_test = torch.from_numpy(y_test).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-01T00:34:25.222539Z",
     "start_time": "2019-11-01T00:34:25.208686Z"
    }
   },
   "outputs": [],
   "source": [
    "class BasicLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(BasicLSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.lstm = nn.LSTMCell(self.input_size, self.hidden_size)  # Ordinary LSTM.\n",
    "#         self.lstm = LSTMCellDropout(self.input_size, self.hidden_size, dropout=lstm_dropout)\n",
    "        self.input_dropout = nn.Dropout(0.5)\n",
    "        self.hidden_dropout = nn.Dropout(0.0)\n",
    "        self.output_dropout = nn.Dropout(0.0)\n",
    "        self.linear = nn.Linear(self.hidden_size, self.output_size)\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, inputs, future=0, y=None):\n",
    "        outputs = []\n",
    "        # reset the state of LSTM\n",
    "        # the state is kept till the end of the sequence\n",
    "        # Input shape: (batch_size, sequence_length, input_size)\n",
    "        batch_size = inputs.size(0)\n",
    "        h_t = torch.zeros(batch_size, self.hidden_size, dtype=torch.float32)\n",
    "        c_t = torch.zeros(batch_size, self.hidden_size, dtype=torch.float32)\n",
    "#         h_t = torch.randn(batch_size, self.hidden_size, dtype=torch.float32)\n",
    "#         c_t = torch.randn(batch_size, self.hidden_size, dtype=torch.float32)\n",
    "        \n",
    "        for i, input_t in enumerate(inputs.chunk(inputs.size(1), dim=1)):\n",
    "            input_t = self.input_dropout(input_t)\n",
    "            # Drop the time dimension.\n",
    "            input_t = input_t.view(input_t.shape[0], input_t.shape[-1])\n",
    "            h_t, c_t = self.lstm(input_t, (h_t, c_t))\n",
    "            h_t = self.hidden_dropout(h_t)\n",
    "            output = self.linear(h_t)\n",
    "            output = self.softmax(output)\n",
    "            outputs += [output]\n",
    "\n",
    "        for i in range(future):\n",
    "            if y is not None and random.random() > 0.5:\n",
    "                output = y[:, [i]]  # teacher forcing\n",
    "            h_t, c_t = self.lstm(output, (h_t, c_t))\n",
    "            output = self.linear(h_t)\n",
    "            outputs += [output]\n",
    "        outputs = torch.stack(outputs, 1).squeeze(2)\n",
    "        final_output = outputs[:, -1, :].view(batch_size, self.output_size)\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-01T00:34:26.076758Z",
     "start_time": "2019-11-01T00:34:26.046927Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "class Optimization:\n",
    "    \"\"\" A helper class to train, test and diagnose the LSTM\"\"\"\n",
    "\n",
    "    def __init__(self, model, loss_fn, optimizer, scheduler):\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.train_losses = []\n",
    "        self.train_accuracy = []\n",
    "        self.val_losses = []\n",
    "        self.val_accuracy = []\n",
    "        self.futures = []\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_batch_data(x, y, batch_size):\n",
    "        if batch_size == -1:\n",
    "            return x, y, 1\n",
    "        for batch, i in enumerate(range(0, len(x) - batch_size, batch_size)):\n",
    "            x_batch = x[i : i + batch_size]\n",
    "            y_batch = y[i : i + batch_size]\n",
    "            yield x_batch, y_batch, batch\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        x_train,\n",
    "        y_train,\n",
    "        x_val=None,\n",
    "        y_val=None,\n",
    "        batch_size=32,\n",
    "        n_epochs=15\n",
    "    ):\n",
    "        seq_len = x_train.shape[1]\n",
    "        for epoch in range(n_epochs):\n",
    "            start_time = time.time()\n",
    "            self.futures = []\n",
    "            current_epoch_accuracy = []\n",
    "            train_loss = 0\n",
    "            for x_batch, y_batch, batch in self.generate_batch_data(x_train, y_train, batch_size):\n",
    "                y_pred = self.model(x_batch)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss = self.loss_fn(y_pred, y_batch)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "                # Accuracy.\n",
    "                bin_y_pred = torch.argmax(y_pred, axis=1)\n",
    "                accuracy = torch.mean((bin_y_pred == y_batch).double())\n",
    "                current_epoch_accuracy.append(accuracy.item())\n",
    "#             self.scheduler.step()\n",
    "            train_loss /= batch\n",
    "            self.train_losses.append(train_loss)\n",
    "            accuracy = np.mean(current_epoch_accuracy)\n",
    "            self.train_accuracy.append(accuracy)\n",
    "            \n",
    "            self._validation(x_val, y_val, batch_size)\n",
    "\n",
    "            elapsed = time.time() - start_time\n",
    "            if x_val is not None:\n",
    "                print(\n",
    "                    \"Epoch {}, Train CE: {:.6f}, Train Acc: {:3f}. Val CE: {:.6f}, Val Acc: {:3f}. Elapsed time: {:.2f}s.\".format(\n",
    "                        epoch + 1,\n",
    "                        train_loss, self.train_accuracy[-1],\n",
    "                        self.val_losses[-1], self.val_accuracy[-1],\n",
    "                        elapsed\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                print(\n",
    "                    \"Epoch {}, Train CE Loss: {:.6f}, Train Accuracy: {:3f}, Elapsed time: {:.2f}s.\".format(\n",
    "                        epoch + 1, train_loss, accuracy, elapsed\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    def _predict(self, x_batch, y_batch, seq_len, do_teacher_forcing):\n",
    "        if do_teacher_forcing:\n",
    "            future = random.randint(1, int(seq_len) / 2)\n",
    "            limit = x_batch.size(1) - future\n",
    "            y_pred = self.model(x_batch[:, :limit], future=future, y=y_batch[:, limit:])\n",
    "        else:\n",
    "            future = 0\n",
    "            y_pred = self.model(x_batch)\n",
    "        self.futures.append(future)\n",
    "        return y_pred\n",
    "\n",
    "    def _validation(self, x_val, y_val, batch_size):\n",
    "        if x_val is None or y_val is None:\n",
    "            return None\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0\n",
    "            batch_accuracy = []\n",
    "            for x_batch, y_batch, batch in self.generate_batch_data(x_val, y_val, batch_size):\n",
    "                y_pred = self.model(x_batch)\n",
    "                loss = self.loss_fn(y_pred, y_batch)\n",
    "                val_loss += loss.item()\n",
    "                bin_y_pred = torch.argmax(y_pred, axis=1)\n",
    "                batch_accuracy.append(torch.mean(\n",
    "                    ((bin_y_pred) == y_batch).double()\n",
    "                ).item())\n",
    "            val_loss /= batch\n",
    "            self.val_losses.append(val_loss)\n",
    "            val_accuracy = np.mean(batch_accuracy)\n",
    "            self.val_accuracy.append(val_accuracy)\n",
    "\n",
    "    def evaluate(self, x_test, y_test, batch_size, future=1):\n",
    "        with torch.no_grad():\n",
    "            test_loss = 0\n",
    "            actual, predicted = [], []\n",
    "            for x_batch, y_batch, batch in self.generate_batch_data(x_test, y_test, batch_size):\n",
    "                y_pred = self.model(x_batch, future=future)\n",
    "                y_pred = (\n",
    "                    y_pred[:, -len(y_batch) :] if y_pred.shape[1] > y_batch.shape[1] else y_pred\n",
    "                )\n",
    "                loss = self.loss_fn(y_pred, y_batch)\n",
    "                test_loss += loss.item()\n",
    "                actual += torch.squeeze(y_batch[:, -1]).data.cpu().numpy().tolist()\n",
    "                predicted += torch.squeeze(y_pred[:, -1]).data.cpu().numpy().tolist()\n",
    "            test_loss /= batch\n",
    "            return actual, predicted, test_loss\n",
    "\n",
    "    def plot_losses(self):\n",
    "        plt.plot(self.train_losses, label=\"Training loss\")\n",
    "        plt.plot(self.val_losses, label=\"Validation loss\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Losses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-01T00:34:49.910191Z",
     "start_time": "2019-11-01T00:34:26.532774Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train CE: 0.734579, Train Acc: 0.566406. Val CE: 0.871133, Val Acc: 0.506250. Elapsed time: 0.33s.\n",
      "Epoch 2, Train CE: 0.730566, Train Acc: 0.568359. Val CE: 0.871950, Val Acc: 0.506250. Elapsed time: 0.21s.\n",
      "Epoch 3, Train CE: 0.730597, Train Acc: 0.568359. Val CE: 0.874720, Val Acc: 0.506250. Elapsed time: 0.21s.\n",
      "Epoch 4, Train CE: 0.728478, Train Acc: 0.568359. Val CE: 0.876759, Val Acc: 0.506250. Elapsed time: 0.20s.\n",
      "Epoch 5, Train CE: 0.728273, Train Acc: 0.568359. Val CE: 0.877691, Val Acc: 0.506250. Elapsed time: 0.21s.\n",
      "Epoch 6, Train CE: 0.727881, Train Acc: 0.568359. Val CE: 0.887904, Val Acc: 0.506250. Elapsed time: 0.20s.\n",
      "Epoch 7, Train CE: 0.727223, Train Acc: 0.568359. Val CE: 0.879892, Val Acc: 0.506250. Elapsed time: 0.24s.\n",
      "Epoch 8, Train CE: 0.729252, Train Acc: 0.568359. Val CE: 0.878636, Val Acc: 0.506250. Elapsed time: 0.20s.\n",
      "Epoch 9, Train CE: 0.725981, Train Acc: 0.568359. Val CE: 0.876315, Val Acc: 0.506250. Elapsed time: 0.20s.\n",
      "Epoch 10, Train CE: 0.730143, Train Acc: 0.568359. Val CE: 0.881450, Val Acc: 0.506250. Elapsed time: 0.19s.\n",
      "Epoch 11, Train CE: 0.724875, Train Acc: 0.568359. Val CE: 0.887393, Val Acc: 0.506250. Elapsed time: 0.22s.\n",
      "Epoch 12, Train CE: 0.730079, Train Acc: 0.568359. Val CE: 0.880789, Val Acc: 0.506250. Elapsed time: 0.20s.\n",
      "Epoch 13, Train CE: 0.727203, Train Acc: 0.568359. Val CE: 0.880216, Val Acc: 0.512500. Elapsed time: 0.21s.\n",
      "Epoch 14, Train CE: 0.728868, Train Acc: 0.568359. Val CE: 0.875363, Val Acc: 0.506250. Elapsed time: 0.20s.\n",
      "Epoch 15, Train CE: 0.726944, Train Acc: 0.568359. Val CE: 0.870131, Val Acc: 0.506250. Elapsed time: 0.22s.\n",
      "Epoch 16, Train CE: 0.729119, Train Acc: 0.568359. Val CE: 0.873982, Val Acc: 0.506250. Elapsed time: 0.29s.\n",
      "Epoch 17, Train CE: 0.727201, Train Acc: 0.568359. Val CE: 0.884906, Val Acc: 0.506250. Elapsed time: 0.23s.\n",
      "Epoch 18, Train CE: 0.727430, Train Acc: 0.568359. Val CE: 0.877092, Val Acc: 0.506250. Elapsed time: 0.31s.\n",
      "Epoch 19, Train CE: 0.728004, Train Acc: 0.568359. Val CE: 0.869264, Val Acc: 0.506250. Elapsed time: 0.34s.\n",
      "Epoch 20, Train CE: 0.725995, Train Acc: 0.568359. Val CE: 0.877958, Val Acc: 0.506250. Elapsed time: 0.23s.\n",
      "Epoch 21, Train CE: 0.726749, Train Acc: 0.570312. Val CE: 0.877985, Val Acc: 0.512500. Elapsed time: 0.26s.\n",
      "Epoch 22, Train CE: 0.726788, Train Acc: 0.566406. Val CE: 0.870330, Val Acc: 0.506250. Elapsed time: 0.22s.\n",
      "Epoch 23, Train CE: 0.724042, Train Acc: 0.566406. Val CE: 0.884052, Val Acc: 0.493750. Elapsed time: 0.24s.\n",
      "Epoch 24, Train CE: 0.718830, Train Acc: 0.574219. Val CE: 0.887335, Val Acc: 0.512500. Elapsed time: 0.22s.\n",
      "Epoch 25, Train CE: 0.725922, Train Acc: 0.562500. Val CE: 0.890232, Val Acc: 0.481250. Elapsed time: 0.26s.\n",
      "Epoch 26, Train CE: 0.727907, Train Acc: 0.550781. Val CE: 0.879019, Val Acc: 0.506250. Elapsed time: 0.23s.\n",
      "Epoch 27, Train CE: 0.728179, Train Acc: 0.562500. Val CE: 0.875586, Val Acc: 0.506250. Elapsed time: 0.26s.\n",
      "Epoch 28, Train CE: 0.721417, Train Acc: 0.568359. Val CE: 0.870882, Val Acc: 0.506250. Elapsed time: 0.31s.\n",
      "Epoch 29, Train CE: 0.722106, Train Acc: 0.568359. Val CE: 0.869500, Val Acc: 0.500000. Elapsed time: 0.25s.\n",
      "Epoch 30, Train CE: 0.721129, Train Acc: 0.566406. Val CE: 0.879283, Val Acc: 0.468750. Elapsed time: 0.24s.\n",
      "Epoch 31, Train CE: 0.722537, Train Acc: 0.568359. Val CE: 0.883538, Val Acc: 0.518750. Elapsed time: 0.23s.\n",
      "Epoch 32, Train CE: 0.722059, Train Acc: 0.560547. Val CE: 0.869668, Val Acc: 0.525000. Elapsed time: 0.25s.\n",
      "Epoch 33, Train CE: 0.725679, Train Acc: 0.564453. Val CE: 0.882068, Val Acc: 0.512500. Elapsed time: 0.22s.\n",
      "Epoch 34, Train CE: 0.722457, Train Acc: 0.589844. Val CE: 0.887055, Val Acc: 0.500000. Elapsed time: 0.24s.\n",
      "Epoch 35, Train CE: 0.718567, Train Acc: 0.562500. Val CE: 0.899136, Val Acc: 0.481250. Elapsed time: 0.22s.\n",
      "Epoch 36, Train CE: 0.719555, Train Acc: 0.576172. Val CE: 0.903419, Val Acc: 0.512500. Elapsed time: 0.23s.\n",
      "Epoch 37, Train CE: 0.725146, Train Acc: 0.566406. Val CE: 0.890185, Val Acc: 0.481250. Elapsed time: 0.22s.\n",
      "Epoch 38, Train CE: 0.720476, Train Acc: 0.568359. Val CE: 0.864109, Val Acc: 0.556250. Elapsed time: 0.20s.\n",
      "Epoch 39, Train CE: 0.725131, Train Acc: 0.564453. Val CE: 0.898591, Val Acc: 0.506250. Elapsed time: 0.21s.\n",
      "Epoch 40, Train CE: 0.712075, Train Acc: 0.560547. Val CE: 0.896317, Val Acc: 0.456250. Elapsed time: 0.20s.\n",
      "Epoch 41, Train CE: 0.726766, Train Acc: 0.525391. Val CE: 0.889816, Val Acc: 0.531250. Elapsed time: 0.22s.\n",
      "Epoch 42, Train CE: 0.719167, Train Acc: 0.572266. Val CE: 0.890875, Val Acc: 0.506250. Elapsed time: 0.20s.\n",
      "Epoch 43, Train CE: 0.717172, Train Acc: 0.572266. Val CE: 0.897996, Val Acc: 0.493750. Elapsed time: 0.20s.\n",
      "Epoch 44, Train CE: 0.718247, Train Acc: 0.560547. Val CE: 0.884581, Val Acc: 0.506250. Elapsed time: 0.19s.\n",
      "Epoch 45, Train CE: 0.720844, Train Acc: 0.566406. Val CE: 0.909270, Val Acc: 0.462500. Elapsed time: 0.21s.\n",
      "Epoch 46, Train CE: 0.721287, Train Acc: 0.568359. Val CE: 0.904352, Val Acc: 0.456250. Elapsed time: 0.19s.\n",
      "Epoch 47, Train CE: 0.724359, Train Acc: 0.566406. Val CE: 0.898728, Val Acc: 0.468750. Elapsed time: 0.22s.\n",
      "Epoch 48, Train CE: 0.720138, Train Acc: 0.574219. Val CE: 0.878569, Val Acc: 0.493750. Elapsed time: 0.19s.\n",
      "Epoch 49, Train CE: 0.713161, Train Acc: 0.574219. Val CE: 0.883408, Val Acc: 0.518750. Elapsed time: 0.21s.\n",
      "Epoch 50, Train CE: 0.722860, Train Acc: 0.558594. Val CE: 0.873476, Val Acc: 0.500000. Elapsed time: 0.19s.\n",
      "Epoch 51, Train CE: 0.716587, Train Acc: 0.570312. Val CE: 0.883827, Val Acc: 0.481250. Elapsed time: 0.23s.\n",
      "Epoch 52, Train CE: 0.717244, Train Acc: 0.568359. Val CE: 0.911300, Val Acc: 0.493750. Elapsed time: 0.19s.\n",
      "Epoch 53, Train CE: 0.719738, Train Acc: 0.576172. Val CE: 0.885211, Val Acc: 0.531250. Elapsed time: 0.21s.\n",
      "Epoch 54, Train CE: 0.710792, Train Acc: 0.580078. Val CE: 0.889162, Val Acc: 0.518750. Elapsed time: 0.21s.\n",
      "Epoch 55, Train CE: 0.713026, Train Acc: 0.582031. Val CE: 0.886359, Val Acc: 0.537500. Elapsed time: 0.28s.\n",
      "Epoch 56, Train CE: 0.707538, Train Acc: 0.583984. Val CE: 0.897492, Val Acc: 0.468750. Elapsed time: 0.22s.\n",
      "Epoch 57, Train CE: 0.708678, Train Acc: 0.597656. Val CE: 0.885716, Val Acc: 0.468750. Elapsed time: 0.20s.\n",
      "Epoch 58, Train CE: 0.718489, Train Acc: 0.574219. Val CE: 0.908901, Val Acc: 0.512500. Elapsed time: 0.21s.\n",
      "Epoch 59, Train CE: 0.719422, Train Acc: 0.564453. Val CE: 0.883128, Val Acc: 0.543750. Elapsed time: 0.21s.\n",
      "Epoch 60, Train CE: 0.724102, Train Acc: 0.560547. Val CE: 0.884193, Val Acc: 0.506250. Elapsed time: 0.20s.\n",
      "Epoch 61, Train CE: 0.709335, Train Acc: 0.580078. Val CE: 0.909283, Val Acc: 0.512500. Elapsed time: 0.22s.\n",
      "Epoch 62, Train CE: 0.710628, Train Acc: 0.583984. Val CE: 0.873665, Val Acc: 0.537500. Elapsed time: 0.21s.\n",
      "Epoch 63, Train CE: 0.711419, Train Acc: 0.574219. Val CE: 0.858681, Val Acc: 0.525000. Elapsed time: 0.20s.\n",
      "Epoch 64, Train CE: 0.701663, Train Acc: 0.611328. Val CE: 0.899075, Val Acc: 0.506250. Elapsed time: 0.26s.\n",
      "Epoch 65, Train CE: 0.714380, Train Acc: 0.582031. Val CE: 0.881461, Val Acc: 0.543750. Elapsed time: 0.20s.\n",
      "Epoch 66, Train CE: 0.721642, Train Acc: 0.574219. Val CE: 0.917984, Val Acc: 0.493750. Elapsed time: 0.23s.\n",
      "Epoch 67, Train CE: 0.736238, Train Acc: 0.587891. Val CE: 0.917431, Val Acc: 0.506250. Elapsed time: 0.20s.\n",
      "Epoch 68, Train CE: 0.725317, Train Acc: 0.570312. Val CE: 0.912360, Val Acc: 0.506250. Elapsed time: 0.24s.\n",
      "Epoch 69, Train CE: 0.731673, Train Acc: 0.533203. Val CE: 0.898559, Val Acc: 0.506250. Elapsed time: 0.21s.\n",
      "Epoch 70, Train CE: 0.722232, Train Acc: 0.566406. Val CE: 0.897187, Val Acc: 0.487500. Elapsed time: 0.22s.\n",
      "Epoch 71, Train CE: 0.718557, Train Acc: 0.574219. Val CE: 0.882811, Val Acc: 0.487500. Elapsed time: 0.21s.\n",
      "Epoch 72, Train CE: 0.719759, Train Acc: 0.564453. Val CE: 0.870459, Val Acc: 0.525000. Elapsed time: 0.23s.\n",
      "Epoch 73, Train CE: 0.715553, Train Acc: 0.566406. Val CE: 0.892890, Val Acc: 0.506250. Elapsed time: 0.32s.\n",
      "Epoch 74, Train CE: 0.713226, Train Acc: 0.580078. Val CE: 0.891102, Val Acc: 0.493750. Elapsed time: 0.22s.\n",
      "Epoch 75, Train CE: 0.715148, Train Acc: 0.574219. Val CE: 0.903919, Val Acc: 0.531250. Elapsed time: 0.27s.\n",
      "Epoch 76, Train CE: 0.704940, Train Acc: 0.582031. Val CE: 0.898168, Val Acc: 0.481250. Elapsed time: 0.22s.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77, Train CE: 0.713532, Train Acc: 0.564453. Val CE: 0.888502, Val Acc: 0.531250. Elapsed time: 0.25s.\n",
      "Epoch 78, Train CE: 0.704304, Train Acc: 0.597656. Val CE: 0.890789, Val Acc: 0.475000. Elapsed time: 0.28s.\n",
      "Epoch 79, Train CE: 0.712227, Train Acc: 0.607422. Val CE: 0.898983, Val Acc: 0.531250. Elapsed time: 0.24s.\n",
      "Epoch 80, Train CE: 0.713435, Train Acc: 0.582031. Val CE: 0.916545, Val Acc: 0.500000. Elapsed time: 0.23s.\n",
      "Epoch 81, Train CE: 0.713127, Train Acc: 0.589844. Val CE: 0.897237, Val Acc: 0.525000. Elapsed time: 0.25s.\n",
      "Epoch 82, Train CE: 0.712249, Train Acc: 0.568359. Val CE: 0.896422, Val Acc: 0.506250. Elapsed time: 0.31s.\n",
      "Epoch 83, Train CE: 0.722042, Train Acc: 0.562500. Val CE: 0.876241, Val Acc: 0.531250. Elapsed time: 0.34s.\n",
      "Epoch 84, Train CE: 0.715487, Train Acc: 0.562500. Val CE: 0.873537, Val Acc: 0.506250. Elapsed time: 0.26s.\n",
      "Epoch 85, Train CE: 0.704958, Train Acc: 0.593750. Val CE: 0.894886, Val Acc: 0.512500. Elapsed time: 0.23s.\n",
      "Epoch 86, Train CE: 0.707435, Train Acc: 0.599609. Val CE: 0.902030, Val Acc: 0.475000. Elapsed time: 0.25s.\n",
      "Epoch 87, Train CE: 0.700825, Train Acc: 0.587891. Val CE: 0.887663, Val Acc: 0.531250. Elapsed time: 0.32s.\n",
      "Epoch 88, Train CE: 0.720874, Train Acc: 0.564453. Val CE: 0.917349, Val Acc: 0.512500. Elapsed time: 0.24s.\n",
      "Epoch 89, Train CE: 0.713444, Train Acc: 0.556641. Val CE: 0.893405, Val Acc: 0.487500. Elapsed time: 0.33s.\n",
      "Epoch 90, Train CE: 0.705411, Train Acc: 0.605469. Val CE: 0.897471, Val Acc: 0.518750. Elapsed time: 0.25s.\n",
      "Epoch 91, Train CE: 0.710573, Train Acc: 0.572266. Val CE: 0.883593, Val Acc: 0.512500. Elapsed time: 0.28s.\n",
      "Epoch 92, Train CE: 0.706017, Train Acc: 0.617188. Val CE: 0.870203, Val Acc: 0.512500. Elapsed time: 0.25s.\n",
      "Epoch 93, Train CE: 0.711798, Train Acc: 0.603516. Val CE: 0.866029, Val Acc: 0.537500. Elapsed time: 0.24s.\n",
      "Epoch 94, Train CE: 0.702074, Train Acc: 0.599609. Val CE: 0.893399, Val Acc: 0.493750. Elapsed time: 0.22s.\n",
      "Epoch 95, Train CE: 0.706324, Train Acc: 0.601562. Val CE: 0.892739, Val Acc: 0.487500. Elapsed time: 0.25s.\n",
      "Epoch 96, Train CE: 0.712461, Train Acc: 0.562500. Val CE: 0.894151, Val Acc: 0.512500. Elapsed time: 0.22s.\n",
      "Epoch 97, Train CE: 0.710211, Train Acc: 0.593750. Val CE: 0.900405, Val Acc: 0.518750. Elapsed time: 0.25s.\n",
      "Epoch 98, Train CE: 0.714749, Train Acc: 0.580078. Val CE: 0.893001, Val Acc: 0.525000. Elapsed time: 0.21s.\n",
      "Epoch 99, Train CE: 0.704950, Train Acc: 0.619141. Val CE: 0.871236, Val Acc: 0.550000. Elapsed time: 0.24s.\n",
      "Epoch 100, Train CE: 0.707103, Train Acc: 0.593750. Val CE: 0.872018, Val Acc: 0.512500. Elapsed time: 0.22s.\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "\n",
    "model = BasicLSTM(input_size=1, hidden_size=32, output_size=2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
    "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.1)\n",
    "optimization = Optimization(model, loss_fn, optimizer, scheduler=None)\n",
    "optimization.train(x_train, y_train, x_val, y_val, n_epochs=100, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 435.59999999999997,
   "position": {
    "height": "40px",
    "left": "952.6px",
    "right": "20px",
    "top": "49px",
    "width": "508.2px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

@article{10.1145/3159652.3159690, 
  author = {Hu, Ziniu and Liu, Weiqing and Bian, Jiang and Liu, Xuanzhe and Liu, Tie-Yan}, 
  title = {{Listening to Chaotic Whispers}}, 
  doi = {10.1145/3159652.3159690}, 
  eprint = {1712.02136}, 
  abstract = {{Stock trend prediction plays a critical role in seeking maximized profit from the stock investment. However, precise trend prediction is very difficult since the highly volatile and non-stationary nature of the stock market. Exploding information on the Internet together with the advancing development of natural language processing and text mining techniques have enabled investors to unveil market trends and volatility from online content. Unfortunately, the quality, trustworthiness, and comprehensiveness of online content related to stock market vary drastically, and a large portion consists of the low-quality news, comments, or even rumors. To address this challenge, we imitate the learning process of human beings facing such chaotic online news, driven by three principles: sequential content dependency, diverse influence, and effective and efficient learning. In this paper, to capture the first two principles, we designed a Hybrid Attention Networks(HAN) to predict the stock trend based on the sequence of recent related news. Moreover, we apply the self-paced learning mechanism to imitate the third principle. Extensive experiments on real-world stock market data demonstrate the effectiveness of our framework. A further simulation illustrates that a straightforward trading strategy based on our proposed framework can significantly increase the annualized return.}}, 
  pages = {261--269}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Hu-2018-Unknown.pdf}, 
  year = {2018}
}
@article{Schmidhuber1997, 
  author = {Hochreiter, Sepp and Schmidhuber, Jürgen}, 
  title = {{Long Short-Term Memory}}, 
  issn = {0899-7667}, 
  doi = {10.1162/neco.1997.9.8.1735}, 
  pmid = {9377276}, 
  abstract = {{Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.}}, 
  pages = {1735--1780}, 
  number = {8}, 
  volume = {9}, 
  journal = {Neural Computation}, 
  year = {1997}
}
@article{Breiman2001, 
  author = {Breiman, Leo}, 
  title = {{Random Forests}}, 
  issn = {0885-6125}, 
  doi = {10.1023/a:1010933404324}, 
  abstract = {{Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund \& R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148–156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.}}, 
  pages = {5--32}, 
  number = {1}, 
  volume = {45}, 
  journal = {Machine Learning}, 
  local-url = {file://localhost/Users/tianyudu/Library/Mobile%20Documents/com~apple~CloudDocs/Downloads/Breiman2001_Article_RandomForests.pdf}, 
  year = {2001}
}
@article{Vladimir1992, 
  author = {Boser, Bernhard E and Guyon, Isabelle M and Vapnik, Vladimir N}, 
  title = {{A training algorithm for optimal margin classifiers}}, 
  journaltitle = {Proceedings of the fifth annual workshop on Computational learning theory}, 
  doi = {10.1145/130385.130401}, 
  abstract = {{A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented. The technique is applicable to a wide variety of the classification functions, including Perceptrons, polynomials, and Radial Basis Functions. The effective number of parameters is adjusted automatically to match the complexity of the problem. The solution is expressed as a linear combination of supporting patterns. These are the subset of training patterns that are closest to the decision boundary. Bounds on the generalization performance based on the leave-one-out method and the VC-dimension are given. Experimental results on optical character recognition problems demonstrate the good generalization obtained when compared with other learning algorithms.}}, 
  pages = {144--152}, 
  local-url = {file://localhost/Users/tianyudu/Library/Mobile%20Documents/com~apple~CloudDocs/Downloads/130385.130401.pdf}, 
  year = {1992}
}
@article{Vladimir1997, 
  author = {Drucker and {Burges, Harris and} and {Kaufman, Christopher JC and} and {Smola, Linda and} and {Vapnik, Alex J and} and Vladimir}, 
  title = {{Support vector regression machines}}, 
  journaltitle = {Advances in neural information processing systems}, 
  pages = {155---161}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Drucker-1997-Unknown.pdf}, 
  year = {1997}
}
@article{Scholkopf2004, 
  author = {Smola, Alex J. and Schölkopf, Bernhard}, 
  title = {{A tutorial on support vector regression}}, 
  issn = {0960-3174}, 
  doi = {10.1023/b:stco.0000035301.49549.88}, 
  abstract = {{In this tutorial we give an overview of the basic ideas underlying Support Vector (SV) machines for function estimation. Furthermore, we include a summary of currently used algorithms for training SV machines, covering both the quadratic (or convex) programming part and advanced methods for dealing with large datasets. Finally, we mention some modifications and extensions that have been applied to the standard SV algorithm, and discuss the aspect of regularization from a SV perspective.}}, 
  pages = {199--222}, 
  number = {3}, 
  volume = {14}, 
  journal = {Statistics and Computing}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Smola-2004-Statistics%20and%20Computing.pdf}, 
  year = {2004}
}
@article{10.1093/biomet/63.1.117, 
  author = {SHIBATA, RITEI}, 
  title = {{Selection of the order of an autoregressive model by Akaike's information criterion}}, 
  issn = {0006-3444}, 
  doi = {10.1093/biomet/63.1.117}, 
  pages = {117--126}, 
  number = {1}, 
  volume = {63}, 
  journal = {Biometrika}, 
  year = {1976}
}
@article{10.1016/j.enpol.2009.05.026, 
  author = {Charles, Amélie and Darné, Olivier}, 
  title = {{The efficiency of the crude oil markets: Evidence from variance ratio tests}}, 
  issn = {0301-4215}, 
  doi = {10.1016/j.enpol.2009.05.026}, 
  abstract = {{This study examines the random walk hypothesis for the crude oil markets, using daily data over the period 1982–2008. The weak-form efficient market hypothesis for two crude oil markets (UK Brent and US West Texas Intermediate) is tested with non-parametric variance ratio tests developed by [Wright J.H., 2000. Alternative variance-ratio tests using ranks and signs. Journal of Business and Economic Statistics, 18, 1–9] and [Belaire-Franch J. and Contreras D., 2004. Ranks and signs-based multiple variance ratio tests. Working paper, Department of Economic Analysis, University of Valencia] as well as the wild-bootstrap variance ratio tests suggested by [Kim, J.H., 2006. Wild bootstrapping variance ratio tests. Economics Letters, 92, 38–43]. We find that the Brent crude oil market is weak-form efficiency while the WTI crude oil market seems to be inefficiency on the 1994–2008 sub-period, suggesting that the deregulation have not improved the efficiency on the WTI crude oil market in the sense of making returns less predictable.}}, 
  pages = {4267--4272}, 
  number = {11}, 
  volume = {37}, 
  journal = {Energy Policy}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Charles-2009-Energy%20Policy.pdf}, 
  year = {2009}
}
@article{10.1016/j.eneco.2009.12.001, 
  author = {Wang, Yudong and Liu, Li}, 
  title = {{Is WTI crude oil market becoming weakly efficient over time?}}, 
  issn = {0140-9883}, 
  doi = {10.1016/j.eneco.2009.12.001}, 
  abstract = {{This paper extends the work in Tabak and Cajueiro [Are the crude oil markets becoming weakly efficient over time, Energy Economics 29 (2007) 28–36] and Alvarez-Ramirez et al. [Short-term predictability of crude oil markets: a detrended fluctuation analysis approach, Energy Economics 30 (2008) 2645–2656]. In this paper, we test for the efficiency of WTI crude oil market through observing the dynamic of local Hurst exponents employing the method of rolling window based on multiscale detrended fluctuation analysis. Empirical results show that short-term, medium-term and long-term behaviors were generally turning into efficient behavior over time. However, in this way, the results also show that the market did not evolve along stable conditions for long times. Multiscale analysis is also implemented based on multifractal detrended fluctuation analysis. We found that the small fluctuations of WTI crude oil market were persistent; however, the large fluctuations had high instability, both in the short- and long-terms. Our discussion is also extended by incorporating arguments from the crude oil market structure for explaining the different correlation dynamics.}}, 
  pages = {987--992}, 
  number = {5}, 
  volume = {32}, 
  journal = {Energy Economics}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Wang-2010-Energy%20Economics.pdf}, 
  year = {2010}
}
@article{Fama1970, 
  author = {Fama, Eugene F.}, 
  title = {{Efficient Capital Markets: A Review of Theory and Empirical Work}}, 
  url = {E.F. Fama Efficient capital markets: a review of theory and empirical work Journal of Finance, 25 (1970), pp. 383-417}, 
  pages = {383---417}, 
  volume = {25}, 
  journal = {The Journal of Finance}, 
  local-url = {file://localhost/Users/tianyudu/Library/Mobile%20Documents/com~apple~CloudDocs/Downloads/2325486.pdf}, 
  year = {1970}
}
@article{Fama1991, 
  author = {Fama, Eugene F.}, 
  title = {{Efficient Capital Markets: II}}, 
  issn = {0022-1082}, 
  doi = {10.1111/j.1540-6261.1991.tb04636.x}, 
  pages = {1575--1617}, 
  number = {5}, 
  volume = {46}, 
  journal = {The Journal of Finance}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/FAMA-1991-The%20Journal%20of%20Finance.pdf}, 
  year = {1991}
}
@article{Jensen1978, 
  author = {Jensen, Michael C}, 
  title = {{Some anomalous evidence regarding market efficiency}}, 
  issn = {0304-405X}, 
  doi = {10.1016/0304-405x(78)90025-9}, 
  pages = {95--101}, 
  number = {2-3}, 
  volume = {6}, 
  journal = {Journal of Financial Economics}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Jensen-1978-Journal%20of%20Financial%20Economics.pdf}, 
  year = {1978}
}
@article{Granger2004, 
  author = {Timmermann, Allan and Granger, Clive W J}, 
  title = {{Efficient market hypothesis and forecasting}}, 
  issn = {0169-2070}, 
  doi = {10.1016/s0169-2070(03)00012-8}, 
  abstract = {{The efficient market hypothesis gives rise to forecasting tests that mirror those adopted when testing the optimality of a forecast in the context of a given information set. However, there are also important differences arising from the fact that market efficiency tests rely on establishing profitable trading opportunities in ‘real time’. Forecasters constantly search for predictable patterns and affect prices when they attempt to exploit trading opportunities. Stable forecasting patterns are therefore unlikely to persist for long periods of time and will self-destruct when discovered by a large number of investors. This gives rise to non-stationarities in the time series of financial returns and complicates both formal tests of market efficiency and the search for successful forecasting approaches.}}, 
  pages = {15--27}, 
  number = {1}, 
  volume = {20}, 
  journal = {International Journal of Forecasting}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Timmermann-2004-International%20Journal%20of%20Forecasting.pdf}, 
  year = {2004}
}
@article{Sephton2016, 
  author = {Mann, Janelle and Sephton, Peter}, 
  title = {{Global relationships across crude oil benchmarks}}, 
  issn = {2405-8513}, 
  doi = {10.1016/j.jcomm.2016.04.002}, 
  abstract = {{This paper empirically examines relationships and dynamics between the price of three crude oil benchmarks, namely the WTI, Brent, and Oman. Threshold cointegration is applied with findings that indicate a long run relationship exists between the pairs (WTI-Brent and WTI-Oman) of spatially separated spot markets. For the WTI-Brent spot price pair, there has not been a reversion to the long run relationship since the reversal in spread between the WTI and Brent. Further, threshold error correction models provide evidence that all three series move to restore the long run relationship in at least one regime for both pairings. Together, these results indicate that there currently is no global benchmark for crude oil. It is recommended that stakeholders such as central banks, traders, and policy makers closely monitor all three spot prices.}}, 
  pages = {1--5}, 
  number = {1}, 
  volume = {2}, 
  journal = {Journal of Commodity Markets}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Mann-2016-Journal%20of%20Commodity%20Markets.pdf}, 
  year = {2016}
}
@article{Nielsen1989, 
  author = {Hecht-Nielsen}, 
  title = {{Theory of the backpropagation neural network}}, 
  doi = {10.1109/ijcnn.1989.118638}, 
  abstract = {{The author presents a survey of the basic theory of the backpropagation neural network architecture covering architectural design, performance measurement, function approximation capability, and learning. The survey includes previously known material, as well as some new results, namely, a formulation of the backpropagation neural network architecture to make it a valid neural network (past formulations violated the locality of processing restriction) and a proof that the backpropagation mean-squared-error function exists and is differentiable. Also included is a theorem showing that any L/sub 2/ function from (0, 1)/sup n/ to R/sup m/ can be implemented to any desired degree of accuracy with a three-layer backpropagation neural network. The author presents a speculative neurophysiological model illustrating how the backpropagation neural network architecture might plausibly be implemented in the mammalian brain for corticocortical learning between nearby regions of the cerebral cortex.<>}}, 
  pages = {593--605 vol.1}, 
  journal = {International 1989 Joint Conference on Neural Networks}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Hecht-Nielsen-1989-International%201989%20Joint%20Conference%20on%20Neural%20Networks.pdf}, 
  year = {1989}
}
@book{Claeskens2008, 
  author = {Claeskens, Gerda and Hjort, Nils Lid}, 
  title = {{Model Selection and Model Averaging}}, 
  isbn = {9780521852258}, 
  url = {https://ideas.repec.org/b/cup/cbooks/9780521852258.html}, 
  abstract = {{Given a data set, you can fit thousands of models at the push of a button, but how do you choose the best? With so many candidate models, overfitting is a real danger. Is the monkey who typed Hamlet actually a good writer? Choosing a model is central to all statistical work with data. We have seen rapid advances in model fitting and in the theoretical understanding of model selection, yet this book is the first to synthesize research and practice from this active field. Model choice criteria are explained, discussed and compared, including the AIC, BIC, DIC and FIC. The uncertainties involved with model selection are tackled, with discussions of frequentist and Bayesian methods; model averaging schemes are presented. Real-data examples are complemented by derivations providing deeper insight into the methodology, and instructive exercises build familiarity with the methods. The companion website features Data sets and R code.}}, 
  series = {Cambridge Books}, 
  publisher = {Cambridge University Press}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Claeskens-2001-Unknown.pdf}, 
  year = {2008}
}
@article{Firedman1997, 
  author = {Friedman, Jerome H.}, 
  title = {{On Bias, Variance, 0/1—Loss, and the Curse-of-Dimensionality}}, 
  issn = {1384-5810}, 
  doi = {10.1023/a:1009778005914}, 
  abstract = {{The classification problem is considered in which an outputvariable y assumes discrete values with respectiveprobabilities that depend upon the simultaneous values of a set of input variablesx = \{x\_1,....,x\_n\}. At issue is how error in the estimates of theseprobabilities affects classification error when the estimates are used ina classification rule. These effects are seen to be somewhat counterintuitive in both their strength and nature. In particular the bias andvariance components of the estimation error combine to influenceclassification in a very different way than with squared error on theprobabilities themselves. Certain types of (very high) bias can becanceled by low variance to produce accurate classification. This candramatically mitigate the effect of the bias associated with some simpleestimators like “naive” Bayes, and the bias induced by thecurse-of-dimensionality on nearest-neighbor procedures. This helps explainwhy such simple methods are often competitive with and sometimes superiorto more sophisticated ones for classification, and why“bagging/aggregating” classifiers can often improveaccuracy. These results also suggest simple modifications to theseprocedures that can (sometimes dramatically) further improve theirclassification performance.}}, 
  pages = {55--77}, 
  number = {1}, 
  volume = {1}, 
  journal = {Data Mining and Knowledge Discovery}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Friedman-1997-Data%20Mining%20and%20Knowledge%20Discovery.pdf}, 
  year = {1997}
}
@article{10.1073/pnas.1915006117, 
  author = {Salganik, Matthew J and Lundberg, Ian and Kindel, Alexander T and Ahearn, Caitlin E and Al-Ghoneim, Khaled and Almaatouq, Abdullah and Altschul, Drew M and Brand, Jennie E and Carnegie, Nicole Bohme and Compton, Ryan James and Datta, Debanjan and Davidson, Thomas and Filippova, Anna and Gilroy, Connor and Goode, Brian J and Jahani, Eaman and Kashyap, Ridhi and Kirchner, Antje and McKay, Stephen and Morgan, Allison C and Pentland, Alex and Polimis, Kivan and Raes, Louis and Rigobon, Daniel E and Roberts, Claudia V and Stanescu, Diana M and Suhara, Yoshihiko and Usmani, Adaner and Wang, Erik H and Adem, Muna and Alhajri, Abdulla and AlShebli, Bedoor and Amin, Redwane and Amos, Ryan B and Argyle, Lisa P and Baer-Bositis, Livia and Büchi, Moritz and Chung, Bo-Ryehn and Eggert, William and Faletto, Gregory and Fan, Zhilin and Freese, Jeremy and Gadgil, Tejomay and Gagné, Josh and Gao, Yue and Halpern-Manners, Andrew and Hashim, Sonia P and Hausen, Sonia and He, Guanhua and Higuera, Kimberly and Hogan, Bernie and Horwitz, Ilana M and Hummel, Lisa M and Jain, Naman and Jin, Kun and Jurgens, David and Kaminski, Patrick and Karapetyan, Areg and Kim, E H and Leizman, Ben and Liu, Naijia and Möser, Malte and Mack, Andrew E and Mahajan, Mayank and Mandell, Noah and Marahrens, Helge and Mercado-Garcia, Diana and Mocz, Viola and Mueller-Gastell, Katariina and Musse, Ahmed and Niu, Qiankun and Nowak, William and Omidvar, Hamidreza and Or, Andrew and Ouyang, Karen and Pinto, Katy M and Porter, Ethan and Porter, Kristin E and Qian, Crystal and Rauf, Tamkinat and Sargsyan, Anahit and Schaffner, Thomas and Schnabel, Landon and Schonfeld, Bryan and Sender, Ben and Tang, Jonathan D and Tsurkov, Emma and Loon, Austin van and Varol, Onur and Wang, Xiafei and Wang, Zhi and Wang, Julia and Wang, Flora and Weissman, Samantha and Whitaker, Kirstie and Wolters, Maria K and Woon, Wei Lee and Wu, James and Wu, Catherine and Yang, Kengran and Yin, Jingwen and Zhao, Bingyu and Zhu, Chenyun and Brooks-Gunn, Jeanne and Engelhardt, Barbara E and Hardt, Moritz and Knox, Dean and Levy, Karen and Narayanan, Arvind and Stewart, Brandon M and Watts, Duncan J and McLanahan, Sara}, 
  title = {{Measuring the predictability of life outcomes with a scientific mass collaboration.}}, 
  issn = {0027-8424}, 
  doi = {10.1073/pnas.1915006117}, 
  pmid = {32229555}, 
  abstract = {{How predictable are life trajectories? We investigated this question with a scientific mass collaboration using the common task method; 160 teams built predictive models for six life outcomes using data from the Fragile Families and Child Wellbeing Study, a high-quality birth cohort study. Despite using a rich dataset and applying machine-learning methods optimized for prediction, the best predictions were not very accurate and were only slightly better than those from a simple benchmark model. Within each outcome, prediction error was strongly associated with the family being predicted and weakly associated with the technique used to generate the prediction. Overall, these results suggest practical limits to the predictability of life outcomes in some settings and illustrate the value of mass collaborations in the social sciences.}}, 
  pages = {201915006}, 
  journal = {Proceedings of the National Academy of Sciences of the United States of America}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Unknown-Unknown-Unknown.pdf}, 
  year = {2020}
}
@misc{CNN, 
  author = {Gan, Nectar}, 
  title = {{China's premier warns local officials not to hide new coronavirus infections}}, 
  journal = {CNN}, 
  url = {https://www.cnn.com/2020/03/25/asia/china-coronavirus-li-keqiang-intl-hnk/index.html}, 
  year = {2020}
}
@misc{WashingtonPost, 
  author = {Fifield, Anna}, 
  title = {{As families tell of pneumonia-like deaths in Wuhan, some wonder if China virus count is too low}}, 
  journal = {The Washington Post}, 
  url = {https://www.washingtonpost.com/world/as-families-tell-of-pneumonia-like-deaths-in-wuhan-some-wonder-if-china-virus-count-is-too-low/2020/01/22/0f50b1e6-3d07-11ea-971f-4ce4f94494b4\_story.html}, 
  year = {2020}
}
@misc{Retures, 
  author = {Cadell, Cate}, 
  title = {{Data suggests virus infections under-reported, exaggerating fatality rate}}, 
  journal = {Reuters}, 
  url = {https://www.reuters.com/article/us-china-health-deaths/data-suggests-virus-infections-under-reported-exaggerating-fatality-rate-idUSKBN1ZZ1AH}, 
  year = {2020}
}
@misc{Forbes, 
  author = {Klebnikov, Sergei}, 
  title = {{Public Health Experts Say Coronavirus Exposure May Be Wider Than China Admits}}, 
  journal = {Forbes}, 
  url = {https://www.forbes.com/sites/sergeiklebnikov/2020/01/22/public-health-experts-say-coronavirus-exposure-may-be-wider-than-china-admits/\#5204bd9972eb}, 
  year = {2020}
}
@article{undefined, 
  author = {Thanh-Tung, Hoang and Tran, Truyen and Venkatesh, Svetha}, 
  title = {{Improving Generalization and Stability of Generative Adversarial Networks}}, 
  eprint = {1902.03984}, 
  abstract = {{Generative Adversarial Networks (GANs) are one of the most popular tools for learning complex high dimensional distributions. However, generalization properties of GANs have not been well understood. In this paper, we analyze the generalization of GANs in practical settings. We show that discriminators trained on discrete datasets with the original GAN loss have poor generalization capability and do not approximate the theoretically optimal discriminator. We propose a zero-centered gradient penalty for improving the generalization of the discriminator by pushing it toward the optimal discriminator. The penalty guarantees the generalization and convergence of GANs. Experiments on synthetic and large scale datasets verify our theoretical analysis.}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Thanh-Tung-2019-Unknown.pdf}, 
  year = {2019}
}
@article{undefined, 
  author = {Zhou, Zhi-Hua and Feng, Ji}, 
  title = {{Deep Forest}}, 
  eprint = {1702.08835}, 
  abstract = {{Current deep learning models are mostly build upon neural networks, i.e., multiple layers of parameterized differentiable nonlinear modules that can be trained by backpropagation. In this paper, we explore the possibility of building deep models based on non-differentiable modules. We conjecture that the mystery behind the success of deep neural networks owes much to three characteristics, i.e., layer-by-layer processing, in-model feature transformation and sufficient model complexity. We propose the gcForest approach, which generates \textbackslashtextit\{deep forest\} holding these characteristics. This is a decision tree ensemble approach, with much less hyper-parameters than deep neural networks, and its model complexity can be automatically determined in a data-dependent way. Experiments show that its performance is quite robust to hyper-parameter settings, such that in most cases, even across different data from different domains, it is able to get excellent performance by using the same default setting. This study opens the door of deep learning based on non-differentiable modules, and exhibits the possibility of constructing deep models without using backpropagation.}}, 
  local-url = {file://localhost/Users/tianyudu/Downloads/1702.08835.pdf}, 
  year = {2017}
}
@article{Eggo2020, 
  author = {Hellewell, Joel and Abbott, Sam and Gimma, Amy and Bosse, Nikos I and Jarvis, Christopher I and Russell, Timothy W and Munday, James D and Kucharski, Adam J and Edmunds, W John and Group, Centre for the Mathematical Modelling of Infectious Diseases COVID-19 Working and Sun, Fiona and Flasche, Stefan and Quilty, Billy J and Davies, Nicholas and Liu, Yang and Clifford, Samuel and Klepac, Petra and Jit, Mark and Diamond, Charlie and Gibbs, Hamish and Zandvoort, Kevin van and Funk, Sebastian and Eggo, Rosalind M}, 
  title = {{Feasibility of controlling COVID-19 outbreaks by isolation of cases and contacts}}, 
  issn = {2214-109X}, 
  doi = {10.1016/s2214-109x(20)30074-7}, 
  abstract = {{Background Isolation of cases and contact tracing is used to control outbreaks of infectious diseases, and has been used for coronavirus disease 2019 (COVID-19). Whether this strategy will achieve control depends on characteristics of both the pathogen and the response. Here we use a mathematical model to assess if isolation and contact tracing are able to control onwards transmission from imported cases of COVID-19. Methods We developed a stochastic transmission model, parameterised to the COVID-19 outbreak. We used the model to quantify the potential effectiveness of contact tracing and isolation of cases at controlling a severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)-like pathogen. We considered scenarios that varied in the number of initial cases, the basic reproduction number (R 0), the delay from symptom onset to isolation, the probability that contacts were traced, the proportion of transmission that occurred before symptom onset, and the proportion of subclinical infections. We assumed isolation prevented all further transmission in the model. Outbreaks were deemed controlled if transmission ended within 12 weeks or before 5000 cases in total. We measured the success of controlling outbreaks using isolation and contact tracing, and quantified the weekly maximum number of cases traced to measure feasibility of public health effort. Findings Simulated outbreaks starting with five initial cases, an R 0 of 1·5, and 0\% transmission before symptom onset could be controlled even with low contact tracing probability; however, the probability of controlling an outbreak decreased with the number of initial cases, when R 0 was 2·5 or 3·5 and with more transmission before symptom onset. Across different initial numbers of cases, the majority of scenarios with an R 0 of 1·5 were controllable with less than 50\% of contacts successfully traced. To control the majority of outbreaks, for R 0 of 2·5 more than 70\% of contacts had to be traced, and for an R 0 of 3·5 more than 90\% of contacts had to be traced. The delay between symptom onset and isolation had the largest role in determining whether an outbreak was controllable when R 0 was 1·5. For R 0 values of 2·5 or 3·5, if there were 40 initial cases, contact tracing and isolation were only potentially feasible when less than 1\% of transmission occurred before symptom onset. Interpretation In most scenarios, highly effective contact tracing and case isolation is enough to control a new outbreak of COVID-19 within 3 months. The probability of control decreases with long delays from symptom onset to isolation, fewer cases ascertained by contact tracing, and increasing transmission before symptoms. This model can be modified to reflect updated transmission characteristics and more specific definitions of outbreak control to assess the potential success of local response efforts. Funding Wellcome Trust, Global Challenges Research Fund, and Health Data Research UK.}}, 
  pages = {e488--e496}, 
  number = {4}, 
  volume = {8}, 
  journal = {The Lancet Global Health}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Hellewell-2020-The%20Lancet%20Global%20Health.pdf}, 
  year = {2020}
}
@article{10.1515/cogl.1993.4.1.1, 
  author = {LANGACKER, RONALD W}, 
  title = {{Reference-point constructions}}, 
  issn = {0936-5907}, 
  doi = {10.1515/cogl.1993.4.1.1}, 
  pages = {1--38}, 
  number = {1}, 
  volume = {4}, 
  journal = {Cognitive Linguistics}, 
  year = {1993}
}
@article{10.1016/0749-5978(90)90022-2, 
  author = {Kameda, Tatsuya and Davis, James H}, 
  title = {{The function of the reference point in individual and group risk decision making}}, 
  issn = {0749-5978}, 
  doi = {10.1016/0749-5978(90)90022-2}, 
  abstract = {{The function of the reference point was studied with individuals and groups deciding among bets. Prospect Theory (Kahneman \& Tversky, 1979) asserts that a psychological reference point is influenced by recent changes in one's current asset. Based on this notion, a person who has experienced losses recently was predicted to make riskier choices than a person who has not, but no systematic risk preference was expected for a person experiencing recent gains. Subjects' individual choices were generally consistent with these notions, although one of the key assumptions of the theory was violated. Assuming that a loss person will have a greater influence over a group decision, a group containing a loss member was anticipated to make riskier decisions than a group that does not. The results of three-person group decisions did not show this pattern; instead, the idiosyncratic risky preference of a loss member was found to be overridden by majority influence from nonloss members. Discussion focused on the implications of the loss member's idiosyncratic choice and of the robustness of social/normative factors relative to task/ individual factors.}}, 
  pages = {55--76}, 
  number = {1}, 
  volume = {46}, 
  journal = {Organizational Behavior and Human Decision Processes}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Kameda-1990-Organizational%20Behavior%20and%20Human%20Decision%20Processes.pdf}, 
  year = {1990}
}
@article{10.2307/1914185, 
  author = {Kahneman, Daniel and Tversky, Amos}, 
  title = {{Prospect Theory: An Analysis of Decision under Risk}}, 
  issn = {0012-9682}, 
  doi = {10.2307/1914185}, 
  pages = {263}, 
  number = {2}, 
  volume = {47}, 
  journal = {Econometrica}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Kahneman-1979-Econometrica.pdf}, 
  year = {1979}
}
@article{10.1007/978-3-319-30526-4_35, 
  author = {Davidsson, Paul and Klügl, Franziska and Verhagen, Harko}, 
  title = {{Springer Handbook of Model-Based Science}}, 
  doi = {10.1007/978-3-319-30526-4\_35}, 
  abstract = {{Understanding and managing complex systems has become one of the biggest challenges for research, policy and industry. Modeling and simulation of complex systems promises to enable us to understand how a human nervous system and brain not just maintain the activities of a metabolism, but enable the production of intelligent behavior, how huge ecosystems adapt to changes, or what actually influences climatic changes. Also man-made systems are getting more complex and difficult, or even impossible, to grasp. Therefore we need methods and tools that can help us in, for example, estimating how different infrastructure investments will affect the transport system and understanding the behavior of large Internet-based systems in different situations. This type of system is becoming the focus of research and sustainable management as there are now techniques, tools and the computational resources available. This chapter discusses modeling and simulation of such complex systems. We will start by discussing what characterizes complex systems.}}, 
  pages = {783--797}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Davidsson-2017-Unknown.pdf}, 
  year = {2017}
}
@article{undefined, 
  author = {}, 
  title = {{www.pnas.org 3/19/2020, 5:44:41 PM.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Unknown-Unknown-Unknown.pdf}
}
@article{10.1073/pnas.082080899, 
  author = {Bonabeau, E}, 
  title = {{Agent-based modeling: Methods and techniques for simulating human systems}}, 
  issn = {0027-8424}, 
  doi = {10.1073/pnas.082080899}, 
  pmid = {12011407}, 
  abstract = {{Agent-based modeling is a powerful simulation modeling technique that has seen a number of applications in the last few years, including applications to real-world business problems. After the basic principles of agent-based simulation are briefly introduced, its four areas of application are discussed by using real-world applications: flow simulation, organizational simulation, market simulation, and diffusion simulation. For each category, one or several business applications are described and analyzed.}}, 
  pages = {7280--7287}, 
  number = {Supplement 3}, 
  volume = {99}, 
  journal = {Proceedings of the National Academy of Sciences}, 
  year = {2002}
}
@article{10.1287/mnsc.2015.2417, 
  author = {Allen, Eric J and Dechow, Patricia M and Pope, Devin G and Wu, George}, 
  title = {{Reference-Dependent Preferences: Evidence from Marathon Runners}}, 
  issn = {0025-1909}, 
  doi = {10.1287/mnsc.2015.2417}, 
  pages = {1657--1672}, 
  number = {6}, 
  volume = {63}, 
  journal = {Management Science}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Allen-2017-Management%20Science.pdf}, 
  year = {2017}
}
@article{undefined, 
  author = {Kingma, Diederik P and Welling, Max}, 
  title = {{Auto-Encoding Variational Bayes}}, 
  eprint = {1312.6114}, 
  abstract = {{How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Kingma-2013-Unknown.pdf}, 
  year = {2013}
}
@article{undefined, 
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina}, 
  title = {{BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}}, 
  eprint = {1810.04805}, 
  abstract = {{We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Devlin-2018-Unknown.pdf}, 
  year = {2018}
}
@article{undefined, 
  author = {Petneházi, Gábor}, 
  title = {{Recurrent Neural Networks for Time Series Forecasting}}, 
  eprint = {1901.00069}, 
  abstract = {{Time series forecasting is difficult. It is difficult even for recurrent neural networks with their inherent ability to learn sequentiality. This article presents a recurrent neural network based time series forecasting framework covering feature engineering, feature importances, point and interval predictions, and forecast evaluation. The description of the method is followed by an empirical study using both LSTM and GRU networks.}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Petneházi-2018-Unknown.pdf}, 
  year = {2018}
}
@article{undefined, 
  author = {Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhutdinov, Ruslan and Zemel, Richard and Bengio, Yoshua}, 
  title = {{Show, Attend and Tell: Neural Image Caption Generation with Visual Attention}}, 
  eprint = {1502.03044}, 
  abstract = {{Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Xu-2015-Unknown.pdf}, 
  year = {2015}
}
@article{undefined, 
  author = {}, 
  title = {{www.gsb.stanford.edu 3/12/2020, 12:11:54 AM.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Unknown-Unknown-Unknown.pdf}
}
@article{undefined, 
  author = {}, 
  title = {{cepa.stanford.edu 3/11/2020, 2:04:51 PM.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Unknown-Unknown-Unknown.pdf}
}
@article{undefined, 
  author = {}, 
  title = {{www.journals.uchicago.edu 3/11/2020, 11:31:02 AM.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Unknown-Unknown-Unknown.pdf}
}
@article{undefined, 
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia}, 
  title = {{Attention Is All You Need}}, 
  eprint = {1706.03762}, 
  abstract = {{The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Vaswani-2017-Unknown.pdf}, 
  year = {2017}
}
@article{10.1080/01621459.2017.1285773, 
  author = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.}, 
  title = {{Variational Inference: A Review for Statisticians}}, 
  issn = {0162-1459}, 
  doi = {10.1080/01621459.2017.1285773}, 
  eprint = {1601.00670}, 
  abstract = {{One of the core problems of modern statistics is to approximate difficult-to-compute probability densities. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation involving the posterior density. In this article, we review variational inference (VI), a method from machine learning that approximates probability densities through optimization. VI has been used in many applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to first posit a family of densities and then to find a member of that family which is close to the target density. Closeness is measured by Kullback–Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this article is to catalyze statistical research on this class of algorithms. Supplementary materials for this article are available online.}}, 
  number = {518}, 
  volume = {112}, 
  journal = {Journal of the American Statistical Association}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Blei-2017-Journal%20of%20the%20American%20Statistical%20Association.pdf}, 
  year = {2017}
}
@book{Ushakov1999, 
  author = {Ushakov, Nikolai G}, 
  title = {{Selected Topics in Characteristic Functions}}, 
  isbn = {9783110935981}, 
  year = {1999}
}
@article{10.1073/pnas.1706530115, 
  author = {Anderson, Ashton and Green, Etan A.}, 
  title = {{Personal bests as reference points}}, 
  issn = {0027-8424}, 
  doi = {10.1073/pnas.1706530115}, 
  pmid = {29434043}, 
  abstract = {{Personal bests act as reference points. Examining 133 million chess games, we find that players exert effort to set new personal best ratings and quit once they have done so. Although specific and difficult goals have been shown to inspire greater motivation than vague pronouncements to “do your best,” doing one’s best can be a specific and difficult goal—and, as we show, motivates in a manner predicted by loss aversion.}}, 
  pages = {1772--1776}, 
  number = {8}, 
  volume = {115}, 
  journal = {Proceedings of the National Academy of Sciences}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Anderson-2018-Proc%20National%20Acad%20Sci.pdf}, 
  year = {2018}
}
@article{undefined, 
  author = {Loshchilov, Ilya and Hutter, Frank}, 
  title = {{Decoupled Weight Decay Regularization}}, 
  eprint = {1711.05101}, 
  abstract = {{L\$\_2\$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is \textbackslashemph\{not\} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L\$\_2\$ regularization (often calling it "weight decay" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by \textbackslashemph\{decoupling\} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at https://github.com/loshchil/AdamW-and-SGDW}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Loshchilov-2017-Unknown.pdf}, 
  year = {2017}
}
@article{undefined, 
  author = {}, 
  title = {{Adya, Collopy\_2002\_How effective are neural networks at forecasting and prediction A review and evaluation.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Adya,%20Collopy_2002_How%20effective%20are%20neural%20networks%20at%20forecasting%20and%20prediction%20A%20review%20and%20evaluation.pdf}
}
@article{undefined, 
  author = {}, 
  title = {{Aggarwal\_2018\_Neural Networks and Deep Learning A Textbook.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Aggarwal_2018_Neural%20Networks%20and%20Deep%20Learning%20A%20Textbook.pdf}
}
@article{undefined, 
  author = {}, 
  title = {{Aguirregabiria - 2009 - Some Notes on Sample Selection Models.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Aguirregabiria%20-%202009%20-%20Some%20Notes%20on%20Sample%20Selection%20Models.pdf}
}
@article{undefined, 
  author = {Ahuja, Chaitanya and Morency, Louis-Philippe}, 
  title = {{Lattice Recurrent Unit: Improving Convergence and Statistical Efficiency for Sequence Modeling}}, 
  eprint = {1710.02254}, 
  abstract = {{Recurrent neural networks have shown remarkable success in modeling sequences. However low resource situations still adversely affect the generalizability of these models. We introduce a new family of models, called Lattice Recurrent Units (LRU), to address the challenge of learning deep multi-layer recurrent models with limited resources. LRU models achieve this goal by creating distinct (but coupled) flow of information inside the units: a first flow along time dimension and a second flow along depth dimension. It also offers a symmetry in how information can flow horizontally and vertically. We analyze the effects of decoupling three different components of our LRU model: Reset Gate, Update Gate and Projected State. We evaluate this family on new LRU models on computational convergence rates and statistical efficiency. Our experiments are performed on four publicly-available datasets, comparing with Grid-LSTM and Recurrent Highway networks. Our results show that LRU has better empirical computational convergence rates and statistical efficiency values, along with learning more accurate language models.}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Ahuja,%20Morency_2017_Lattice%20Recurrent%20Unit%20Improving%20Convergence%20and%20Statistical%20Efficiency%20for%20Sequence%20Modeling.pdf}, 
  year = {2017}
}
@article{10.1093/restud/rdv052, 
  author = {Alaoui, Larbi and Penta, Antonio}, 
  title = {{Endogenous Depth of Reasoning}}, 
  issn = {0034-6527}, 
  doi = {10.1093/restud/rdv052}, 
  pages = {1297--1333}, 
  number = {4}, 
  volume = {83}, 
  journal = {The Review of Economic Studies}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Alaoui,%20Penta_2016_Endogenous%20depth%20of%20reasoning.pdf}, 
  year = {2015}
}
@article{10.1515/jaiscr-2018-0009, 
  author = {Akdeniz, Esra and Egrioglu, Erol and Bas, Eren and Yolcu, Ufuk}, 
  title = {{An ARMA Type Pi-Sigma Artificial Neural Network for Nonlinear Time Series Forecasting}}, 
  doi = {10.1515/jaiscr-2018-0009}, 
  abstract = {{Real-life time series have complex and non-linear structures. Artificial Neural Networks have been frequently used in the literature to analyze non-linear time series. High order artificial neural networks, in view of other artificial neural network types, are more adaptable to the data because of their expandable model order. In this paper, a new recurrent architecture for Pi-Sigma artificial neural networks is proposed. A learning algorithm based on particle swarm optimization is also used as a tool for the training of the proposed neural network. The proposed new high order artificial neural network is applied to three real life time series data and also a simulation study is performed for Istanbul Stock Exchange data set.}}, 
  pages = {121--132}, 
  number = {2}, 
  volume = {8}, 
  journal = {Journal of Artificial Intelligence and Soft Computing Research}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Akdeniz%20et%20al._2018_An%20ARMA%20Type%20Pi-Sigma%20Artificial%20Neural%20Network%20for%20Nonlinear%20Time%20Series%20Forecasting.pdf}, 
  year = {2018}
}
@article{10.1007/s10614-006-9041-7, 
  author = {Aminian, Farzan and Suarez, E. Dante and Aminian, Mehran and Walz, Daniel T.}, 
  title = {{Forecasting Economic Data with Neural Networks}}, 
  issn = {0927-7099}, 
  doi = {10.1007/s10614-006-9041-7}, 
  abstract = {{Studies in recent years have attempted to forecast macroeconomic phenomena with neural networks reporting mixed results. This work represents an investigation of this problem using U.S. Real Gross Domestic Production and Industrial Production as case studies. This work is based on a coefficient of determination which accurately measures the ability of linear or nonlinear models to forecast economic data. The significance of our work is twofold: (1) It confirms recent work that neural networks significantly outperform linear regression due to nonlinearities inherent in the data sets, and (2) it provides a systematic approach that guarantees to find the maximum correlation between input(s) and output of interest.}}, 
  pages = {71--88}, 
  number = {1}, 
  volume = {28}, 
  journal = {Computational Economics}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Aminian%20et%20al._2006_Forecasting%20economic%20data%20with%20neural%20networks.pdf}, 
  year = {2006}
}
@article{undefined, 
  author = {}, 
  title = {{Ando et al.\_2002\_Presence of autoantibody against attr Val30Met after sequential liver transplantation.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Ando%20et%20al._2002_Presence%20of%20autoantibody%20against%20attr%20Val30Met%20after%20sequential%20liver%20transplantation.pdf}
}
@article{undefined, 
  author = {}, 
  title = {{Arellano\_2008\_Binary Models with Endogenous Explanatory Variables Class Notes.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Arellano_2008_Binary%20Models%20with%20Endogenous%20Explanatory%20Variables%20Class%20Notes.pdf}
}
@article{undefined, 
  author = {}, 
  title = {{Angrist, Krueger\_1991\_Does compulsory school attendance affect schooling and earnings.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Angrist,%20Krueger_1991_Does%20compulsory%20school%20attendance%20affect%20schooling%20and%20earnings.pdf}
}
@article{undefined, 
  author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E}, 
  title = {{Layer Normalization}}, 
  eprint = {1607.06450}, 
  abstract = {{Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Ba,%20Kiros,%20Hinton%20-%202016%20-%20Layer%20Normalization.pdf}, 
  year = {2016}
}
@article{undefined, 
  author = {}, 
  title = {{Arellano\_2008\_Tobit and Selection Models Class Notes.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Arellano_2008_Tobit%20and%20Selection%20Models%20Class%20Notes.pdf}
}
@article{undefined, 
  author = {}, 
  title = {{Bank of England\_2019\_Machine learning in UK financial services.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Bank%20of%20England_2019_Machine%20learning%20in%20UK%20financial%20services.pdf}
}
@article{undefined, 
  author = {}, 
  title = {{Baum et al.\_2012\_Binary choice models with endogenous regressors.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Baum%20et%20al._2012_Binary%20choice%20models%20with%20endogenous%20regressors.pdf}
}
@article{10.1080/07350015.2011.648859, 
  author = {Baumeister, Christiane and Kilian, Lutz}, 
  title = {{Real-Time Forecasts of the Real Price of Oil}}, 
  issn = {0735-0015}, 
  doi = {10.1080/07350015.2011.648859}, 
  pages = {326--336}, 
  number = {2}, 
  volume = {30}, 
  journal = {Journal of Business \& Economic Statistics}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Baumeister,%20Kilian_2012_Real-time%20forecasts%20of%20the%20real%20price%20of%20oil.pdf}, 
  year = {2012}
}
@article{10.1080/07350015.2014.949342, 
  author = {Baumeister, Christiane and Kilian, Lutz}, 
  title = {{Forecasting the Real Price of Oil in a Changing World: A Forecast Combination Approach}}, 
  issn = {0735-0015}, 
  doi = {10.1080/07350015.2014.949342}, 
  pages = {338--351}, 
  number = {3}, 
  volume = {33}, 
  journal = {Journal of Business \& Economic Statistics}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Baumeister,%20Kilian_2015_Forecasting%20the%20Real%20Price%20of%20Oil%20in%20a%20Changing%20World%20A%20Forecast%20Combination%20Approach.pdf}, 
  year = {2015}
}
@article{Kilian2016, 
  author = {Baumeister, Christiane and Kilian, Lutz}, 
  title = {{Forty Years of Oil Price Fluctuations: Why the Price of Oil May Still Surprise Us}}, 
  issn = {0895-3309}, 
  doi = {10.1257/jep.30.1.139}, 
  pages = {139--160}, 
  number = {1}, 
  volume = {30}, 
  journal = {Journal of Economic Perspectives}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Baumeister,%20Kilian_2016_Forty%20years%20of%20oil%20price%20fluctuations%20Why%20the%20price%20of%20oil%20may%20still%20surprise%20us.pdf}, 
  year = {2016}
}
@article{undefined, 
  author = {}, 
  title = {{Bengio, Simard, Frasconi\_1994\_Learning long-term dependencies with gradient descent is difficult.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Bengio,%20Simard,%20Frasconi_1994_Learning%20long-term%20dependencies%20with%20gradient%20descent%20is%20difficult.pdf}
}
@article{10.1007/978-3-319-74380-6, 
  author = {Beran, Jan}, 
  title = {{Mathematical Foundations of Time Series Analysis, A Concise Introduction}}, 
  doi = {10.1007/978-3-319-74380-6}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Beran_2018_Mathematical%20foundations%20of%20time%20series%20analysis%20A%20concise%20introduction.pdf}, 
  year = {2017}
}
@article{10.1007/978-3-319-70338-1, 
  author = {Bianchi, Filippo Maria and Maiorino, Enrico and Kampffmeyer, Michael C. and Rizzi, Antonello and Jenssen, Robert}, 
  title = {{Recurrent Neural Networks for Short-Term Load Forecasting, An Overview and Comparative Analysis}}, 
  doi = {10.1007/978-3-319-70338-1}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Bianchi,%20Kampffmeyer%20-%20Unknown%20-%20Recurrent%20Neural%20Networks%20for%20Short-Term%20Load%20Forecasting%20An%20Overview%20and%20Comparative%20Analysis.pdf}, 
  year = {2017}
}
@article{undefined, 
  author = {}, 
  title = {{Bergstra et al.\_2011\_Algorithms for hyper-parameter optimization.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Bergstra%20et%20al._2011_Algorithms%20for%20hyper-parameter%20optimization.pdf}
}
@article{10.1257/jep.28.2.3, 
  author = {Varian, Hal R}, 
  title = {{Big Data: New Tricks for Econometrics}}, 
  issn = {0895-3309}, 
  doi = {10.1257/jep.28.2.3}, 
  pages = {3--28}, 
  number = {2}, 
  volume = {28}, 
  journal = {Journal of Economic Perspectives}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Big_Data_New_Tricks_for_Econo%202.pdf}, 
  year = {2014}
}
@article{10.1080/0003684052000343679, 
  author = {*, Jane M. Binner and Bissoondeeal, Rakesh K. and Elger, Thomas and Gazely, Alicia M. and Mullineux, Andrew W.}, 
  title = {{A comparison of linear forecasting models and neural networks: an application to Euro inflation and Euro Divisia}}, 
  issn = {0003-6846}, 
  doi = {10.1080/0003684052000343679}, 
  abstract = {{Linear models reach their limitations in applications with nonlinearities in the data. In this paper new empirical evidence is provided on the relative Euro inflation forecasting performance of linear and non-linear models. The well established and widely used univariate ARIMA and multivariate VAR models are used as linear forecasting models whereas neural networks (NN) are used as non-linear forecasting models. It is endeavoured to keep the level of subjectivity in the NN building process to a minimum in an attempt to exploit the full potentials of the NN. It is also investigated whether the historically poor performance of the theoretically superior measure of the monetary services flow, Divisia, relative to the traditional Simple Sum measure could be attributed to a certain extent to the evaluation of these indices within a linear framework. Results obtained suggest that non-linear models provide better within-sample and out-of-sample forecasts and linear models are simply a subset of them. The Divisia index also outperforms the Simple Sum index when evaluated in a non-linear framework.}}, 
  pages = {665--680}, 
  number = {6}, 
  volume = {37}, 
  journal = {Applied Economics}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Binner%20et%20al.%20-%202005%20-%20A%20comparison%20of%20linear%20forecasting%20models%20and%20neural%20networks%20An%20application%20to%20Euro%20inflation%20and%20Euro%20Divisia.pdf}, 
  year = {2005}
}
@article{10.1080/0003684052000343679, 
  author = {*, Jane M. Binner and Bissoondeeal, Rakesh K. and Elger, Thomas and Gazely, Alicia M. and Mullineux, Andrew W.}, 
  title = {{A comparison of linear forecasting models and neural networks: an application to Euro inflation and Euro Divisia}}, 
  issn = {0003-6846}, 
  doi = {10.1080/0003684052000343679}, 
  abstract = {{Linear models reach their limitations in applications with nonlinearities in the data. In this paper new empirical evidence is provided on the relative Euro inflation forecasting performance of linear and non-linear models. The well established and widely used univariate ARIMA and multivariate VAR models are used as linear forecasting models whereas neural networks (NN) are used as non-linear forecasting models. It is endeavoured to keep the level of subjectivity in the NN building process to a minimum in an attempt to exploit the full potentials of the NN. It is also investigated whether the historically poor performance of the theoretically superior measure of the monetary services flow, Divisia, relative to the traditional Simple Sum measure could be attributed to a certain extent to the evaluation of these indices within a linear framework. Results obtained suggest that non-linear models provide better within-sample and out-of-sample forecasts and linear models are simply a subset of them. The Divisia index also outperforms the Simple Sum index when evaluated in a non-linear framework.}}, 
  pages = {665--680}, 
  number = {6}, 
  volume = {37}, 
  journal = {Applied Economics}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Binner%20et%20al._2005_A%20comparison%20of%20linear%20forecasting%20models%20and%20neural%20networks%20An%20application%20to%20Euro%20inflation%20and%20Euro%20Divisia.pdf}, 
  year = {2005}
}
@article{10.1111/j.1475-5890.1999.tb00001.x, 
  author = {Blundell, Richard and Dearden, Lorraine and Meghir, Costas and Sianesi, Barbara}, 
  title = {{Human Capital Investment: The Returns from Education and Training to the Individual, the Firm and the Economy}}, 
  issn = {1475-5890}, 
  doi = {10.1111/j.1475-5890.1999.tb00001.x}, 
  abstract = {{This paper provides a non-technical review of the evidence on the returns to education and training for the individual, the firm and the economy at large. It begins by reviewing the empirical work that has attempted to estimate the true causal effect of education and training on individual earnings, focusing on the recent literature that has attempted to control for potential biases in the estimated returns to education and training. It then moves on to review the literature that has looked at the returns from human capital investments to employers. Lack of suitable data and methodological difficulties have resulted in a paucity of studies that have carried out sound empirical work on this issue. In the final part of the review, we look at the work that has tried to assess the contribution of human capital to national economic growth at the macroeconomic level. This work has generally involved using either a ‘growth accounting’ theoretical framework or ‘new growth’ theories. Although the empirical macroeconomic evidence that accompanies this work does not generally allow one to distinguish between the two approaches, there is a substantial body of evidence on the contribution of education to economic growth.}}, 
  pages = {1--23}, 
  number = {1}, 
  volume = {20}, 
  journal = {Fiscal Studies}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Blundell%20et%20al._1999_Human%20Capital%20Investment%20The%20Returns%20from%20Education%20and%20Training%20to%20the%20Individual,%20the%20Firm%20and%20the%20Economy.pdf}, 
  year = {1999}
}
@article{undefined, 
  author = {}, 
  title = {{Bontempi, Taieb, Borgne - 2003 - Machine Learning Strategies for Time Series Forecasting.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Bontempi,%20Taieb,%20Borgne%20-%202003%20-%20Machine%20Learning%20Strategies%20for%20Time%20Series%20Forecasting.pdf}
}
@article{undefined, 
  author = {}, 
  title = {{Black, Kolesnikova, Taylor\_2007\_Earnings Functions When Wages and Prices Vary by Location.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Black,%20Kolesnikova,%20Taylor_2007_Earnings%20Functions%20When%20Wages%20and%20Prices%20Vary%20by%20Location.pdf}
}
@article{undefined, 
  author = {}, 
  title = {{Borwein\_2010\_A Very Complicated Proof of the Minimax Theorem.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Borwein_2010_A%20Very%20Complicated%20Proof%20of%20the%20Minimax%20Theorem.pdf}
}
@article{10.1016/j.ijepes.2018.11.022, 
  author = {Bracale, Antonio and Carpinelli, Guido and Falco, Pasquale De and Hong, Tao}, 
  title = {{Short-term industrial reactive power forecasting}}, 
  issn = {0142-0615}, 
  doi = {10.1016/j.ijepes.2018.11.022}, 
  abstract = {{ Reactive power forecasting is essential for managing energy systems of factories and industrial plants. However, the scientific community has devoted scant attention to industrial load forecasting, and even less to reactive power forecasting. Many challenges in developing a short-term reactive power forecasting system for factories have rarely been studied. Industrial loads may depend on many factors, such as scheduled processes and work shifts, which are uncommon or unnecessary in classical load forecasting models. Moreover, the features of reactive power are significantly different from active power, so some commonly used variables in classical load forecasting models may become meaningless for forecasting reactive power. In this paper, we develop several models to forecast industrial reactive power. These models are constructed based on two forecasting techniques (e.g., multiple linear regression and support vector regression) and two variable selection methods (e.g., cross validation and least absolute shrinkage and selection operator). In the numerical applications based on real data collected from an Italian factory at both aggregate and individual load levels, the proposed models outperform four benchmark models in short forecast horizons.}}, 
  pages = {177--185}, 
  number = {Int J Forecast 32 3 2016}, 
  volume = {107}, 
  journal = {International Journal of Electrical Power \& Energy Systems}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Bracale%20et%20al._2019_Short-term%20industrial%20reactive%20power%20forecasting.pdf}, 
  year = {2019}
}
@article{10.1080/07474938.2015.1035163, 
  author = {Kock, Anders Bredahl and Teräsvirta, Timo}, 
  title = {{Forecasting Macroeconomic Variables Using Neural Network Models and Three Automated Model Selection Techniques}}, 
  issn = {0747-4938}, 
  doi = {10.1080/07474938.2015.1035163}, 
  abstract = {{When forecasting with neural network models one faces several problems, all of which influence the accuracy of the forecasts. First, neural networks are often hard to estimate due to their highly nonlinear structure. To alleviate the problem, White (2006) presented a solution (QuickNet) that converts the specification and nonlinear estimation problem into a linear model selection and estimation problem. We shall compare its performance to that of two other procedures building on the linearization idea: the Marginal Bridge Estimator and Autometrics. Second, one must decide whether forecasting should be carried out recursively or directly. This choice is investigated in this work. The economic time series used in this study are the consumer price indices for the G7 and the Scandinavian countries. In addition, a number of simulations are carried out and results reported in the article.}}, 
  pages = {1753--1779}, 
  number = {8-10}, 
  volume = {35}, 
  journal = {Econometric Reviews}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Bredahl%20Kock,%20Teräsvirta_2016_Forecasting%20Macroeconomic%20Variables%20Using%20Neural%20Network%20Models%20and%20Three%20Automated%20Model%20Selection%20Techn.pdf}, 
  year = {2015}
}
@article{undefined, 
  author = {}, 
  title = {{Brand, Xie\_2010\_Who Benefits Most from College.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Brand,%20Xie_2010_Who%20Benefits%20Most%20from%20College.pdf}
}
@article{10.1007/978-3-319-69014-8, 
  author = {Basuchoudhary, Atin and Bang, James T. and Sen, Tinni}, 
  title = {{Machine-learning Techniques in Economics, New Tools for Predicting Economic Growth}}, 
  doi = {10.1007/978-3-319-69014-8}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Briefs,%20Economics%20-%20Unknown%20-%20Atin%20Basuchoudhary%20James%20T.%20Bang%20Tinni%20Sen.pdf}, 
  year = {2017}
}
@article{undefined, 
  author = {}, 
  title = {{Brockwell, Davis\_1991\_Time Series Theroy and Methods.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Brockwell,%20Davis_1991_Time%20Series%20Theroy%20and%20Methods.pdf}
}
@article{undefined, 
  author = {}, 
  title = {{Busseti, Osband, Wong\_2012\_Deep Learning for Time Series Modeling CS 229 Final Project Report.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Busseti,%20Osband,%20Wong_2012_Deep%20Learning%20for%20Time%20Series%20Modeling%20CS%20229%20Final%20Project%20Report.pdf}
}
@article{Xiu2019, 
  author = {Bybee, Leland and Kelly, Bryan T. and Manela, Asaf and Xiu, Dacheng}, 
  title = {{The Structure of Economic News}}, 
  doi = {10.2139/ssrn.3446225}, 
  abstract = {{We propose an approach to measuring the state of the economy via textual analysis of business news. From the full text content of 800,000 Wall Street Journal articles for 1984–2017, we estimate a topic model that summarizes business news as easily interpretable topical themes and quantifies the proportion of news attention allocated to each theme at each point in time. We then use our news attention estimates as inputs into statistical models of numerical economic time series. We demonstrate that these text-based inputs accurately track a wide range of economic activity measures and that they have incremental forecasting power for macroeconomic outcomes, above and beyond standard numerical predictors. Finally, we use our model to retrieve the news-based narratives that underly “shocks” in numerical economic data.}}, 
  journal = {SSRN Electronic Journal}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Bybee%20et%20al._2019_The%20Structure%20of%20Economic%20News.pdf}, 
  year = {2019}
}
@article{undefined, 
  author = {}, 
  title = {{Camerer, Ho, Chong\_2004\_A cognitive hierarchy model of games.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Camerer,%20Ho,%20Chong_2004_A%20cognitive%20hierarchy%20model%20of%20games.pdf}
}
@article{10.1016/j.physa.2018.11.061, 
  author = {Cao, Jian and Li, Zhi and Li, Jian}, 
  title = {{Financial time series forecasting model based on CEEMDAN and LSTM}}, 
  issn = {0378-4371}, 
  doi = {10.1016/j.physa.2018.11.061}, 
  abstract = {{ In order to improve the accuracy of the stock market prices forecasting, two hybrid forecasting models are proposed in this paper which combine the two kinds of empirical mode decomposition (EMD) with the long short-term memory (LSTM). The financial time series is a kind of non-linear and non-stationary random signal, which can be decomposed into several intrinsic mode functions of different time scales by the original EMD and the complete ensemble empirical mode decomposition with adaptive noise (CEEMDAN). To ensure the effect of historical data onto the prediction result, the LSTM prediction models are established for all each characteristic series from EMD and CEEMDAN deposition. The final prediction results are obtained by reconstructing each prediction series. The forecasting performance of the proposed models is verified by linear regression analysis of the major global stock market indices. Compared with single LSTM model, support vector machine (SVM), multi-layer perceptron (MLP) and other hybrid models, the experimental results show that the proposed models display a better performance in one-step-ahead forecasting of financial time series.}}, 
  pages = {127--139}, 
  number = {J. Econ. Lit. 41 2 2003}, 
  volume = {519}, 
  journal = {Physica A: Statistical Mechanics and its Applications}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Cao,%20Li,%20Li_2019_Financial%20time%20series%20forecasting%20model%20based%20on%20CEEMDAN%20and%20LSTM.pdf}, 
  year = {2018}
}
@article{undefined, 
  author = {}, 
  title = {{Cameron, Trivedi\_2006\_16 . Tobit and Selection.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Cameron,%20Trivedi_2006_16%20.%20Tobit%20and%20Selection.pdf}
}
@article{10.1016/j.physa.2018.11.061, 
  author = {Cao, Jian and Li, Zhi and Li, Jian}, 
  title = {{Financial time series forecasting model based on CEEMDAN and LSTM}}, 
  issn = {0378-4371}, 
  doi = {10.1016/j.physa.2018.11.061}, 
  abstract = {{ In order to improve the accuracy of the stock market prices forecasting, two hybrid forecasting models are proposed in this paper which combine the two kinds of empirical mode decomposition (EMD) with the long short-term memory (LSTM). The financial time series is a kind of non-linear and non-stationary random signal, which can be decomposed into several intrinsic mode functions of different time scales by the original EMD and the complete ensemble empirical mode decomposition with adaptive noise (CEEMDAN). To ensure the effect of historical data onto the prediction result, the LSTM prediction models are established for all each characteristic series from EMD and CEEMDAN deposition. The final prediction results are obtained by reconstructing each prediction series. The forecasting performance of the proposed models is verified by linear regression analysis of the major global stock market indices. Compared with single LSTM model, support vector machine (SVM), multi-layer perceptron (MLP) and other hybrid models, the experimental results show that the proposed models display a better performance in one-step-ahead forecasting of financial time series.}}, 
  pages = {127--139}, 
  number = {J. Econ. Lit. 41 2 2003}, 
  volume = {519}, 
  journal = {Physica A: Statistical Mechanics and its Applications}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Cao,%20Li,%20Li_2019_Financial%20time%20series%20forecasting%20model%20based%20on%20CEEMDAN%20and%20LSTM(2).pdf}, 
  year = {2018}
}
@article{10.1007/978-3-319-75304-1, 
  author = {Caterini, Anthony L. and Chang, Dong Eui}, 
  title = {{Deep Neural Networks in a Mathematical Framework}}, 
  doi = {10.1007/978-3-319-75304-1}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Caterini,%20Eui%20-%20Unknown%20-%20Deep%20Neural%20Networks%20in%20a%20Mathematical%20Framework.pdf}, 
  year = {2018}
}
@article{undefined, 
  author = {}, 
  title = {{Carleton Athey, Luca\_2018\_Economists (and Economics) in Tech Companies.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Carleton%20Athey,%20Luca_2018_Economists%20(and%20Economics)%20in%20Tech%20Companies.pdf}
}
@article{10.1038/s41598-018-24271-9, 
  author = {Che, Zhengping and Purushotham, Sanjay and Cho, Kyunghyun and Sontag, David and Liu, Yan}, 
  title = {{Recurrent Neural Networks for Multivariate Time Series with Missing Values}}, 
  doi = {10.1038/s41598-018-24271-9}, 
  pmid = {29666385}, 
  abstract = {{Multivariate time series data in practical applications, such as health care, geoscience, and biology, are characterized by a variety of missing values. In time series prediction and other related tasks, it has been noted that missing values and their missing patterns are often correlated with the target labels, a.k.a., informative missingness. There is very limited work on exploiting the missing patterns for effective imputation and improving prediction performance. In this paper, we develop novel deep learning models, namely GRU-D, as one of the early attempts. GRU-D is based on Gated Recurrent Unit (GRU), a state-of-the-art recurrent neural network. It takes two representations of missing patterns, i.e., masking and time interval, and effectively incorporates them into a deep model architecture so that it not only captures the long-term temporal dependencies in time series, but also utilizes the missing patterns to achieve better prediction results. Experiments of time series classification tasks on real-world clinical datasets (MIMIC-III, PhysioNet) and synthetic datasets demonstrate that our models achieve state-of-the-art performance and provide useful insights for better understanding and utilization of missing values in time series analysis.}}, 
  pages = {6085}, 
  number = {1}, 
  volume = {8}, 
  journal = {Scientific Reports}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Che%20et%20al.%20-%202018%20-%20Recurrent%20Neural%20Networks%20for%20Multivariate%20Time%20Series%20with%20Missing%20Values.pdf}, 
  year = {2018}
}
@article{10.1016/s0893-6080(05)80092-9, 
  author = {Chakraborty, Kanad and Mehrotra, Kishan and Mohan, Chilukuri K. and Ranka, Sanjay}, 
  title = {{Forecasting the behavior of multivariate time series using neural networks}}, 
  issn = {0893-6080}, 
  doi = {10.1016/s0893-6080(05)80092-9}, 
  abstract = {{This paper presents a neural network approach to multivariate time-series analysis. Real world observations of flour prices in three cities have been used as a benchmark in our experiments. Feedforward connectionist networks have been designed to model flour prices over the period from August 1972 to November 1980 for the cities of Buffalo, Minneapolis, and Kansas City. Remarkable success has been achieved in training the networks to learn the price curve for each of these cities and in making accurate price predictions. Our results show that the neural network approach is a leading contender with the statistical modeling approaches.}}, 
  pages = {961--970}, 
  number = {6}, 
  volume = {5}, 
  journal = {Neural Networks}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Chakraborty%20et%20al._1992_Forecasting%20the%20behavior%20of%20multivariate%20time%20series%20using%20neural%20networks.pdf}, 
  year = {1992}
}
@article{undefined, 
  author = {Cho, Kyunghyun and Merrienboer, Bart van and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua}, 
  title = {{Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation}}, 
  eprint = {1406.1078}, 
  abstract = {{In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Cho%20et%20al._2014_Learning%20Phrase%20Representations%20using%20RNN%20Encoder-Decoder%20for%20Statistical%20Machine%20Translation.pdf}, 
  year = {2014}
}
@article{undefined, 
  author = {}, 
  title = {{Chen, Ranciere - Unknown - Financial Information and Macroeconomic Forecasts 4.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Chen,%20Ranciere%20-%20Unknown%20-%20Financial%20Information%20and%20Macroeconomic%20Forecasts%204.pdf}
}
@article{undefined, 
  author = {Claesen, Marc and Moor, Bart De}, 
  title = {{Hyperparameter Search in Machine Learning}}, 
  eprint = {1502.02127}, 
  abstract = {{We introduce the hyperparameter search problem in the field of machine learning and discuss its main challenges from an optimization perspective. Machine learning methods attempt to build models that capture some element of interest based on given data. Most common learning algorithms feature a set of hyperparameters that must be determined before training commences. The choice of hyperparameters can significantly affect the resulting model's performance, but determining good values can be complex; hence a disciplined, theoretically sound search strategy is essential.}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Claesen,%20De%20Moor_2015_Hyperparameter%20Search%20in%20Machine%20Learning.pdf}, 
  year = {2015}
}
@article{undefined, 
  author = {Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua}, 
  title = {{Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling}}, 
  eprint = {1412.3555}, 
  abstract = {{In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Chung%20et%20al._2014_Empirical%20Evaluation%20of%20Gated%20Recurrent%20Neural%20Networks%20on%20Sequence%20Modeling.pdf}, 
  year = {2014}
}
@article{undefined, 
  author = {}, 
  title = {{Chung et al.\_2015\_Gated Feedback Recurrent Neural Networks.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Chung%20et%20al._2015_Gated%20Feedback%20Recurrent%20Neural%20Networks.pdf}
}
@article{10.1016/j.econmod.2013.09.024, 
  author = {Claveria, Oscar and Torra, Salvador}, 
  title = {{Forecasting tourism demand to Catalonia: Neural networks vs. time series models}}, 
  issn = {0264-9993}, 
  doi = {10.1016/j.econmod.2013.09.024}, 
  abstract = {{The increasing interest aroused by more advanced forecasting techniques, together with the requirement for more accurate forecasts of tourism demand at the destination level due to the constant growth of world tourism, has lead us to evaluate the forecasting performance of neural modelling relative to that of time series methods at a regional level. Seasonality and volatility are important features of tourism data, which makes it a particularly favourable context in which to compare the forecasting performance of linear models to that of nonlinear alternative approaches. Pre-processed official statistical data of overnight stays and tourist arrivals from all the different countries of origin to Catalonia from 2001 to 2009 is used in the study. When comparing the forecasting accuracy of the different techniques for different time horizons, autoregressive integrated moving average models outperform self-exciting threshold autoregressions and artificial neural network models, especially for shorter horizons. These results suggest that the there is a trade-off between the degree of pre-processing and the accuracy of the forecasts obtained with neural networks, which are more suitable in the presence of nonlinearity in the data. In spite of the significant differences between countries, which can be explained by different patterns of consumer behaviour, we also find that forecasts of tourist arrivals are more accurate than forecasts of overnight stays.}}, 
  pages = {220--228}, 
  volume = {36}, 
  journal = {Economic Modelling}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Claveria,%20Torra_2014_Forecasting%20tourism%20demand%20to%20Catalonia%20Neural%20networks%20vs.%20time%20series%20models.pdf}, 
  year = {2014}
}
@article{10.1016/j.cie.2007.06.005, 
  author = {Co, Henry C. and Boosarawongse, Rujirek}, 
  title = {{Forecasting Thailand’s rice export: Statistical techniques vs. artificial neural networks}}, 
  issn = {0360-8352}, 
  doi = {10.1016/j.cie.2007.06.005}, 
  abstract = {{Forecasting the international trade of rice is difficult because demand and supply are affected by many unpredictable factors (e.g., trade barriers and subsidies, agricultural and environmental factors, meteorological factors, biophysical factors, changing demographics, etc.) that interact in a complex manner. This paper compares the performance of artificial neural networks (ANNs) with exponential smoothing and ARIMA models in forecasting rice exports from Thailand. To ascertain that the models can reproduce acceptable results on unseen future, we evaluated various aggregate measures of forecast error (MAE, MSE, MAPE, and RMSE) during the validation process of the models. The results reveal that while the Holt–Winters and the Box–Jenkins models showed satisfactory goodness of fit, the models did not perform as well in predicting unseen data during validation. On the other hand, the ANNs performed relatively well as they were able to track the dynamic non-linear trend and seasonality, and the interactions between them.}}, 
  pages = {610--627}, 
  number = {4}, 
  volume = {53}, 
  journal = {Computers \& Industrial Engineering}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Co%20-%202007%20-%20Forecasting%20Thailand%20’%20s%20rice%20export%20Statistical%20techniques%20vs%20.%20artificial%20neural%20networks.pdf}, 
  year = {2007}
}
@article{10.1016/j.ijforecast.2003.10.004, 
  author = {Clements, Michael P. and Franses, Philip Hans and Swanson, Norman R.}, 
  title = {{Forecasting economic and financial time-series with non-linear models}}, 
  issn = {0169-2070}, 
  doi = {10.1016/j.ijforecast.2003.10.004}, 
  abstract = {{In this paper we discuss the current state-of-the-art in estimating, evaluating, and selecting among non-linear forecasting models for economic and financial time series. We review theoretical and empirical issues, including predictive density, interval and point evaluation and model selection, loss functions, data-mining, and aggregation. In addition, we argue that although the evidence in favor of constructing forecasts using non-linear models is rather sparse, there is reason to be optimistic. However, much remains to be done. Finally, we outline a variety of topics for future research, and discuss a number of areas which have received considerable attention in the recent literature, but where many questions remain.}}, 
  pages = {169--183}, 
  number = {2}, 
  volume = {20}, 
  journal = {International Journal of Forecasting}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Clements,%20Hans,%20Swanson%20-%202004%20-%20Forecasting%20economic%20and%20financial%20time-series%20with%20non-linear%20models%205.pdf}, 
  year = {2004}
}
@article{undefined, 
  author = {}, 
  title = {{Connor, Atlas\_2002\_Recurrent neural networks and time series prediction.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Connor,%20Atlas_2002_Recurrent%20neural%20networks%20and%20time%20series%20prediction.pdf}
}
@article{undefined, 
  author = {}, 
  title = {{Connor, Atlas, Martin\_1992\_Recurrent Networks and NARMA Modeling.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Connor,%20Atlas,%20Martin_1992_Recurrent%20Networks%20and%20NARMA%20Modeling.pdf}
}
@article{undefined, 
  author = {}, 
  title = {{Connor, Atlas, Martin - 1992 - Recurrent Networks and NARMA Modeling.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Connor,%20Atlas,%20Martin%20-%201992%20-%20Recurrent%20Networks%20and%20NARMA%20Modeling.pdf}
}
@article{10.18651/rwp2017-11, 
  author = {Cook, Thomas and Hall, Aaron Smalter}, 
  title = {{Macroeconomic Indicator Forecasting with Deep Neural Networks}}, 
  issn = {1936-5330}, 
  doi = {10.18651/rwp2017-11}, 
  journal = {The Federal Reserve Bank of Kansas City Research Working Papers}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Cook%20et%20al.%20-%202017%20-%20Macroeconomic%20Indicator%20Forecasting%20with%20Deep%20Neural%20Networks.pdf}, 
  year = {2017}
}
@article{undefined, 
  author = {}, 
  title = {{Dale, Krueger\_2002\_Estimating the payoff to attending a more selective college An application of selection on observables and unobservab.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Dale,%20Krueger_2002_Estimating%20the%20payoff%20to%20attending%20a%20more%20selective%20college%20An%20application%20of%20selection%20on%20observables%20and%20unobservab.pdf}
}
@article{10.1007/bf02551274, 
  author = {Cybenko, G.}, 
  title = {{Approximation by superpositions of a sigmoidal function}}, 
  issn = {0932-4194}, 
  doi = {10.1007/bf02551274}, 
  abstract = {{In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.}}, 
  pages = {303--314}, 
  number = {4}, 
  volume = {2}, 
  journal = {Mathematics of Control, Signals and Systems}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Cybenkot_1989_Approximation%20by%20Superpositions%20of%20a%20Sigmoidal%20Function.pdf}, 
  year = {1989}
}
@article{10.1007/978-0-387-71720-3, 
  author = {Yu, Lean and Wang, Shouyang and Lai, Kin Keung}, 
  title = {{Foreign-Exchange-Rate Forecasting With Artificial Neural Networks}}, 
  doi = {10.1007/978-0-387-71720-3}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Dalgleish%20et%20al.%20-%202007%20-%20No%20Title.pdf}, 
  year = {2007}
}
@article{10.1353/jhr.2014.0015, 
  author = {Dale, Stacy B. and Krueger, Alan B.}, 
  title = {{Estimating the Effects of College Characteristics over the Career Using Administrative Earnings Data}}, 
  issn = {1548-8004}, 
  doi = {10.1353/jhr.2014.0015}, 
  abstract = {{We estimate the labor market effect of attending a highly selective college, using the College and Beyond Survey linked to Social Security Administration data. We extend earlier work by estimating effects for students that entered college in 1976 over a longer time horizon (from 1983 through 2007) and for a more recent cohort (1989). For both cohorts, the effects of college characteristics on earnings are sizeable (and similar in magnitude) in standard regression models. In selectionadjusted models, these effects generally fall to close to zero; however, these effects remain large for certain subgroups, such as for black and Hispanic students.}}, 
  pages = {323--358}, 
  number = {2}, 
  volume = {49}, 
  journal = {Journal of Human Resources}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Dale,%20Krueger_2014_Estimating%20the%20Effects%20of%20College%20Characteristics%20over%20the%20Career%20Using%20Administrative%20Earnings%20Data.pdf}, 
  year = {2014}
}
@article{undefined, 
  author = {}, 
  title = {{Dietterich - 2002 - (impo)(George recom)(Sequenced prediction)Machine learning for sequential data A review.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Dietterich%20-%202002%20-%20(impo)(George%20recom)(Sequenced%20prediction)Machine%20learning%20for%20sequential%20data%20A%20review.pdf}
}
@article{undefined, 
  author = {}, 
  title = {{Denmark\_2017\_Education at a Glance OECD Indicators is the authoritative source for information on the state of education around the worl.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Denmark_2017_Education%20at%20a%20Glance%20OECD%20Indicators%20is%20the%20authoritative%20source%20for%20information%20on%20the%20state%20of%20education%20around%20the%20worl.pdf}
}
@article{undefined, 
  author = {}, 
  title = {{David Albouy\_1994\_The Difference in Difference Estimator.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/David%20Albouy_1994_The%20Difference%20in%20Difference%20Estimator.pdf}
}
@article{10.1007/3-540-70659-3_2, 
  author = {Dietterich, Thomas G.}, 
  title = {{Structural, Syntactic, and Statistical Pattern Recognition, Joint IAPR International Workshops SSPR 2002 and SPR 2002 Windsor, Ontario, Canada, August 6–9, 2002 Proceedings}}, 
  doi = {10.1007/3-540-70659-3\_2}, 
  abstract = {{Statistical learning problems in many fields involve sequential data. This paper formalizes the principal learning tasks and describes the methods that have been developed within the machine learning research community for addressing these problems. These methods include sliding window methods, recurrent sliding windows, hidden Markov models, conditional random fields, and graph transformer networks. The paper also discusses some open research issues.}}, 
  pages = {15--30}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Dietterich%20-%202002%20-%20Machine%20Learning%20for%20Sequential%20Data%20A%20Review.pdf}, 
  year = {2002}
}
@article{undefined, 
  author = {}, 
  title = {{Duchi, Hazan, Singer\_2011\_Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Duchi,%20Hazan,%20Singer_2011_Adaptive%20Subgradient%20Methods%20for%20Online%20Learning%20and%20Stochastic%20Optimization.pdf}
}
@article{undefined, 
  author = {}, 
  title = {{Du, Pandey, Xing\_2017\_Modeling approaches for time series forecasting and anomaly detection.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Du,%20Pandey,%20Xing_2017_Modeling%20approaches%20for%20time%20series%20forecasting%20and%20anomaly%20detection.pdf}
}
@article{undefined, 
  author = {}, 
  title = {{Dziechciarz\_2015\_Measurement of Rate of Return in Education. Research Directions.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Dziechciarz_2015_Measurement%20of%20Rate%20of%20Return%20in%20Education.%20Research%20Directions.pdf}
}
@article{10.1007/s11063-014-9342-0, 
  author = {Egrioglu, Erol and Yolcu, Ufuk and Aladag, Cagdas Hakan and Bas, Eren}, 
  title = {{Recurrent Multiplicative Neuron Model Artificial Neural Network for Non-linear Time Series Forecasting}}, 
  issn = {1370-4621}, 
  doi = {10.1007/s11063-014-9342-0}, 
  abstract = {{Artificial neural networks (ANN) have been widely used in recent years to model non-linear time series since ANN approach is a responsive method and does not require some assumptions such as normality or linearity. An important problem with using ANN for time series forecasting is to determine the number of neurons in hidden layer. There have been some approaches in the literature to deal with the problem of determining the number of neurons in hidden layer. A new ANN model was suggested which is called multiplicative neuron model (MNM) in the literature. MNM has only one neuron in hidden layer. Therefore, the problem of determining the number of neurons in hidden layer is automatically solved when MNM is employed. Also, MNM can produce accurate forecasts for non-linear time series. ANN models utilized for non-linear time series have generally autoregressive structures since lagged variables of time series are generally inputs of these models. On the other hand, it is a well-known fact that better forecasts for real life time series can be obtained from models whose inputs are lagged variables of error. In this study, a new recurrent multiplicative neuron neural network model is firstly proposed. In the proposed method, lagged variables of error are included in the model. Also, the problem of determining the number of neurons in hidden layer is avoided when the proposed method is used. To train the proposed neural network model, particle swarm optimization algorithm was used. To evaluate the performance of the proposed model, it was applied to a real life time series. Then, results produced by the proposed method were compared to those obtained from other methods. It was observed that the proposed method has superior performance to existing methods.}}, 
  pages = {249--258}, 
  number = {2}, 
  volume = {41}, 
  journal = {Neural Processing Letters}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Egrioglu%20et%20al._2015_Recurrent%20Multiplicative%20Neuron%20Model%20Artificial%20Neural%20Network%20for%20Non-linear%20Time%20Series%20Forecasting.pdf}, 
  year = {2015}
}
@article{undefined, 
  author = {}, 
  title = {{Egerton\_1973\_The University of Chicago Press.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Egerton_1973_The%20University%20of%20Chicago%20Press.pdf}
}
@article{undefined, 
  author = {}, 
  title = {{Emrah Önder - 2013 - Forecasting Macroeconomic Variables using Artificial Neural Network and Traditional Smoothing Techniques 4.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Emrah%20Önder%20-%202013%20-%20Forecasting%20Macroeconomic%20Variables%20using%20Artificial%20Neural%20Network%20and%20Traditional%20Smoothing%20Techniques%204.pdf}
}
@article{undefined, 
  author = {Ensor, Katherine B. and Han, Yu and Ostdiek, Barbara and Turnbull, Stuart M.}, 
  title = {{Dynamic Jump Intensities and News Arrival in Oil Futures Markets}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Ensor,%20Ostdiek,%20Turnbull_2019_Dynamic%20Jump%20Intensities%20and%20News%20Arrival%20in%20Oil%20Futures%20Markets.pdf}, 
  year = {2019}
}
@article{10.1016/s2212-5671(15)01619-6, 
  author = {Falat, Lukas and Pancikova, Lucia}, 
  title = {{Quantitative Modelling in Economics with Advanced Artificial Neural Networks}}, 
  issn = {2212-5671}, 
  doi = {10.1016/s2212-5671(15)01619-6}, 
  abstract = {{In this paper, authors present a new approach in forecasting economic time series - application of artificial neural networks. Authors apply feed forward artificial neural network of the RBF type into the process of forecasting the financial data. Except for the standard RBF, authors also test their own new versions of this neural network combined with other techniques of the ML. These models represent new and more advanced version of the standard neural network. Authors add the evolutionary approach into the ANN and also combine the standard algorithm for adapting weights of the ANN with an unsupervised clustering algorithm called K-means. Finally, all of these methods are compared and contrasted with standard (statistical) approach on real economic data to show the potential of using artificial neural network in modelling economic variables.}}, 
  pages = {194--201}, 
  volume = {34}, 
  journal = {Procedia Economics and Finance}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Falat,%20Pancikova_2015_Quantitative%20Modelling%20in%20Economics%20with%20Advanced%20Artificial%20Neural%20Networks.pdf}, 
  year = {2015}
}
@article{undefined, 
  author = {}, 
  title = {{Fair, Shiller\_1990\_Comparing Information in Forecasts from Econometric Models.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Fair,%20Shiller_1990_Comparing%20Information%20in%20Forecasts%20from%20Econometric%20Models.pdf}
}
@article{10.1016/j.econmod.2014.03.024, 
  author = {Feng, Lihua and Zhang, Jianzhen}, 
  title = {{Application of artificial neural networks in tendency forecasting of economic growth}}, 
  issn = {0264-9993}, 
  doi = {10.1016/j.econmod.2014.03.024}, 
  abstract = {{Economic growth results from the synthesis influence of various known or unknown and certain or uncertain factors. Mapping of the stimuli effects and the input and output estimates of artificial neural networks (ANN) are obtained via combinations of nonlinear functions. This approach offers the advantages of self-learning, self-organization, self-adaptation, and fault tolerance, as well as the potential for use in forecasting applications of economic growth. Furthermore, the ANN technology allows the use of multiple variables in both the input and output layers. This capability is very important for economic growth calculations because economic development is often a function of many influential variables. Herein, a forecasting system of economic growth with related application has been proposed, based on ANN. The results show that the Zhejiang proportions of tertiary industry in China for 1995, 1996, and 1997 were 32.305\%, 32.174\%, and 32.114\%, respectively, and the comparative errors eann were 0.64\%, −0.08\%, and −0.27\%, respectively, indicating that the forecast result of ANN was better than that of the GM(1.1) model. This method offered better performance and efficiency.}}, 
  pages = {76--80}, 
  volume = {40}, 
  journal = {Economic Modelling}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Feng,%20Zhang_2014_Application%20of%20artificial%20neural%20networks%20in%20tendency%20forecasting%20of%20economic%20growth.pdf}, 
  year = {2014}
}
@article{10.1023/a:1012074215150, 
  author = {Frank, R. J. and Davey, N. and Hunt, S. P.}, 
  title = {{Time Series Prediction and Neural Networks}}, 
  issn = {0921-0296}, 
  doi = {10.1023/a:1012074215150}, 
  abstract = {{Neural Network approaches to time series prediction are briefly discussed, and the need to find the appropriate sample rate and an appropriately sized input window identified. Relevant theoretical results from dynamic systems theory are briefly introduced, and heuristics for finding the appropriate sampling rate and embedding dimension, and thence window size, are discussed. The method is applied to several time series and the resulting generalisation performance of the trained feed-forward neural network predictors is analysed. It is shown that the heuristics can provide useful information in defining the appropriate network architecture.}}, 
  pages = {91--103}, 
  number = {1-3}, 
  volume = {31}, 
  journal = {Journal of Intelligent and Robotic Systems}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Frank,%20Davey,%20Hunt_2001_Time%20series%20prediction%20and%20neural%20networks.pdf}, 
  year = {2001}
}
@article{10.1016/j.neucom.2015.03.100, 
  author = {Galeshchuk, Svitlana}, 
  title = {{Neural networks performance in exchange rate prediction}}, 
  issn = {0925-2312}, 
  doi = {10.1016/j.neucom.2015.03.100}, 
  abstract = {{Exploration of ANNs for the economic purposes is described and empirically examined with the foreign exchange market data. For the experiments, panel data of the exchange rates (USD/EUR, JPN/USD, USD/GBP) are examined and optimized to be used for time-series predictions with neural networks. In this stage the input selection, in which the processing steps to prepare the raw data to a suitable input for the models are investigated. The best neural network is found with the best forecasting abilities, based on a certain performance measure. A visual graphs on the experiments data set is presented after processing steps, to illustrate that particular results. The out-of-sample results are compared with training ones.}}, 
  pages = {446--452}, 
  volume = {172}, 
  journal = {Neurocomputing}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Galeshchuk_2016_Neural%20networks%20performance%20in%20exchange%20rate%20prediction.pdf}, 
  year = {2016}
}
@article{undefined, 
  author = {Gamboa, John Cristian Borges}, 
  title = {{Deep Learning for Time-Series Analysis}}, 
  eprint = {1701.01887}, 
  abstract = {{In many real-world application, e.g., speech recognition or sleep stage classification, data are captured over the course of time, constituting a Time-Series. Time-Series often contain temporal dependencies that cause two otherwise identical points of time to belong to different classes or predict different behavior. This characteristic generally increases the difficulty of analysing them. Existing techniques often depended on hand-crafted features that were expensive to create and required expert knowledge of the field. With the advent of Deep Learning new models of unsupervised learning of features for Time-series analysis and forecast have been developed. Such new developments are the topic of this paper: a review of the main Deep Learning techniques is presented, and some applications on Time-Series analysis are summaried. The results make it clear that Deep Learning has a lot to contribute to the field.}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Gamboa%20-%202017%20-%20Deep%20Learning%20for%20Time-Series%20Analysis.pdf}, 
  year = {2017}
}
@article{10.1007/978-3-642-24797-2, 
  author = {Graves, Alex}, 
  title = {{Supervised Sequence Labelling with Recurrent Neural Networks}}, 
  doi = {10.1007/978-3-642-24797-2}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Gao,%20Chen%20-%202018%20-%20Spectrum%20sensing%20exploiting%20the%20maximum%20value%20of%20power%20spectrum%20density%20in%20wireless%20sensor%20network.pdf}, 
  year = {2012}
}
@article{undefined, 
  author = {}, 
  title = {{Geoff, Ryan\_2012\_Lecture 16 October 18 Review on duality.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Geoff,%20Ryan_2012_Lecture%2016%20October%2018%20Review%20on%20duality.pdf}
}
@article{10.2139/ssrn.1667442, 
  author = {Giovanis, Eleftherios}, 
  title = {{Applications of Neural Network Radial Basis Function in Economics and Financial Time Series}}, 
  doi = {10.2139/ssrn.1667442}, 
  abstract = {{In this paper we present the Radial Basis Neural Network Function. We examine some simple numerical examples of time-series in economics and finance. The forecasting performance is significant superior, especially in financial time-series, to traditional econometric modeling indicating that artificial intelligence procedure are more appropriate. Some MATLAB routines are presented for further application research.}}, 
  journal = {SSRN Electronic Journal}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Giovanis_Unknown_Applications%20of%20Neural%20Network%20Radial%20Basis%20Function%20in%20Economics%20and%20Financial%20Time%20Series.pdf}, 
  year = {2010}
}
@article{10.1109/tnnls.2017.2709324, 
  author = {Godfrey, Luke B and Gashler, Michael S}, 
  title = {{Neural Decomposition of Time-Series Data for Effective Generalization}}, 
  doi = {10.1109/tnnls.2017.2709324}, 
  pmid = {28650827}, 
  eprint = {1705.09137}, 
  abstract = {{We present a neural network technique for the analysis and extrapolation of time-series data called Neural Decomposition (ND). Units with a sinusoidal activation function are used to perform a Fourier-like decomposition of training samples into a sum of sinusoids, augmented by units with nonperiodic activation functions to capture linear trends and other nonperiodic components. We show how careful weight initialization can be combined with regularization to form a simple model that generalizes well. Our method generalizes effectively on the Mackey-Glass series, a dataset of unemployment rates as reported by the U.S. Department of Labor Statistics, a time-series of monthly international airline passengers, the monthly ozone concentration in downtown Los Angeles, and an unevenly sampled time-series of oxygen isotope measurements from a cave in north India. We find that ND outperforms popular time-series forecasting techniques including LSTM, echo state networks, ARIMA, SARIMA, SVR with a radial basis function, and Gashler and Ashmore's model.}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Godfrey,%20Gashler_2018_Neural%20decomposition%20of%20time-series%20data%20for%20effective%20generalization.pdf}, 
  year = {2017}
}
@article{10.1177/0891242403256447, 
  author = {Goetz, Stephan J. and Rupasingha, Anil}, 
  title = {{The Returns on Higher Education: Estimates for the 48 Contiguous States}}, 
  issn = {0891-2424}, 
  doi = {10.1177/0891242403256447}, 
  abstract = {{Governors of a number of states are seeking to expand the proportion of workers with college degrees so that their economies do not fall behind in the new economy. How- ever, because of pronounced differences in the compositions and levels of development of state economies, returns on higher education are far from equal across the states. We estimate the average return on a college degree in each state in terms of its effect on per capita income, holding constant other factors that also influence levels of per capita income.}}, 
  pages = {337--351}, 
  number = {4}, 
  volume = {17}, 
  journal = {Economic Development Quarterly}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Goetz,%20Rupasingha_2003_The%20returns%20on%20higher%20education%20Estimates%20for%20the%2048%20contiguous%20states.pdf}, 
  year = {2003}
}
@article{undefined, 
  author = {}, 
  title = {{Gronau\_1974\_Wage Comparisons--A Selectivity Bias.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Gronau_1974_Wage%20Comparisons--A%20Selectivity%20Bias.pdf}
}
@article{10.1016/j.eswa.2011.02.068, 
  author = {Guresen, Erkam and Kayakutlu, Gulgun and Daim, Tugrul U.}, 
  title = {{Using artificial neural network models in stock market index prediction}}, 
  issn = {0957-4174}, 
  doi = {10.1016/j.eswa.2011.02.068}, 
  abstract = {{Forecasting stock exchange rates is an important financial problem that is receiving increasing attention. During the last few years, a number of neural network models and hybrid models have been proposed for obtaining accurate prediction results, in an attempt to outperform the traditional linear and nonlinear approaches. This paper evaluates the effectiveness of neural network models which are known to be dynamic and effective in stock-market predictions. The models analysed are multi-layer perceptron (MLP), dynamic artificial neural network (DAN2) and the hybrid neural networks which use generalized autoregressive conditional heteroscedasticity (GARCH) to extract new input variables. The comparison for each model is done in two view points: Mean Square Error (MSE) and Mean Absolute Deviate (MAD) using real exchange daily rate values of NASDAQ Stock Exchange index.}}, 
  pages = {10389--10397}, 
  number = {8}, 
  volume = {38}, 
  journal = {Expert Systems with Applications}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Guresen,%20Kayakutlu,%20Daim_2011_Using%20artificial%20neural%20network%20models%20in%20stock%20market%20index%20prediction.pdf}, 
  year = {2011}
}
@article{10.1016/j.econedurev.2004.07.013, 
  author = {Hamermesh, Daniel S. and Parker, Amy}, 
  title = {{Beauty in the classroom: instructors’ pulchritude and putative pedagogical productivity}}, 
  issn = {0272-7757}, 
  doi = {10.1016/j.econedurev.2004.07.013}, 
  abstract = {{Adjusted for many other determinants, beauty affects earnings; but does it lead directly to the differences in productivity that we believe generate earnings differences? We take a large sample of student instructional ratings for a group of university teachers and acquire six independent measures of their beauty, and a number of other descriptors of them and their classes. Instructors who are viewed as better looking receive higher instructional ratings, with the impact of a move from the 10th to the 90th percentile of beauty being substantial. This impact exists within university departments and even within particular courses, and is larger for male than for female instructors. Disentangling whether this outcome represents productivity or discrimination is, as with the issue generally, probably impossible.}}, 
  pages = {369--376}, 
  number = {4}, 
  volume = {24}, 
  journal = {Economics of Education Review}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Hamermesh,%20Parker_2005_Beauty%20in%20the%20classroom%20Instructors'%20pulchritude%20and%20putative%20pedagogical%20productivity.pdf}, 
  year = {2005}
}
@article{10.1134/s0005117913090129, 
  author = {Gusev, K. Yu. and Burkovskii, V. L.}, 
  title = {{A neural network forecasting model for integrated economic indicators}}, 
  issn = {0005-1179}, 
  doi = {10.1134/s0005117913090129}, 
  abstract = {{This paper considers forecasting models for integrated economic indicators. The first model is implemented as a fuzzy system, and the second one represents a fuzzy-neural module. Finally, we study the possibility of using the fuzzy system as a service tool in the fuzzy-neural module.}}, 
  pages = {1567--1572}, 
  number = {9}, 
  volume = {74}, 
  journal = {Automation and Remote Control}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Gusev,%20Burkovskii_2013_A%20neural%20network%20forecasting%20model%20for%20integrated%20economic%20indicators.pdf}, 
  year = {2013}
}
@article{undefined, 
  author = {}, 
  title = {{Han et al.\_2009\_Route Choice under Uncertainty.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Han%20et%20al._2009_Route%20Choice%20under%20Uncertainty.pdf}
}
@article{undefined, 
  author = {}, 
  title = {{Hamilton\_2009\_Understanding Crude Oil Prices(2).pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Hamilton_2009_Understanding%20Crude%20Oil%20Prices(2).pdf}
}
@article{10.1111/1467-6419.00191, 
  author = {Harmon, Colm and Oosterbeek, Hessel and Walker, Ian}, 
  title = {{The Returns to Education: Microeconomics}}, 
  issn = {1467-6419}, 
  doi = {10.1111/1467-6419.00191}, 
  abstract = {{In this paper we focus on education as a private decision to invest in “human capital” and the estimation of the rate of return to that private investment. While the literature is replete with studies that estimate the rate of return using regression methods where the estimated return is obtained as the coefficient on a years of education variable in a log wage equation that contains controls for work experience and other individual characteristics, the issue is surrounded with difficulties. We outline the theoretical arguments underpinning the empirical developments and show that the evidence on private returns to the individual is compelling. Despite some of these issues surrounding the estimation of the return to schooling, our evidence, based on estimates from a variety of datasets and specifications, is that there is an unambiguously positive effect on the earnings of an individual from participation in education. Moreover, the size of the effect seems large relative to the returns on other investments.}}, 
  pages = {115--156}, 
  number = {2}, 
  volume = {17}, 
  journal = {Journal of Economic Surveys}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Harmon,%20Walker_2003_The%20returns%20to%20education%20Microeconomics.pdf}, 
  year = {2003}
}
@article{undefined, 
  author = {Hartford, Jason and Lewis, Greg and Leyton-Brown, Kevin and Taddy, Matt}, 
  title = {{Counterfactual Prediction with Deep Instrumental Variables Networks}}, 
  eprint = {1612.09596}, 
  abstract = {{We are in the middle of a remarkable rise in the use and capability of artificial intelligence. Much of this growth has been fueled by the success of deep learning architectures: models that map from observables to outputs via multiple layers of latent representations. These deep learning algorithms are effective tools for unstructured prediction, and they can be combined in AI systems to solve complex automated reasoning problems. This paper provides a recipe for combining ML algorithms to solve for causal effects in the presence of instrumental variables -- sources of treatment randomization that are conditionally independent from the response. We show that a flexible IV specification resolves into two prediction tasks that can be solved with deep neural nets: a first-stage network for treatment prediction and a second-stage network whose loss function involves integration over the conditional treatment distribution. This Deep IV framework imposes some specific structure on the stochastic gradient descent routine used for training, but it is general enough that we can take advantage of off-the-shelf ML capabilities and avoid extensive algorithm customization. We outline how to obtain out-of-sample causal validation in order to avoid over-fit. We also introduce schemes for both Bayesian and frequentist inference: the former via a novel adaptation of dropout training, and the latter via a data splitting routine.}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Hartford%20et%20al._2016_Counterfactual%20Prediction%20with%20Deep%20Instrumental%20Variables%20Networks.pdf}, 
  year = {2016}
}
@article{undefined, 
  author = {}, 
  title = {{Heckman - 1979 - Sample Selection Bias as a Specification Error.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Heckman%20-%201979%20-%20Sample%20Selection%20Bias%20as%20a%20Specification%20Error.pdf}
}
@article{undefined, 
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian}, 
  title = {{Deep Residual Learning for Image Recognition}}, 
  eprint = {1512.03385}, 
  abstract = {{Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/He%20et%20al._2015_Deep%20Residual%20Learning%20for%20Image%20Recognition.pdf}, 
  year = {2015}
}
@article{undefined, 
  author = {}, 
  title = {{Heckman et al.\_2003\_NBER WORKING PAPER SERIES FIFTY YEARS OF MINCER EARNINGS REGRESSIONS Fifty Years of Mincer Earnings Regressions.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Heckman%20et%20al._2003_NBER%20WORKING%20PAPER%20SERIES%20FIFTY%20YEARS%20OF%20MINCER%20EARNINGS%20REGRESSIONS%20Fifty%20Years%20of%20Mincer%20Earnings%20Regressions.pdf}
}
@article{10.1016/j.econedurev.2011.05.002, 
  author = {Henderson, Daniel J. and Polachek, Solomon W. and Wang, Le}, 
  title = {{Heterogeneity in schooling rates of return}}, 
  issn = {0272-7757}, 
  doi = {10.1016/j.econedurev.2011.05.002}, 
  abstract = {{This paper relaxes the assumption of homogeneous rates of return to schooling by employing nonparametric kernel regression. This approach allows us to examine the differences in rates of return to education both across and within groups. Similar to previous studies we find that on average blacks have higher returns to education than whites, natives have higher returns than immigrants and younger workers have higher returns than older workers. Contrary to previous studies we find that the average gap of the rate of return between white and black workers is larger than previously thought and the gap is smaller between immigrants and natives. We also uncover significant heterogeneity, the extent of which differs both across and within groups. Finally, we uncover the characteristics common amongst those with the smallest and largest returns to education.}}, 
  pages = {1202--1214}, 
  number = {6}, 
  volume = {30}, 
  journal = {Economics of Education Review}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Henderson,%20Polachek,%20Wang_2011_Heterogeneity%20in%20schooling%20rates%20of%20return.pdf}, 
  year = {2011}
}
@article{undefined, 
  author = {}, 
  title = {{Heckman\_1979\_Sample Selection Bias as a Specification Error.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Heckman_1979_Sample%20Selection%20Bias%20as%20a%20Specification%20Error.pdf}
}
@article{undefined, 
  author = {}, 
  title = {{Heckman, Tobias, Vytlacil\_2006\_Four Parameters of Interest in the Evaluation of Social Programs.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Heckman,%20Tobias,%20Vytlacil_2006_Four%20Parameters%20of%20Interest%20in%20the%20Evaluation%20of%20Social%20Programs.pdf}
}
@article{undefined, 
  author = {Hinton, Geoffrey E and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R}, 
  title = {{Improving neural networks by preventing co-adaptation of feature detectors}}, 
  eprint = {1207.0580}, 
  abstract = {{When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This "overfitting" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random "dropout" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Hinton%20et%20al._2012_Improving%20neural%20networks%20by%20preventing%20co-adaptation%20of%20feature%20detectors.pdf}, 
  year = {2012}
}
@article{10.1016/s0360-8352(02)00036-0, 
  author = {Ho, S.L and Xie, M and Goh, T.N}, 
  title = {{A comparative study of neural network and Box-Jenkins ARIMA modeling in time series prediction}}, 
  issn = {0360-8352}, 
  doi = {10.1016/s0360-8352(02)00036-0}, 
  abstract = {{This paper aims to investigate suitable time series models for repairable system failure analysis. A comparative study of the Box-Jenkins autoregressive integrated moving average (ARIMA) models and the artificial neural network models in predicting failures are carried out. The neural network architectures evaluated are the multi-layer feed-forward network and the recurrent network. Simulation results on a set of compressor failures showed that in modeling the stochastic nature of reliability data, both the ARIMA and the recurrent neural network (RNN) models outperform the feed-forward model; in terms of lower predictive errors and higher percentage of correct reversal detection. However, both models perform better with short term forecasting. The effect of varying the damped feedback weights in the recurrent net is also investigated and it was found that RNN at the optimal weighting factor gives satisfactory performances compared to the ARIMA model.}}, 
  pages = {371--375}, 
  number = {2-4}, 
  volume = {42}, 
  journal = {Computers \& Industrial Engineering}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Ho,%20Xie,%20Goh_2002_A%20comparative%20study%20of%20NN%20and%20BoxJenkins%20ARIMA%20modeling%20in%20time%20series%20prediction.pdf.pdf}, 
  year = {2002}
}
@article{undefined, 
  author = {}, 
  title = {{Hochreiter, Schmidhuber\_1997\_Long short term memory.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Hochreiter,%20Schmidhuber_1997_Long%20short%20term%20memory.pdf}
}
@article{undefined, 
  author = {}, 
  title = {{Hoang\_2013\_The Box-Jenkins Methodology for Time Series Models.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Hoang_2013_The%20Box-Jenkins%20Methodology%20for%20Time%20Series%20Models.pdf}
}
@article{10.1016/j.econedurev.2012.03.001, 
  author = {Hoogerheide, Lennart and Block, Joern H. and Thurik, Roy}, 
  title = {{Family background variables as instruments for education in income regressions: A Bayesian analysis}}, 
  issn = {0272-7757}, 
  doi = {10.1016/j.econedurev.2012.03.001}, 
  abstract = {{The validity of family background variables instrumenting education in income regressions has been much criticized. In this paper, we use data from the 2004 German Socio-Economic Panel and Bayesian analysis to analyze to what degree violations of the strict validity assumption affect the estimation results. We show that, in case of moderate direct effects of the instrument on the dependent variable, the results do not deviate much from the benchmark case of no such effect (perfect validity of the instrument's exclusion restriction). In many cases, the size of the bias is smaller than the width of the 95\% posterior interval for the effect of education on income. Thus, a violation of the strict validity assumption does not necessarily lead to results which are strongly different from those of the strict validity case. This finding provides confidence in the use of family background variables as instruments in income regressions.}}, 
  pages = {515--523}, 
  number = {5}, 
  volume = {31}, 
  journal = {Economics of Education Review}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Hoogerheide,%20Block,%20Thurik_2012_Family%20background%20variables%20as%20instruments%20for%20education%20in%20income%20regressions%20A%20Bayesian%20analysis.pdf}, 
  year = {2012}
}
@article{10.1145/3159652.3159690, 
  author = {Hu, Ziniu and Liu, Weiqing and Bian, Jiang and Liu, Xuanzhe and Liu, Tie-Yan}, 
  title = {{Listening to Chaotic Whispers: A Deep Learning Framework for News-oriented Stock Trend Prediction}}, 
  doi = {10.1145/3159652.3159690}, 
  eprint = {1712.02136}, 
  abstract = {{Stock trend prediction plays a critical role in seeking maximized profit from the stock investment. However, precise trend prediction is very difficult since the highly volatile and non-stationary nature of the stock market. Exploding information on the Internet together with the advancing development of natural language processing and text mining techniques have enabled investors to unveil market trends and volatility from online content. Unfortunately, the quality, trustworthiness, and comprehensiveness of online content related to stock market vary drastically, and a large portion consists of the low-quality news, comments, or even rumors. To address this challenge, we imitate the learning process of human beings facing such chaotic online news, driven by three principles: sequential content dependency, diverse influence, and effective and efficient learning. In this paper, to capture the first two principles, we designed a Hybrid Attention Networks(HAN) to predict the stock trend based on the sequence of recent related news. Moreover, we apply the self-paced learning mechanism to imitate the third principle. Extensive experiments on real-world stock market data demonstrate the effectiveness of our framework. A further simulation illustrates that a straightforward trading strategy based on our proposed framework can significantly increase the annualized return.}}, 
  pages = {261--269}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Hu%20et%20al._2018_Listening%20to%20chaotic%20whispers%20A%20deep%20learning%20framework%20for%20news-oriented%20Stock%20trend%20prediction.pdf}, 
  year = {2018}
}
@article{undefined, 
  author = {}, 
  title = {{Hornik\_1991\_Approximation Capabilities of Multilayer Neural Network.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Hornik_1991_Approximation%20Capabilities%20of%20Multilayer%20Neural%20Network.pdf}
}
@article{undefined, 
  author = {}, 
  title = {{Huang et al.\_2007\_FORECASTING.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Huang%20et%20al._2007_FORECASTING.pdf}
}
@article{10.1177/0003122410363567, 
  author = {Brand, Jennie E. and Xie, Yu}, 
  title = {{Who Benefits Most from College?}}, 
  issn = {0003-1224}, 
  doi = {10.1177/0003122410363567}, 
  pmid = {20454549}, 
  abstract = {{In this article, we consider how the economic return to a college education varies across members of the U.S. population. Based on principles of comparative advantage, scholars commonly presume that positive selection is at work, that is, individuals who are most likely to select into college also benefit most from college. Net of observed economic and noneconomic factors influencing college attendance, we conjecture that individuals who are least likely to obtain a college education benefit the most from college. We call this theory the negative selection hypothesis. To adjudicate between the two hypotheses, we study the effects of completing college on earnings by propensity score strata using an innovative hierarchical linear model with data from the National Longitudinal Survey of Youth 1979 and the Wisconsin Longitudinal Study. For both cohorts, for both men and women, and for every observed stage of the life course, we find evidence suggesting negative selection. Results from auxiliary analyses lend further support to the negative selection hypothesis.}}, 
  pages = {273--302}, 
  number = {2}, 
  volume = {75}, 
  journal = {American Sociological Review}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Jennie,%20Yu_2010_Who%20benefits%20most%20from%20college%20Evidence%20for%20negative%20selection%20in%20heterogeneous%20economic%20returns%20to%20higher%20education.pdf}, 
  year = {2010}
}
@article{undefined, 
  author = {Ismail, Aya Abdelsalam and Wood, Timothy and Bravo, Héctor Corrada}, 
  title = {{Improving Long-Horizon Forecasts with Expectation-Biased LSTM Networks}}, 
  eprint = {1804.06776}, 
  abstract = {{State-of-the-art forecasting methods using Recurrent Neural Net- works (RNN) based on Long-Short Term Memory (LSTM) cells have shown exceptional performance targeting short-horizon forecasts, e.g given a set of predictor features, forecast a target value for the next few time steps in the future. However, in many applica- tions, the performance of these methods decays as the forecasting horizon extends beyond these few time steps. This paper aims to explore the challenges of long-horizon forecasting using LSTM networks. Here, we illustrate the long-horizon forecasting problem in datasets from neuroscience and energy supply management. We then propose expectation-biasing, an approach motivated by the literature of Dynamic Belief Networks, as a solution to improve long-horizon forecasting using LSTMs. We propose two LSTM ar- chitectures along with two methods for expectation biasing that significantly outperforms standard practice.}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Ismail,%20Wood,%20Bravo_2018_Improving%20Long-Horizon%20Forecasts%20with%20Expectation-Biased%20LSTM%20Networks.pdf}, 
  year = {2018}
}
@article{undefined, 
  author = {}, 
  title = {{Johnes\_2000\_Up around the bend Linear and nonlinear models of the UK economy compared.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Johnes_2000_Up%20around%20the%20bend%20Linear%20and%20nonlinear%20models%20of%20the%20UK%20economy%20compared.pdf}
}
@article{10.1257/jep.31.1.231, 
  author = {Berk, Jonathan B and Harvey, Campbell R and Hirshleifer, David}, 
  title = {{How to Write an Effective Referee Report and Improve the Scientific Review Process}}, 
  issn = {0895-3309}, 
  doi = {10.1257/jep.31.1.231}, 
  pages = {231--244}, 
  number = {1}, 
  volume = {31}, 
  journal = {Journal of Economic Perspectives}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Jonathan%20B.%20Berk,%20Campbell%20R.%20Harvey_2017_How%20to%20Write%20an%20Effective%20Referee%20Report%20and%20Improve%20the%20Scientific%20Review%20Process.pdf}, 
  year = {2017}
}
@article{10.1016/0925-2312(95)00039-9, 
  author = {Kaastra, Iebeling and Boyd, Milton}, 
  title = {{Designing a neural network for forecasting financial and economic time series}}, 
  issn = {0925-2312}, 
  doi = {10.1016/0925-2312(95)00039-9}, 
  abstract = {{Artificial neural networks are universal and highly flexible function approximators first used in the fields of cognitive science and engineering. In recent years, neural network applications in finance for such tasks as pattern recognition, classification, and time series forecasting have dramatically increased. However, the large number of parameters that must be selected to develop a neural network forecasting model have meant that the design process still involves much trial and error. The objective of this paper is to provide a practical introductory guide in the design of a neural network for forecasting economic time series data. An eight-step procedure to design a neural network forecasting model is explained including a discussion of tradeoffs in parameter selection, some common pitfalls, and points of disagreement among practitioners.}}, 
  pages = {215--236}, 
  number = {3}, 
  volume = {10}, 
  journal = {Neurocomputing}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Kaastra,%20Boyd%20-%201996%20-%20Designing%20a%20neural%20network%20for%20forecasting%20financial%20and%20economic%20time%20series(2).pdf}, 
  year = {1996}
}
@article{10.1088/1742-6596/824/1/012038, 
  author = {Walid and Alamsyah}, 
  title = {{Recurrent Neural Network For Forecasting Time Series With Long Memory Pattern}}, 
  issn = {1742-6596}, 
  doi = {10.1088/1742-6596/824/1/012038}, 
  abstract = {{Recurrent Neural Network as one of the hybrid models are often used to predict and estimate the issues related to electricity, can be used to describe the cause of the swelling of electrical load which experienced by PLN. In this research will be developed RNN forecasting procedures at the time series with long memory patterns. Considering the application is the national electrical load which of course has a different trend with the condition of the electrical load in any country. This research produces the algorithm of time series forecasting which has long memory pattern using E-RNN after this referred to the algorithm of integrated fractional recurrent neural networks (FIRNN).The prediction results of long memory time series using models Fractional Integrated Recurrent Neural Network (FIRNN) showed that the model with the selection of data difference in the range of [-1,1] and the model of Fractional Integrated Recurrent Neural Network (FIRNN) (24,6,1) provides the smallest MSE value, which is 0.00149684.}}, 
  pages = {012038}, 
  number = {1}, 
  volume = {824}, 
  journal = {Journal of Physics: Conference Series}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Kane,%20Mishra,%20Dutta_2016_Preface%20International%20Conference%20on%20Recent%20Trends%20in%20Physics%20(ICRTP%202016).pdf}, 
  year = {2017}
}
@article{10.1016/j.neunet.2019.04.014, 
  author = {Karim, Fazle and Majumdar, Somshubra and Darabi, Houshang and Harford, Samuel}, 
  title = {{Multivariate LSTM-FCNs for Time Series Classification}}, 
  doi = {10.1016/j.neunet.2019.04.014}, 
  pmid = {31121421}, 
  eprint = {1801.04503}, 
  abstract = {{Over the past decade, multivariate time series classification has received great attention. We propose transforming the existing univariate time series classification models, the Long Short Term Memory Fully Convolutional Network (LSTM-FCN) and Attention LSTM-FCN (ALSTM-FCN), into a multivariate time series classification model by augmenting the fully convolutional block with a squeeze-and-excitation block to further improve accuracy. Our proposed models outperform most state-of-the-art models while requiring minimum preprocessing. The proposed models work efficiently on various complex multivariate time series classification tasks such as activity recognition or action recognition. Furthermore, the proposed models are highly efficient at test time and small enough to deploy on memory constrained systems.}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Karim%20et%20al._2018_Multivariate%20LSTM-FCNs%20for%20Time%20Series%20Classification.pdf}, 
  year = {2018}
}
@article{10.1109/access.2017.2779939, 
  author = {Karim, Fazle and Majumdar, Somshubra and Darabi, Houshang and Chen, Shun}, 
  title = {{LSTM Fully Convolutional Networks for Time Series Classification}}, 
  doi = {10.1109/access.2017.2779939}, 
  eprint = {1709.05206}, 
  abstract = {{Fully convolutional neural networks (FCN) have been shown to achieve state-of-the-art performance on the task of classifying time series sequences. We propose the augmentation of fully convolutional networks with long short term memory recurrent neural network (LSTM RNN) sub-modules for time series classification. Our proposed models significantly enhance the performance of fully convolutional networks with a nominal increase in model size and require minimal preprocessing of the dataset. The proposed Long Short Term Memory Fully Convolutional Network (LSTM-FCN) achieves state-of-the-art performance compared to others. We also explore the usage of attention mechanism to improve time series classification with the Attention Long Short Term Memory Fully Convolutional Network (ALSTM-FCN). Utilization of the attention mechanism allows one to visualize the decision process of the LSTM cell. Furthermore, we propose fine-tuning as a method to enhance the performance of trained models. An overall analysis of the performance of our model is provided and compared to other techniques.}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Karim%20et%20al.%20-%20Unknown%20-%20LSTM%20Fully%20Convolutional%20Networks%20for%20Time%20Series%20Classification.pdf}, 
  year = {2017}
}
@article{undefined, 
  author = {}, 
  title = {{Kaastra, Boyd\_1996\_Designing a neural network for forecasting financial and economic time series.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Kaastra,%20Boyd_1996_Designing%20a%20neural%20network%20for%20forecasting%20financial%20and%20economic%20time%20series.pdf}
}
@article{undefined, 
  author = {}, 
  title = {{Kasabov, Member, Song\_2002\_DENFIS Dynamic Evolving Neural-Fuzzy Inference System and Its Application for Time-Series Prediction.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Kasabov,%20Member,%20Song_2002_DENFIS%20Dynamic%20Evolving%20Neural-Fuzzy%20Inference%20System%20and%20Its%20Application%20for%20Time-Series%20Prediction.pdf}
}
@article{undefined, 
  author = {}, 
  title = {{Kerman, Wang, Vaver\_2017\_Estimating Ad Effectiveness using Geo Experiments in a Time-Based Regression Framework.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Kerman,%20Wang,%20Vaver_2017_Estimating%20Ad%20Effectiveness%20using%20Geo%20Experiments%20in%20a%20Time-Based%20Regression%20Framework.pdf}
}
@article{undefined, 
  author = {Keren, Gil and Schuller, Björn}, 
  title = {{Convolutional RNN: an Enhanced Model for Extracting Features from Sequential Data}}, 
  eprint = {1602.05875}, 
  abstract = {{Traditional convolutional layers extract features from patches of data by applying a non-linearity on an affine function of the input. We propose a model that enhances this feature extraction process for the case of sequential data, by feeding patches of the data into a recurrent neural network and using the outputs or hidden states of the recurrent units to compute the extracted features. By doing so, we exploit the fact that a window containing a few frames of the sequential data is a sequence itself and this additional structure might encapsulate valuable information. In addition, we allow for more steps of computation in the feature extraction process, which is potentially beneficial as an affine function followed by a non-linearity can result in too simple features. Using our convolutional recurrent layers we obtain an improvement in performance in two audio classification tasks, compared to traditional convolutional layers. Tensorflow code for the convolutional recurrent layers is publicly available in https://github.com/cruvadom/Convolutional-RNN.}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Keren,%20Schuller_2016_Convolutional%20RNN%20an%20Enhanced%20Model%20for%20Extracting%20Features%20from%20Sequential%20Data.pdf}, 
  year = {2016}
}
@article{10.1016/j.eswa.2014.08.004, 
  author = {Nassirtoussi, Arman Khadjeh and Aghabozorgi, Saeed and Wah, Teh Ying and Ngo, David Chek Ling}, 
  title = {{Text mining of news-headlines for FOREX market prediction: A Multi-layer Dimension Reduction Algorithm with semantics and sentiment}}, 
  issn = {0957-4174}, 
  doi = {10.1016/j.eswa.2014.08.004}, 
  abstract = {{In this paper a novel approach is proposed to predict intraday directional-movements of a currency-pair in the foreign exchange market based on the text of breaking financial news-headlines. The motivation behind this work is twofold: First, although market-prediction through text-mining is shown to be a promising area of work in the literature, the text-mining approaches utilized in it at this stage are not much beyond basic ones as it is still an emerging field. This work is an effort to put more emphasis on the text-mining methods and tackle some specific aspects thereof that are weak in previous works, namely: the problem of high dimensionality as well as the problem of ignoring sentiment and semantics in dealing with textual language. This research assumes that addressing these aspects of text-mining have an impact on the quality of the achieved results. The proposed system proves this assumption to be right. The second part of the motivation is to research a specific market, namely, the foreign exchange market, which seems not to have been researched in the previous works based on predictive text-mining. Therefore, results of this work also successfully demonstrate a predictive relationship between this specific market-type and the textual data of news. Besides the above two main components of the motivation, there are other specific aspects that make the setup of the proposed system and the conducted experiment unique, for example, the use of news article-headlines only and not news article-bodies, which enables usage of short pieces of text rather than long ones; or the use of general financial breaking news without any further filtration.In order to accomplish the above, this work produces a multi-layer algorithm that tackles each of the mentioned aspects of the text-mining problem at a designated layer. The first layer is termed the Semantic Abstraction Layer and addresses the problem of co-reference in text mining that is contributing to sparsity. Co-reference occurs when two or more words in a text corpus refer to the same concept. This work produces a custom approach by the name of Heuristic-Hypernyms Feature-Selection which creates a way to recognize words with the same parent-word to be regarded as one entity. As a result, prediction accuracy increases significantly at this layer which is attributed to appropriate noise-reduction from the feature-space.The second layer is termed Sentiment Integration Layer, which integrates sentiment analysis capability into the algorithm by proposing a sentiment weight by the name of SumScore that reflects investors’ sentiment. Additionally, this layer reduces the dimensions by eliminating those that are of zero value in terms of sentiment and thereby improves prediction accuracy.The third layer encompasses a dynamic model creation algorithm, termed Synchronous Targeted Feature Reduction (STFR). It is suitable for the challenge at hand whereby the mining of a stream of text is concerned. It updates the models with the most recent information available and, more importantly, it ensures that the dimensions are reduced to the absolute minimum.The algorithm and each of its layers are extensively evaluated using real market data and news content across multiple years and have proven to be solid and superior to any other comparable solution. The proposed techniques implemented in the system, result in significantly high directional-accuracies of up to 83.33\%.On top of a well-rounded multifaceted algorithm, this work contributes a much needed research framework for this context with a test-bed of data that must make future research endeavors more convenient. The produced algorithm is scalable and its modular design allows improvement in each of its layers in future research. This paper provides ample details to reproduce the entire system and the conducted experiments.}}, 
  pages = {306--324}, 
  number = {1}, 
  volume = {42}, 
  journal = {Expert Systems with Applications}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Khadjeh%20Nassirtoussi%20et%20al._2015_Text%20mining%20of%20news-headlines%20for%20FOREX%20market%20prediction%20A%20Multi-layer%20Dimension%20Reduction%20Algorithm%20w.pdf}, 
  year = {2015}
}
@article{undefined, 
  author = {}, 
  title = {{Khashei, Bijari - 2010 - Expert Systems with Applications An artificial neural network ( p , d , q ) model for timeseries forecasting.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Khashei,%20Bijari%20-%202010%20-%20Expert%20Systems%20with%20Applications%20An%20artificial%20neural%20network%20(%20p%20,%20d%20,%20q%20)%20model%20for%20timeseries%20forecasting.pdf}
}
@article{10.1016/j.asoc.2010.10.015, 
  author = {Khashei, Mehdi and Bijari, Mehdi}, 
  title = {{A novel hybridization of artificial neural networks and ARIMA models for time series forecasting}}, 
  issn = {1568-4946}, 
  doi = {10.1016/j.asoc.2010.10.015}, 
  abstract = {{Improving forecasting especially time series forecasting accuracy is an important yet often difficult task facing decision makers in many areas. Both theoretical and empirical findings have indicated that integration of different models can be an effective way of improving upon their predictive performance, especially when the models in combination are quite different. Artificial neural networks (ANNs) are flexible computing frameworks and universal approximators that can be applied to a wide range of forecasting problems with a high degree of accuracy. However, using ANNs to model linear problems have yielded mixed results, and hence; it is not wise to apply ANNs blindly to any type of data. Autoregressive integrated moving average (ARIMA) models are one of the most popular linear models in time series forecasting, which have been widely applied in order to construct more accurate hybrid models during the past decade. Although, hybrid techniques, which decompose a time series into its linear and nonlinear components, have recently been shown to be successful for single models, these models have some disadvantages. In this paper, a novel hybridization of artificial neural networks and ARIMA model is proposed in order to overcome mentioned limitation of ANNs and yield more general and more accurate forecasting model than traditional hybrid ARIMA-ANNs models. In our proposed model, the unique advantages of ARIMA models in linear modeling are used in order to identify and magnify the existing linear structure in data, and then a neural network is used in order to determine a model to capture the underlying data generating process and predict, using preprocessed data. Empirical results with three well-known real data sets indicate that the proposed model can be an effective way to improve forecasting accuracy achieved by traditional hybrid models and also either of the components models used separately.}}, 
  pages = {2664--2675}, 
  number = {2}, 
  volume = {11}, 
  journal = {Applied Soft Computing}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Khashei,%20Bijari_2011_A%20novel%20hybridization%20of%20artificial%20neural%20networks%20and%20ARIMA%20models%20for%20time%20series%20forecasting.pdf}, 
  year = {2011}
}
@article{undefined, 
  author = {}, 
  title = {{Kline - 2004 - Methods for Multi-Step Time Series Forecasting with Neural Networks.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Kline%20-%202004%20-%20Methods%20for%20Multi-Step%20Time%20Series%20Forecasting%20with%20Neural%20Networks.pdf}
}
@article{undefined, 
  author = {}, 
  title = {{Kingma, Ba\_2014\_Adam A Method for Stochastic Optimization.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Kingma,%20Ba_2014_Adam%20A%20Method%20for%20Stochastic%20Optimization.pdf}
}
@article{10.3982/ecta11983, 
  author = {Kneeland, Terri}, 
  title = {{Identifying Higher‐Order Rationality}}, 
  issn = {1468-0262}, 
  doi = {10.3982/ecta11983}, 
  abstract = {{Strategic choice data from a carefully chosen set of ring-network games are used to obtain individual-level estimates of higher-order rationality. The experimental design exploits a natural exclusion restriction that is considerably weaker than the assumptions underlying alternative designs in the literature. In our data set, 93 percent of subjects are rational, 71 percent are rational and believe others are rational, 44 percent are rational and hold second-order beliefs that others are rational, and 22 percent are rational and hold at least third-order beliefs that others are rational.}}, 
  pages = {2065--2079}, 
  number = {5}, 
  volume = {83}, 
  journal = {Econometrica}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Kneeland_2015_Identifying%20Higher-Order%20Rationality.pdf}, 
  year = {2015}
}
@article{10.1177/003754979105700508, 
  author = {Tang, Zaiyong and Almeida, Chrys de and Fishwick, Paul A.}, 
  title = {{Time series forecasting using neural networks vs. Box- Jenkins methodology}}, 
  issn = {0037-5497}, 
  doi = {10.1177/003754979105700508}, 
  abstract = {{We discuss the results of a comparative study of the performance of neural networks and conventional methods in forecasting time series. Our work was initially inspired by previously published works that yielded inconsistent results about comparative performance. We have experimented with three time series of different complexity using different feed forward, backpropagation neural network models and the standard Box-Jenkins model. Our experiments demonstrate that for time series with long memory, both methods produced comparable results. However, for series with short memory, neural networks outper formed the Box-Jenkins model. We note that some of the comparable results arise since the neural network and time series model appear to be functionally similar models. We have found that for time series of different complexities there are optimal neural network topologies and parameters that enable them to learn more efficiently. Our initial conclusions are that neural networks are robust and provide good long-term forecasting. They are also parsimonious in their data requirements. Neural networks represent a promising alternative for forecasting, but there are problems deter mining the optimal topology and parameters for efficient learning.}}, 
  pages = {303--310}, 
  number = {5}, 
  volume = {57}, 
  journal = {Simulation}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Kolarik,%20Rudorfer_2007_Time%20series%20forecasting%20using%20neural%20networks.pdf}, 
  year = {1991}
}
@article{undefined, 
  author = {}, 
  title = {{Kohzadi et al.\_1996\_A comparison of artificial neural network and time series models for forecasting commodity prices.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Kohzadi%20et%20al._1996_A%20comparison%20of%20artificial%20neural%20network%20and%20time%20series%20models%20for%20forecasting%20commodity%20prices.pdf}
}
@article{10.1016/0925-2312(95)00020-8, 
  author = {Kohzadi, Nowrouz and Boyd, Milton S. and Kermanshahi, Bahman and Kaastra, Iebeling}, 
  title = {{A comparison of artificial neural network and time series models for forecasting commodity prices}}, 
  issn = {0925-2312}, 
  doi = {10.1016/0925-2312(95)00020-8}, 
  abstract = {{A feedforward neural network which can account for nonlinear relationships was used to compare ARIMA and neural network price forecasting performance. Data used was monthly live cattle and wheat prices from 1950 through 1990. The experiment was repeated seven times for successive three year periods. This involved using a walk forward or sliding window approach from 1970 through 1990 which generated out of sample results. The neural network models achieved a 27 percent and 56 percent lower mean squared error than ARIMA model. The absolute mean error and mean absolute percent error were also lower for the neural network models. The neural network models were able to capture a significant number of turning points for both wheat and cattle, while the ARIMA model was only able to do so for wheat. Since this forecasting method is not problem specific and uses only past prices, it can be applied to other forecasting problems such as stocks and other financial prices.}}, 
  pages = {169--181}, 
  number = {2}, 
  volume = {10}, 
  journal = {Neurocomputing}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Kohzadi%20et%20al._1996_A%20comparison%20of%20artificial%20neural%20network%20and%20time%20series%20models%20for%20forecasting%20commodity%20prices(2).pdf}, 
  year = {1996}
}
@article{undefined, 
  author = {}, 
  title = {{Kolesnikova\_2010\_The return to education isn't calculated easily.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Kolesnikova_2010_The%20return%20to%20education%20isn't%20calculated%20easily.pdf}
}
@article{10.2139/ssrn.3032458, 
  author = {Kominers, Scott Duke and Teytelboym, Alexander and Crawford, Vincent P.}, 
  title = {{An Invitation to Market Design}}, 
  doi = {10.2139/ssrn.3032458}, 
  abstract = {{Market design seeks to translate economic theory and analysis into practical solutions to real-world problems. By redesigning both the rules that guide market transactions and the infrastructure that enables those transactions to take place, market designers can address a broad range of market failures. In this paper, we illustrate the process and power of market design through three examples: the design of medical residency matching programs; a scrip system to allocate food donations to food banks; and the recent “Incentive Auction” that reallocated wireless spectrum from television broadcasters to telecoms. Our lead examples show how effective market design can encourage participation, reduce gaming, and aggregate information, in order to improve liquidity, efficiency, and equity in markets. We also discuss a number of fruitful applications of market design in other areas of economic and public policy.}}, 
  journal = {SSRN Electronic Journal}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Kominers,%20Teytelboym,%20Crawford_2017_An%20invitation%20to%20market%20design.pdf}, 
  year = {2017}
}
@article{undefined, 
  author = {}, 
  title = {{Kondratenko, Kuperin\_2003\_Using Recurrent Neural Networks To Forecasting of Forex.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Kondratenko,%20Kuperin_2003_Using%20Recurrent%20Neural%20Networks%20To%20Forecasting%20of%20Forex.pdf}
}
@article{10.13152/ijrvet.2.4.4, 
  author = {Kopatz, Susanne and Pilz, Matthias}, 
  title = {{The Academic Takes it All? A Comparison of Returns to Investment in Education between Graduates and Apprentices in Canada}}, 
  issn = {2197-8646}, 
  doi = {10.13152/ijrvet.2.4.4}, 
  abstract = {{This paper analyses the returns to education of specific occupations in Canada. The purpose is to scrutinize whether and in how far academic and vocational education do differ in monetary benefits regarding individual returns. Therefore, two different methodologies of calculation are used to compute the concrete returns to education. As a result it is shown empirically that within the here selected occupational groups (e.g. librarians and electricians) there is no decisive earnings benefit regarding academic careers, although a positive correlation of income level and educational achievement can be verified. Our findings justify revisiting the underlying assumption that vocational education and training cannot generate benefits comparable with those generated by higher education. The earnings data suggest that monetary aspects may be less crucial than generally assumed to the reputation and perceived value of vocational education and training. Therefore, social status and prestige seem to be the most significant contributory factors to vocational training's low status in Canada.}}, 
  pages = {308--324}, 
  number = {4}, 
  volume = {2}, 
  journal = {International Journal for Research in Vocational Education and Training}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Kopatz,%20Pilz_2015_The%20Academic%20Takes%20it%20All%20A%20Comparison%20of%20Returns%20to%20Investment%20in%20Education%20between%20Graduates%20and%20Apprentices%20in%20Cana.pdf}, 
  year = {2015}
}
@article{undefined, 
  author = {Kreif, Noemi and DiazOrdaz, Karla}, 
  title = {{Machine learning in policy evaluation: new tools for causal inference}}, 
  eprint = {1903.00402}, 
  abstract = {{While machine learning (ML) methods have received a lot of attention in recent years, these methods are primarily for prediction. Empirical researchers conducting policy evaluations are, on the other hand, pre-occupied with causal problems, trying to answer counterfactual questions: what would have happened in the absence of a policy? Because these counterfactuals can never be directly observed (described as the "fundamental problem of causal inference") prediction tools from the ML literature cannot be readily used for causal inference. In the last decade, major innovations have taken place incorporating supervised ML tools into estimators for causal parameters such as the average treatment effect (ATE). This holds the promise of attenuating model misspecification issues, and increasing of transparency in model selection. One particularly mature strand of the literature include approaches that incorporate supervised ML approaches in the estimation of the ATE of a binary treatment, under the \textbackslashtextit\{unconfoundedness\} and positivity assumptions (also known as exchangeability and overlap assumptions). This article reviews popular supervised machine learning algorithms, including the Super Learner. Then, some specific uses of machine learning for treatment effect estimation are introduced and illustrated, namely (1) to create balance among treated and control groups, (2) to estimate so-called nuisance models (e.g. the propensity score, or conditional expectations of the outcome) in semi-parametric estimators that target causal parameters (e.g. targeted maximum likelihood estimation or the double ML estimator), and (3) the use of machine learning for variable selection in situations with a high number of covariates.}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Kreif,%20DiazOrdaz_2019_Machine%20learning%20in%20policy%20evaluation%20new%20tools%20for%20causal%20inference.pdf}, 
  year = {2019}
}
@article{10.1016/j.physa.2016.08.062, 
  author = {Kordanuli, Bojana and Barjaktarović, Lidija and Jeremić, Ljiljana and Alizamir, Meysam}, 
  title = {{Appraisal of artificial neural network for forecasting of economic parameters}}, 
  issn = {0378-4371}, 
  doi = {10.1016/j.physa.2016.08.062}, 
  abstract = {{The main aim of this research is to develop and apply artificial neural network (ANN) with extreme learning machine (ELM) and back propagation (BP) to forecast gross domestic product (GDP) and Hirschman–Herfindahl Index (HHI). GDP could be developed based on combination of different factors. In this investigation GDP forecasting based on the agriculture and industry added value in gross domestic product (GDP) was analysed separately. Other inputs are final consumption expenditure of general government, gross fixed capital formation (investments) and fertility rate. The relation between product market competition and corporate investment is contentious. On one hand, the relation can be positive, but on the other hand, the relation can be negative. Several methods have been proposed to monitor market power for the purpose of developing procedures to mitigate or eliminate the effects. The most widely used methods are based on indices such as the Hirschman–Herfindahl Index (HHI). The reliability of the ANN models were accessed based on simulation results and using several statistical indicators. Based upon simulation results, it was presented that ELM shows better performances than BP learning algorithm in applications of GDP and HHI forecasting.}}, 
  pages = {515--519}, 
  volume = {465}, 
  journal = {Physica A: Statistical Mechanics and its Applications}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Kordanuli%20et%20al._2017_Appraisal%20of%20artificial%20neural%20network%20for%20forecasting%20of%20economic%20parameters.pdf}, 
  year = {2017}
}
@article{undefined, 
  author = {}, 
  title = {{Lachiheb, Gouider\_2018\_A hierarchical Deep neural network design for stock returns prediction.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Lachiheb,%20Gouider_2018_A%20hierarchical%20Deep%20neural%20network%20design%20for%20stock%20returns%20prediction.pdf}
}
@article{undefined, 
  author = {}, 
  title = {{Kurt Hornik, Maxwell Stinchcombe, Halbert White\_1989\_Multilayer Feedforward Networks are Universal Approximators.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Kurt%20Hornik,%20Maxwell%20Stinchcombe,%20Halbert%20White_1989_Multilayer%20Feedforward%20Networks%20are%20Universal%20Approximators.pdf}
}
@article{undefined, 
  author = {Lai, Guokun and Chang, Wei-Cheng and Yang, Yiming and Liu, Hanxiao}, 
  title = {{Modeling Long- and Short-Term Temporal Patterns with Deep Neural Networks}}, 
  eprint = {1703.07015}, 
  abstract = {{Multivariate time series forecasting is an important machine learning problem across many domains, including predictions of solar plant energy output, electricity consumption, and traffic jam situation. Temporal data arise in these real-world applications often involves a mixture of long-term and short-term patterns, for which traditional approaches such as Autoregressive models and Gaussian Process may fail. In this paper, we proposed a novel deep learning framework, namely Long- and Short-term Time-series network (LSTNet), to address this open challenge. LSTNet uses the Convolution Neural Network (CNN) and the Recurrent Neural Network (RNN) to extract short-term local dependency patterns among variables and to discover long-term patterns for time series trends. Furthermore, we leverage traditional autoregressive model to tackle the scale insensitive problem of the neural network model. In our evaluation on real-world data with complex mixtures of repetitive patterns, LSTNet achieved significant performance improvements over that of several state-of-the-art baseline methods. All the data and experiment codes are available online.}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Lai%20-%202018%20-%20Modeling%20Long-%20and%20Short-Term%20Temporal%20Patterns%20with%20Deep%20Neural%20Networks.pdf}, 
  year = {2017}
}
@article{undefined, 
  author = {}, 
  title = {{LEIGH\_2008\_Returns To Education in Australia.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/LEIGH_2008_Returns%20To%20Education%20in%20Australia.pdf}
}
@article{10.1080/13504850320000078653, 
  author = {Lemke, Robert J. and Rischall, Isaac C.}, 
  title = {{Skill, parental income, and IV estimation of the returns to schooling}}, 
  issn = {1350-4851}, 
  doi = {10.1080/13504850320000078653}, 
  abstract = {{Recently, attention has moved away from using parental background variables, such as parental education, in favour of using institutional features of the education system as instruments when estimating the return to schooling. In this paper, these possible instruments are revisited. Using the National Longitudinal Survey of Youth, several specifications of the wage equation are estimated and three types of instruments used - parental education, quarter of birth, and college proximity. It is shown that under some specifications - in particular, by including parental income and individual skill in the wage equation - parental education appears to be a valid and useful instrument. On the other hand, when using the institutional instruments, the weak correlation between the instruments and years of schooling produces imprecise and likely biased estimates of the return to schooling.}}, 
  pages = {281--286}, 
  number = {5}, 
  volume = {10}, 
  journal = {Applied Economics Letters}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Lemke,%20Rischall_2003_Skill,%20parental%20income,%20and%20IV%20estimation%20of%20the%20returns%20to%20schooling.pdf}, 
  year = {2003}
}
@article{undefined, 
  author = {}, 
  title = {{Leung, Daouk, Chen - 2000 - Forecasting stock indices a comparison of classification and level estimation models.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Leung,%20Daouk,%20Chen%20-%202000%20-%20Forecasting%20stock%20indices%20a%20comparison%20of%20classification%20and%20level%20estimation%20models.pdf}
}
@article{undefined, 
  author = {}, 
  title = {{Lemieux\_2016\_The Evolution of the Returns to Human Capital in Canada ,.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Lemieux_2016_The%20Evolution%20of%20the%20Returns%20to%20Human%20Capital%20in%20Canada%20,.pdf}
}
@article{10.1016/j.asoc.2017.06.035, 
  author = {Li, Jinbo and Pedrycz, Witold and Jamal, Iqbal}, 
  title = {{Multivariate time series anomaly detection: A framework of Hidden Markov Models}}, 
  issn = {1568-4946}, 
  doi = {10.1016/j.asoc.2017.06.035}, 
  abstract = {{In this study, we develop an approach to multivariate time series anomaly detection focused on the transformation of multivariate time series to univariate time series. Several transformation techniques involving Fuzzy C-Means (FCM) clustering and fuzzy integral are studied. In the sequel, a Hidden Markov Model (HMM), one of the commonly encountered statistical methods, is engaged here to detect anomalies in multivariate time series. We construct HMM-based anomaly detectors and in this context compare several transformation methods. A suite of experimental studies along with some comparative analysis is reported.}}, 
  pages = {229--240}, 
  volume = {60}, 
  journal = {Applied Soft Computing}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Li,%20Pedrycz,%20Jamal_2017_Multivariate%20time%20series%20anomaly%20detection%20A%20framework%20of%20Hidden%20Markov%20Models.pdf}, 
  year = {2017}
}
@article{undefined, 
  author = {}, 
  title = {{Lin, Guo, Aberer - 2017 - HYBRID NEURAL NETWORKS OVER TIME SERIES FOR TREND FORECASTING.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Lin,%20Guo,%20Aberer%20-%202017%20-%20HYBRID%20NEURAL%20NETWORKS%20OVER%20TIME%20SERIES%20FOR%20TREND%20FORECASTING.pdf}
}
@article{undefined, 
  author = {Li, Xiangang and Wu, Xihong}, 
  title = {{Constructing Long Short-Term Memory based Deep Recurrent Neural Networks for Large Vocabulary Speech Recognition}}, 
  eprint = {1410.4281}, 
  abstract = {{Long short-term memory (LSTM) based acoustic modeling methods have recently been shown to give state-of-the-art performance on some speech recognition tasks. To achieve a further performance improvement, in this research, deep extensions on LSTM are investigated considering that deep hierarchical model has turned out to be more efficient than a shallow one. Motivated by previous research on constructing deep recurrent neural networks (RNNs), alternative deep LSTM architectures are proposed and empirically evaluated on a large vocabulary conversational telephone speech recognition task. Meanwhile, regarding to multi-GPU devices, the training process for LSTM networks is introduced and discussed. Experimental results demonstrate that the deep LSTM networks benefit from the depth and yield the state-of-the-art performance on this task.}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Li,%20Wu_2015_Long%20short-term%20memory%20based%20convolutional%20recurrent%20neural%20networks%20for%20large%20vocabulary%20speech%20recognition.pdf}, 
  year = {2014}
}
@article{undefined, 
  author = {}, 
  title = {{López de Prado\_2018\_Advances in Financial Machine Learning.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/López%20de%20Prado_2018_Advances%20in%20Financial%20Machine%20Learning.pdf}
}
@article{undefined, 
  author = {}, 
  title = {{Lundberg\_2017\_Causal forests A tutorial in high-dimensional causal inference.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Lundberg_2017_Causal%20forests%20A%20tutorial%20in%20high-dimensional%20causal%20inference.pdf}
}
@article{10.1016/s0895-7177(00)00272-7, 
  author = {Luk, Kin C. and Ball, J.E. and Sharma, A.}, 
  title = {{An application of artificial neural networks for rainfall forecasting}}, 
  issn = {0895-7177}, 
  doi = {10.1016/s0895-7177(00)00272-7}, 
  abstract = {{Rainfall forecasting is important for many catchment management applications, in particular for flood warning systems. The variability of rainfall in space and time, however, renders quantitative forecasting of rainfall extremely difficult. The depth of rainfall and its distribution in the temporal and spatial dimensions depends on many variables, such as pressure, temperature, and wind speed and direction. Due to the complexity of the atmospheric processes by which rainfall is generated and the lack of available data on the necessary temporal and spatial scales, it is not feasible generally to forecast rainfall using a physically based process model. Recent developments in artificial intelligence and, in particular, those techniques aimed at pattern recognition, however, provide an alternative approach for developing of a rainfall forecasting model. Artificial neural networks (ANNs), which perform a nonlinear mapping between inputs and outputs, are one such technique. Presented in this paper are the results of a study investigating the application of ANNs to forecast the spatial distribution of rainfall for an urban catchment. Three alternative types of ANNs, namely multilayer feedforward neural networks, partial recurrent neural networks, and time delay neural networks, were identified, developed and, as presented in this paper, found to provide reasonable predictions of the rainfall depth one time-step in advance. The data requirements for and the accuracy obtainable from these three alternative types of ANNs are discussed.}}, 
  pages = {683--693}, 
  number = {6-7}, 
  volume = {33}, 
  journal = {Mathematical and Computer Modelling}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Luk,%20Ball,%20Sharma_2001_An%20application%20of%20artificial%20neural%20networks%20for%20rainfall%20forecasting.pdf}, 
  year = {2001}
}
@article{undefined, 
  author = {}, 
  title = {{Mackay\_1875\_GloVe Global Vectors for Word Representation.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Mackay_1875_GloVe%20Global%20Vectors%20for%20Word%20Representation.pdf}
}
@article{undefined, 
  author = {}, 
  title = {{McCann, Guillen\_2017\_Five lectures on optimal transportation Geometry, regularity and applications.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/McCann,%20Guillen_2017_Five%20lectures%20on%20optimal%20transportation%20Geometry,%20regularity%20and%20applications.pdf}
}
@article{undefined, 
  author = {}, 
  title = {{Margareta, Constantin\_1959\_NEW TECHNIQUES APPLIED IN ECONOMICS. ARTIFICIAL NEURAL NETWORK Udrescu Margareta.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Margareta,%20Constantin_1959_NEW%20TECHNIQUES%20APPLIED%20IN%20ECONOMICS.%20ARTIFICIAL%20NEURAL%20NETWORK%20Udrescu%20Margareta.pdf}
}
@article{undefined, 
  author = {}, 
  title = {{Makridakis, Hibon\_1997\_ARMA models and the Box-Jenkins methodology.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Makridakis,%20Hibon_1997_ARMA%20models%20and%20the%20Box-Jenkins%20methodology.pdf}
}
@article{undefined, 
  author = {}, 
  title = {{McKinney\_1976\_Python for Data Analysis Data Wrangling with Pandas, NumPy, and IPython.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/McKinney_1976_Python%20for%20Data%20Analysis%20Data%20Wrangling%20with%20Pandas,%20NumPy,%20and%20IPython.pdf}
}
@article{undefined, 
  author = {}, 
  title = {{McKinney, Perktold, Seabold\_2011\_Time Series Analysis in Python with statsmodels.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/McKinney,%20Perktold,%20Seabold_2011_Time%20Series%20Analysis%20in%20Python%20with%20statsmodels.pdf}
}
@article{10.1016/j.physa.2016.08.040, 
  author = {Milačić, Ljubiša and Jović, Srđan and Vujović, Tanja and Miljković, Jovica}, 
  title = {{Application of artificial neural network with extreme learning machine for economic growth estimation}}, 
  issn = {0378-4371}, 
  doi = {10.1016/j.physa.2016.08.040}, 
  abstract = {{The purpose of this research is to develop and apply the artificial neural network (ANN) with extreme learning machine (ELM) to forecast gross domestic product (GDP) growth rate. The economic growth forecasting was analyzed based on agriculture, manufacturing, industry and services value added in GDP. The results were compared with ANN with back propagation (BP) learning approach since BP could be considered as conventional learning methodology. The reliability of the computational models was accessed based on simulation results and using several statistical indicators. Based on results, it was shown that ANN with ELM learning methodology can be applied effectively in applications of GDP forecasting.}}, 
  pages = {285--288}, 
  volume = {465}, 
  journal = {Physica A: Statistical Mechanics and its Applications}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Milačić%20et%20al._2017_Application%20of%20artificial%20neural%20network%20with%20extreme%20learning%20machine%20for%20economic%20growth%20estimation.pdf}, 
  year = {2017}
}
@article{10.1016/j.jefas.2016.07.002, 
  author = {Moghaddam, Amin Hedayati and Moghaddam, Moein Hedayati and Esfandyari, Morteza}, 
  title = {{Stock market index prediction using artificial neural network}}, 
  issn = {2077-1886}, 
  doi = {10.1016/j.jefas.2016.07.002}, 
  abstract = {{In this study the ability of artificial neural network (ANN) in forecasting the daily NASDAQ stock exchange rate was investigated. Several feed forward ANNs that were trained by the back propagation algorithm have been assessed. The methodology used in this study considered the short-term historical stock prices as well as the day of week as inputs. Daily stock exchange rates of NASDAQ from January 28, 2015 to 18 June, 2015 are used to develop a robust model. First 70 days (January 28 to March 7) are selected as training dataset and the last 29 days are used for testing the model prediction ability. Networks for NASDAQ index prediction for two type of input dataset (four prior days and nine prior days) were developed and validated.}}, 
  pages = {89--93}, 
  number = {41}, 
  volume = {21}, 
  journal = {Journal of Economics, Finance and Administrative Science}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Moghaddam,%20Moghaddam,%20Esfandyari_2016_Predicción%20del%20índice%20del%20mercado%20bursátil%20utilizando%20una%20red%20neuronal%20artificial.pdf}, 
  year = {2016}
}
@article{undefined, 
  author = {}, 
  title = {{Misra\_2019\_Mish A Self Regularized Non-Monotonic Neural Activation Function.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Misra_2019_Mish%20A%20Self%20Regularized%20Non-Monotonic%20Neural%20Activation%20Function.pdf}
}
@article{undefined, 
  author = {}, 
  title = {{Miller, Cupchik\_2016\_A Synthetic World Population for Agent-Based Social Simulation.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Miller,%20Cupchik_2016_A%20Synthetic%20World%20Population%20for%20Agent-Based%20Social%20Simulation.pdf}
}
@article{10.1016/s0272-7757(99)00023-0, 
  author = {Monks, James}, 
  title = {{The returns to individual and college characteristics Evidence from the National Longitudinal Survey of Youth}}, 
  issn = {0272-7757}, 
  doi = {10.1016/s0272-7757(99)00023-0}, 
  abstract = {{There is growing interest in the heterogeneity of earnings among college graduates. This study examines earnings differentials across both individual and institutional characteristics. Using data from the National Longitudinal Survey of Youth, it can be seen that graduates from highly or most selective colleges and universities earn significantly more than graduates from less selective institutions. Additionally, graduates from graduate degree granting and research universities, and private universities earn more than their counterparts from liberal arts colleges and public institutions. There is, however, variation across racial and gender groups in the returns to individual and college characteristics. These findings are important in an educational environment where the (market) value of a liberal arts education is under scrutiny, and where the higher costs of private versus public colleges and universities are being questioned.}}, 
  pages = {279--289}, 
  number = {3}, 
  volume = {19}, 
  journal = {Economics of Education Review}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Monks_2000_The%20returns%20to%20individual%20and%20college%20characteristics%20Evidence%20from%20the%20National%20Longitudinal%20Survey%20of%20Youth.pdf}, 
  year = {2000}
}
@article{undefined, 
  author = {}, 
  title = {{Moody - 1995 - Economic forecasting Challenges and neural network solutions.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Moody%20-%201995%20-%20Economic%20forecasting%20Challenges%20and%20neural%20network%20solutions.pdf}
}
@article{undefined, 
  author = {}, 
  title = {{Mohammadi, Mundra, Socher\_2015\_CS 224D Deep Learning for NLP 1 Spring 2015.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Mohammadi,%20Mundra,%20Socher_2015_CS%20224D%20Deep%20Learning%20for%20NLP%201%20Spring%202015.pdf}
}
@article{undefined, 
  author = {}, 
  title = {{Moshiri, Cameron\_2002\_Neural network versus econometric models in forecasting inflation.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Moshiri,%20Cameron_2002_Neural%20network%20versus%20econometric%20models%20in%20forecasting%20inflation.pdf}
}
@article{undefined, 
  author = {}, 
  title = {{Moshiri, Cameron\_2000\_Neural network versus econometric models in forecasting inflation.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Moshiri,%20Cameron_2000_Neural%20network%20versus%20econometric%20models%20in%20forecasting%20inflation.pdf}
}
@article{10.1016/j.jeconom.2003.10.015, 
  author = {Moretti, Enrico}, 
  title = {{Estimating the social return to higher education: evidence from longitudinal and repeated cross-sectional data}}, 
  issn = {0304-4076}, 
  doi = {10.1016/j.jeconom.2003.10.015}, 
  abstract = {{Economists have speculated for at least a century that the social return to education may exceed the private return. In this paper, I estimate spillovers from college education by comparing wages for otherwise similar individuals who work in cities with different shares of college graduates in the labor force. A key issue in this comparison is the presence of unobservable characteristics of individuals and cities that may raise wages and be correlated with college share. I use longitudinal data to estimate a model of non-random selection of workers among cities. I account for unobservable city-specific demand shocks by using two instrumental variables: the (lagged) city demographic structure and the presence of a land-grant college. I find that a percentage point increase in the supply of college graduates raises high school drop-outs’ wages by 1.9\%, high school graduates’ wages by 1.6\%, and college graduates wages by 0.4\%. The effect is larger for less educated groups, as predicted by a conventional demand and supply model. But even for college graduates, an increase in the supply of college graduates increases wages, as predicted by a model that includes conventional demand and supply factors as well as spillovers.}}, 
  pages = {175--212}, 
  number = {1-2}, 
  volume = {121}, 
  journal = {Journal of Econometrics}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Moretti_2004_Estimating%20the%20social%20return%20to%20higher%20education%20Evidence%20from%20longitudinal%20and%20repeated%20cross-sectional%20data.pdf}, 
  year = {2004}
}
@article{10.1016/j.egypro.2019.01.952, 
  author = {Muzaffar, Shahzad and Afshari, Afshin}, 
  title = {{Short-Term Load Forecasts Using LSTM Networks}}, 
  issn = {1876-6102}, 
  doi = {10.1016/j.egypro.2019.01.952}, 
  abstract = {{ With the increasing load requirements and the sophistication of power stations, knowing in advance about the electrical load not only at short-term periods such as hours or couple of days but also over the longer-term periods such as weeks and months is indispensable for a range of benefits such as important technical and economic impacts. Traditional methods such as ARMA, SARIMA, and ARMAX have been used for decades. In recent years, the artificial intelligence (AI) techniques such as neural networks and deep learning are emerging in the field of time series analysis. Towards this end, the artificial neural networks (ANN) and recurrent neural networks (RNN) are being explored and have shown promises in much better forecasting as compared to traditional methods. Long short-term memory (LSTM) networks are a special kind of RNN that have the capabilities to learn the long-term dependencies. In this work, we have picked up an electrical load data with exogenous variables including temperature, humidity, and wind speed. The data is used to train the LSTM network. For a fair comparison, the data is also used in traditional methods to model the load time series. The trained LSTM network and the developed models are then used to forecast over the horizons of 24 hours, 48 hours, 7 days and 30 days. The forecasts generated by the LSTM are compared with the results of traditional methods using RMSE and MAPE for all the forecast horizons. The results of a number of experiments show that the LSTM based forecast is better than other methods and have the potential to further improve the accuracies of forecasts.}}, 
  pages = {2922--2927}, 
  volume = {158}, 
  journal = {Energy Procedia}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Muzaffar,%20Afshari_2019_Short-term%20load%20forecasts%20using%20LSTM%20networks.pdf}, 
  year = {2019}
}
@article{10.1016/j.econmod.2015.12.014, 
  author = {Mostafa, Mohamed M. and El-Masry, Ahmed A.}, 
  title = {{Oil price forecasting using gene expression programming and artificial neural networks}}, 
  issn = {0264-9993}, 
  doi = {10.1016/j.econmod.2015.12.014}, 
  abstract = {{This study aims to forecast oil prices using evolutionary techniques such as gene expression programming (GEP) and artificial neural network (NN) models to predict oil prices over the period from January 2, 1986 to June 12, 2012. Autoregressive integrated moving average (ARIMA) models are employed to benchmark evolutionary models. The results reveal that the GEP technique outperforms traditional statistical techniques in predicting oil prices. Further, the GEP model outperforms the NN and the ARIMA models in terms of the mean squared error, the root mean squared error and the mean absolute error. Finally, the GEP model also has the highest explanatory power as measured by the R-squared statistic. The results of this study have important implications for both theory and practice.}}, 
  pages = {40--53}, 
  volume = {54}, 
  journal = {Economic Modelling}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Mostafa,%20El-Masry_2016_Oil%20price%20forecasting%20using%20gene%20expression%20programming%20and%20artificial%20neural%20networks.pdf}, 
  year = {2016}
}
@article{undefined, 
  author = {}, 
  title = {{Nachbar\_2014\_Semi-Continuity.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Nachbar_2014_Semi-Continuity.pdf}
}
@article{10.1787/eco_studies-v2002-art3-en, 
  author = {Blöndal, Sveinbjörn and Field, Simon and Girouard, Nathalie}, 
  title = {{Investment in human capital through upper-secondary and tertiary education}}, 
  issn = {0255-0822}, 
  doi = {10.1787/eco\_studies-v2002-art3-en}, 
  abstract = {{This article examines various efficiency and equity aspects related to the skill acquisition of young people and older adults. The analysis suggests that human capital investment is associated with significant labour-market gains for individuals, including higher post-tax earnings and better employment prospects, which exceed the investment costs, mainly foregone earnings and tuition fees, by a significant margin. It also shows that the net benefits are strongly influenced by policy related factors, such as study length, tuition subsidies and student support. Overall, the estimates reported in the article indicate that there are strong incentives for the average student to continue studying beyond the compulsory schooling age, and also point to the benefits of such investment in education for society as a whole. However, the net gains fall with age, mainly reflecting a shorter period to take advantage of the benefits that come with education. Finally, the article notes that students in higher education tend to come from more affluent backgrounds and that they benefit from large public subsidies, whereas young people from disadvantaged backgrounds are less likely to participate in tertiary education and thus benefit from public subsidies.}}, 
  pages = {41--89}, 
  number = {1}, 
  volume = {2002}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/N._2002_Investment%20in%20human%20capital%20throught%20upper-secondary%20and%20tertiary%20education.pdf}, 
  year = {2003}
}
@article{undefined, 
  author = {}, 
  title = {{Nag et al.\_2017\_Use of phenolic resin in coke making at Tata Steel.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Nag%20et%20al._2017_Use%20of%20phenolic%20resin%20in%20coke%20making%20at%20Tata%20Steel.pdf}
}
@article{10.1002/for.838, 
  author = {Nag, Ashok K. and Mitra, Amit}, 
  title = {{Forecasting daily foreign exchange rates using genetically optimized neural networks}}, 
  issn = {1099-131X}, 
  doi = {10.1002/for.838}, 
  abstract = {{Forecasting currency exchange rates is an important financial problem that has received much attention especially because of its intrinsic difficulty and practical applications. The statistical distribution of foreign exchange rates and their linear unpredictability are recurrent themes in the literature of international finance. Failure of various structural econometric models and models based on linear time series techniques to deliver superior forecasts to the simplest of all models, the simple random walk model, have prompted researchers to use various non-linear techniques. A number of non-linear time series models have been proposed in the recent past for obtaining accurate prediction results, in an attempt to ameliorate the performance of simple random walk models. In this paper, we use a hybrid artificial intelligence method, based on neural network and genetic algorithm for modelling daily foreign exchange rates. A detailed comparison of the proposed method with non-linear statistical models is also performed. The results indicate superior performance of the proposed method as compared to the traditional non-linear time series techniques and also fixed-geometry neural network models. Copyright © 2002 John Wiley \& Sons, Ltd.}}, 
  pages = {501--511}, 
  number = {7}, 
  volume = {21}, 
  journal = {Journal of Forecasting}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Nag,%20Mitra_2002_Forecasting%20daily%20foreign%20exchange%20rates%20using%20genetically%20optimized%20neural%20networks.pdf}, 
  year = {2002}
}
@article{undefined, 
  author = {}, 
  title = {{Nagel\_1995\_Unraveling in Guessing Games An Experimental Study.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Nagel_1995_Unraveling%20in%20Guessing%20Games%20An%20Experimental%20Study.pdf}
}
@article{10.1007/978-3-319-32862-1, 
  author = {Neusser, Klaus}, 
  title = {{Time Series Econometrics}}, 
  doi = {10.1007/978-3-319-32862-1}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Neusser%20-%20Unknown%20-%20Time%20Series%20Econometrics.pdf}, 
  year = {2016}
}
@article{10.1016/j.eswa.2010.06.026, 
  author = {Nanda, S.R. and Mahanty, B. and Tiwari, M.K.}, 
  title = {{Clustering Indian stock market data for portfolio management}}, 
  issn = {0957-4174}, 
  doi = {10.1016/j.eswa.2010.06.026}, 
  abstract = {{In this paper a data mining approach for classification of stocks into clusters is presented. After classification, the stocks could be selected from these groups for building a portfolio. It meets the criterion of minimizing the risk by diversification of a portfolio. The clustering approach categorizes stocks on certain investment criteria. We have used stock returns at different times along with their valuation ratios from the stocks of Bombay Stock Exchange for the fiscal year 2007–2008. Results of our analysis show that K-means cluster analysis builds the most compact clusters as compared to SOM and Fuzzy C-means for stock classification data. We then select stocks from the clusters to build a portfolio, minimizing portfolio risk and compare the returns with that of the benchmark index, i.e. Sensex.}}, 
  pages = {8793--8798}, 
  number = {12}, 
  volume = {37}, 
  journal = {Expert Systems with Applications}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Nanda,%20Mahanty,%20Tiwari_2010_Clustering%20indian%20stock%20market%20data%20for%20portfolio%20management.pdf}, 
  year = {2010}
}
@article{undefined, 
  author = {}, 
  title = {{Newey, Powell\_2003\_Instrumental variable estimation of nonparametric models.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Newey,%20Powell_2003_Instrumental%20variable%20estimation%20of%20nonparametric%20models.pdf}
}
@article{10.1016/j.ijforecast.2010.02.008, 
  author = {Nyberg, Henri}, 
  title = {{Forecasting the direction of the US stock market with dynamic binary probit models}}, 
  issn = {0169-2070}, 
  doi = {10.1016/j.ijforecast.2010.02.008}, 
  abstract = {{Several empirical studies have documented that the signs of excess stock returns are, to some extent, predictable. In this paper, we consider the predictive ability of the binary dependent dynamic probit model in predicting the direction of monthly excess stock returns. The recession forecast obtained from the model for a binary recession indicator appears to be the most useful predictive variable, and once it is employed, the sign of the excess return is predictable in-sample. The new dynamic “error correction” probit model proposed in the paper yields better out-of-sample sign forecasts, with the resulting average trading returns being higher than those of either the buy-and-hold strategy or trading rules based on ARMAX models.}}, 
  pages = {561--578}, 
  number = {2}, 
  volume = {27}, 
  journal = {International Journal of Forecasting}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Nyberg%20-%202011%20-%20Forecasting%20the%20direction%20of%20the%20US%20stock%20market%20with%20dynamic%20binary%20probit%20models%202.pdf}, 
  year = {2011}
}
@article{undefined, 
  author = {}, 
  title = {{Obara\_2012\_Supermodular Games.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Obara_2012_Supermodular%20Games.pdf}
}
@article{10.1787/9789264258051-en, 
  author = {OECD}, 
  title = {{Skills Matter}}, 
  issn = {2307-8723}, 
  doi = {10.1787/9789264258051-en}, 
  abstract = {{In the wake of the technological revolution that began in the last decades of the 20th century, labour market demand for information-processing and other high-level cognitive and interpersonal skills is growing substantially. The Survey of Adult Skills, a product of the OECD Programme for the International Assessment of Adult Competencies (PIAAC), was designed to provide insights into the availability of some of these key skills in society and how they are used at work and at home. The first survey of its kind, it directly measures proficiency in several information-processing skills – namely literacy, numeracy and problem solving in technology-rich environments.This volume reports results from the 24 countries and regions that participated in the first round of the survey in 2011-12 (first published in OECD Skills Outlook 2013: First Results from the Survey of Adult Skills) and from the nine additional countries that participated in the second round in 2014-15 (Chile, Greece, Indonesia [Jakarta], Israel, Lithuania, New Zealand, Singapore, Slovenia and Turkey). It describes adults’ proficiency in the three information-processing skills assessed, and examines how skills proficiency is related to labour market and social outcomes. Another related report, The Survey of Adult Skills: Reader’s Companion, Second Edition, describes the design and methodology of the survey and its relationship to other international assessments of young students and adults.}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/OECD_2016_OECD%20Skills%20Studies%202016%20Skills%20Matter%20Further%20Results%20from%20The%20Survey%20of%20Adult%20Skills.pdf}, 
  year = {2016}
}
@article{undefined, 
  author = {}, 
  title = {{OECD\_2017\_Benchmarking Higher Education System Performance Conceptual framework and data.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/OECD_2017_Benchmarking%20Higher%20Education%20System%20Performance%20Conceptual%20framework%20and%20data.pdf}
}
@article{10.1007/s001810000053, 
  author = {Arias, Omar and Hallock, Kevin F. and Sosa-Escudero, Walter}, 
  title = {{Individual heterogeneity in the returns to schooling: instrumental variables quantile regression using twins data}}, 
  issn = {0377-7332}, 
  doi = {10.1007/s001810000053}, 
  abstract = {{Considerable effort has been exercised in estimating mean returns to education while carefully considering biases arising from unmeasured ability and measurement error. Recent work has investigated whether there are variations from the “mean” return to education across the population with mixed results. We use an instrumental variables estimator for quantile regression on a sample of twins to estimate an entire family of returns to education at different quantiles of the conditional distribution of wages while addressing simultaneity and measurement error biases. We test whether there is individual heterogeneity in returns to education and find that: more able individuals obtain more schooling perhaps due to lower marginal costs and/or higher marginal benefits of schooling and that higher ability individuals (those further to the right in the conditional distribution of wages) have higher returns to schooling consistent with a non-trivial interaction between schooling and unobserved abilities in the generation of earnings. The estimated returns are never lower than 9 percent and can be as high as 13 percent at the top of the conditional distribution of wages but they vary significantly only along the lower to middle quantiles. Our findings may have meaningful implications for the design of educational policies.}}, 
  pages = {7--40}, 
  number = {1}, 
  volume = {26}, 
  journal = {Empirical Economics}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Omar,%20Sosa-Escudero,%20Hallock_2001_Individual%20heterogeneity%20in%20the%20returns%20to%20schooling%20instrumental%20variables%20quantile%20regression%20using.pdf}, 
  year = {2001}
}
@article{undefined, 
  author = {}, 
  title = {{Oreopoulous, Petronijevic\_2013\_Making College Worth It A Review of Research on teh Returns to Higher Education.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Oreopoulous,%20Petronijevic_2013_Making%20College%20Worth%20It%20A%20Review%20of%20Research%20on%20teh%20Returns%20to%20Higher%20Education.pdf}
}
@article{10.1016/j.engappai.2009.09.015, 
  author = {Faruk, Durdu Ömer}, 
  title = {{A hybrid neural network and ARIMA model for water quality time series prediction}}, 
  issn = {0952-1976}, 
  doi = {10.1016/j.engappai.2009.09.015}, 
  abstract = {{Accurate predictions of time series data have motivated the researchers to develop innovative models for water resources management. Time series data often contain both linear and nonlinear patterns. Therefore, neither ARIMA nor neural networks can be adequate in modeling and predicting time series data. The ARIMA model cannot deal with nonlinear relationships while the neural network model alone is not able to handle both linear and nonlinear patterns equally well. In the present study, a hybrid ARIMA and neural network model is proposed that is capable of exploiting the strengths of traditional time series approaches and artificial neural networks. The proposed approach consists of an ARIMA methodology and feed-forward, backpropagation network structure with an optimized conjugated training algorithm. The hybrid approach for time series prediction is tested using 108-month observations of water quality data, including water temperature, boron and dissolved oxygen, during 1996–2004 at Büyük Menderes river, Turkey. Specifically, the results from the hybrid model provide a robust modeling framework capable of capturing the nonlinear nature of the complex time series and thus producing more accurate predictions. The correlation coefficients between the hybrid model predicted values and observed data for boron, dissolved oxygen and water temperature are 0.902, 0.893, and 0.909, respectively, which are satisfactory in common model applications. Predicted water quality data from the hybrid model are compared with those from the ARIMA methodology and neural network architecture using the accuracy measures. Owing to its ability in recognizing time series patterns and nonlinear characteristics, the hybrid model provides much better accuracy over the ARIMA and neural network models for water quality predictions.}}, 
  pages = {586--594}, 
  number = {4}, 
  volume = {23}, 
  journal = {Engineering Applications of Artificial Intelligence}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Ömer%20Faruk_2010_A%20hybrid%20neural%20network%20and%20ARIMA%20model%20for%20water%20quality%20time%20series%20prediction.pdf}, 
  year = {2010}
}
@article{10.1007/s11207-019-1434-6, 
  author = {Pala, Zeydin and Atici, Ramazan}, 
  title = {{Forecasting Sunspot Time Series Using Deep Learning Methods}}, 
  issn = {0038-0938}, 
  doi = {10.1007/s11207-019-1434-6}, 
  abstract = {{To predict Solar Cycle 25, we used the values of sunspot number (SSN), which have been measured regularly from 1749 to the present. In this study, we converted the SSN dataset, which consists of SSNs between 1749 – 2018, into a time series, and made the ten-year forecast with the help of deep-learning (DL) algorithms. Our results show that algorithms such as long-short-term memory (LSTM) and neural network autoregression (NNAR), which are DL algorithms, perform better than many algorithms such as ARIMA, Naive, Seasonal Naive, Mean and Drift, which are expressed as classical algorithms in a large time-series estimation process. Using the R programming language, it was also predicted that the maximum amplitude of Solar Cycle (SC) 25 will be reached between 2022 and 2023.}}, 
  pages = {50}, 
  number = {5}, 
  volume = {294}, 
  journal = {Solar Physics}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Pala,%20Atici%20-%202019%20-%20Forecasting%20Sunspot%20Time%20Series%20Using%20Deep%20Learning%20Methods.pdf}, 
  year = {2019}
}
@article{undefined, 
  author = {}, 
  title = {{Panigrahi, Karali, Behera\_2013\_Time Series Forecasting using Evolutionary Neural Network.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Panigrahi,%20Karali,%20Behera_2013_Time%20Series%20Forecasting%20using%20Evolutionary%20Neural%20Network.pdf}
}
@article{10.1007/s11207-019-1434-6, 
  author = {Pala, Zeydin and Atici, Ramazan}, 
  title = {{Forecasting Sunspot Time Series Using Deep Learning Methods}}, 
  issn = {0038-0938}, 
  doi = {10.1007/s11207-019-1434-6}, 
  abstract = {{To predict Solar Cycle 25, we used the values of sunspot number (SSN), which have been measured regularly from 1749 to the present. In this study, we converted the SSN dataset, which consists of SSNs between 1749 – 2018, into a time series, and made the ten-year forecast with the help of deep-learning (DL) algorithms. Our results show that algorithms such as long-short-term memory (LSTM) and neural network autoregression (NNAR), which are DL algorithms, perform better than many algorithms such as ARIMA, Naive, Seasonal Naive, Mean and Drift, which are expressed as classical algorithms in a large time-series estimation process. Using the R programming language, it was also predicted that the maximum amplitude of Solar Cycle (SC) 25 will be reached between 2022 and 2023.}}, 
  pages = {50}, 
  number = {5}, 
  volume = {294}, 
  journal = {Solar Physics}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Pala,%20Atici_2019_Forecasting%20Sunspot%20Time%20Series%20Using%20Deep%20Learning%20Methods.pdf}, 
  year = {2019}
}
@article{undefined, 
  author = {}, 
  title = {{Pardalos\_2010\_Convex optimization theory.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Pardalos_2010_Convex%20optimization%20theory.pdf}
}
@article{10.1126/science.aaa8403, 
  author = {Parkes, David C. and Wellman, Michael P.}, 
  title = {{Economic reasoning and artificial intelligence}}, 
  issn = {0036-8075}, 
  doi = {10.1126/science.aaa8403}, 
  pmid = {26185245}, 
  abstract = {{The field of artificial intelligence (AI) strives to build rational agents capable of perceiving the world around them and taking actions to advance specified goals. Put another way, AI researchers aim to construct a synthetic homo economicus, the mythical perfectly rational agent of neoclassical economics. We review progress toward creating this new species of machine, machina economicus, and discuss some challenges in designing AIs that can reason effectively in economic contexts. Supposing that AI succeeds in this quest, or at least comes close enough that it is useful to think about AIs in rationalistic terms, we ask how to design the rules of interaction in multi-agent systems that come to represent an economy of AIs. Theories of normative design from economics may prove more relevant for artificial agents than human agents, with AIs that better respect idealized assumptions of rationality than people, interacting through novel rules and incentive systems quite distinct from those tailored for people.}}, 
  pages = {267--272}, 
  number = {6245}, 
  volume = {349}, 
  journal = {Science}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Parkes,%20Wellman_Unknown_Economic%20reasoning%20and%20artificial%20intelligence.pdf}, 
  year = {2015}
}
@article{undefined, 
  author = {}, 
  title = {{Psacharopoulos\_1995\_The Profitability of Investment in Education Concepts and Methods.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Psacharopoulos_1995_The%20Profitability%20of%20Investment%20in%20Education%20Concepts%20and%20Methods.pdf}
}
@article{undefined, 
  author = {}, 
  title = {{Poteliene, Tamasauskiene\_2013\_Human capital investment Measuring returns to education.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Poteliene,%20Tamasauskiene_2013_Human%20capital%20investment%20Measuring%20returns%20to%20education.pdf}
}
@article{undefined, 
  author = {}, 
  title = {{Peddinti, Povey, Khudanpur\_2015\_A time delay neural network architecture for efficient modeling of long temporal contexts.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Peddinti,%20Povey,%20Khudanpur_2015_A%20time%20delay%20neural%20network%20architecture%20for%20efficient%20modeling%20of%20long%20temporal%20contexts.pdf}
}
@article{10.1080/09645292.2018.1484426, 
  author = {Psacharopoulos, George and Patrinos, Harry Anthony}, 
  title = {{Returns to investment in education: a decennial review of the global literature}}, 
  issn = {0964-5292}, 
  doi = {10.1080/09645292.2018.1484426}, 
  abstract = {{In the 60-plus year history of returns to investment in education estimates, there have been several compilations in the literature. This paper updates Psacharopoulos and Patrinos and reviews the latest trends and patterns based on 1120 estimates in 139 countries from 1950 to 2014. The private average global return to a year of schooling is 9\% a year. Private returns to higher education increased, raising issues of financing and equity. Social returns to schooling remain high. Women continue to experience higher average returns to schooling, showing that girls’ education remains a priority.}}, 
  pages = {1--14}, 
  number = {5}, 
  volume = {26}, 
  journal = {Education Economics}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Psacharopoulos,%20Patrinos_2018_Returns%20to%20investment%20in%20education%20a%20decennial%20review%20of%20the%20global%20literature.pdf}, 
  year = {2018}
}
@article{undefined, 
  author = {}, 
  title = {{Publications, Sn, An - 2003 - Economic forecasting Models , indicators and 3.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Publications,%20Sn,%20An%20-%202003%20-%20Economic%20forecasting%20Models%20,%20indicators%20and%203.pdf}
}
@article{10.1080/09645299400000016, 
  author = {Psacharopoulos, George and Ng, Ying Chu}, 
  title = {{Earnings and Education in Latin America}}, 
  issn = {0964-5292}, 
  doi = {10.1080/09645299400000016}, 
  abstract = {{This paper uses household survey data for 18 Latin American countries to assess earnings differentials by level of education and how these differentials have changed during the 1980s. Introduction of the cost of education allows the estimation of private and social rates of return to investment in education across several dimensions: by education level, gender, sector of employment, nature of the secondary school curriculum and over time. The results show that, in most countries, the premium associated with higher education has decreased during the 1980s and that investment in primary education exhibits the highest rate of return among the levels considered.}}, 
  pages = {187--207}, 
  number = {2}, 
  volume = {2}, 
  journal = {Education Economics}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Psacharopoulos,%20Ng_1994_Earnings%20and%20education%20in%20Latin%20America%20EBSCOhost.pdf}, 
  year = {2006}
}
@article{undefined, 
  author = {}, 
  title = {{Rasmussen, Williams\_2006\_(Adaptive\_Computation\_and\_Machine\_Learning)Carl\_Edward\_Rasmussen,\_Christopher\_K.\_I.\_Williams-Gaussian\_Pro(2005).pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Rasmussen,%20Williams_2006_(Adaptive_Computation_and_Machine_Learning)Carl_Edward_Rasmussen,_Christopher_K._I._Williams-Gaussian_Pro(2005).pdf}
}
@article{undefined, 
  author = {}, 
  title = {{Qiu et al.\_2014\_Ensemble deep learning for regression and time series forecasting.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Qiu%20et%20al._2014_Ensemble%20deep%20learning%20for%20regression%20and%20time%20series%20forecasting.pdf}
}
@article{undefined, 
  author = {}, 
  title = {{Reid - Unknown - Using neural network to approximate macroeconomic forecast models for BRICS nations.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Reid%20-%20Unknown%20-%20Using%20neural%20network%20to%20approximate%20macroeconomic%20forecast%20models%20for%20BRICS%20nations.pdf}
}
@article{10.1016/j.eswa.2014.12.003, 
  author = {Rather, Akhter Mohiuddin and Agarwal, Arun and Sastry, V.N.}, 
  title = {{Recurrent neural network and a hybrid model for prediction of stock returns}}, 
  issn = {0957-4174}, 
  doi = {10.1016/j.eswa.2014.12.003}, 
  abstract = {{In this paper, we propose a robust and novel hybrid model for prediction of stock returns. The proposed model is constituted of two linear models: autoregressive moving average model, exponential smoothing model and a non-linear model: recurrent neural network. Training data for recurrent neural network is generated by a new regression model. Recurrent neural network produces satisfactory predictions as compared to linear models. With the goal to further improve the accuracy of predictions, the proposed hybrid prediction model merges predictions obtained from these three prediction based models. An optimization model is introduced which generates optimal weights for proposed model; the model is solved using genetic algorithms. The results confirm about the accuracy of the prediction performance of recurrent neural network. As expected, an outstanding prediction performance has been obtained from proposed hybrid prediction model as it outperforms recurrent neural network. The proposed model is certainly expected to be a promising approach in the field of prediction based models where data is non-linear, whose patterns are difficult to be captured by traditional models.}}, 
  pages = {3234--3241}, 
  number = {6}, 
  volume = {42}, 
  journal = {Expert Systems with Applications}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Rather,%20Agarwal,%20Sastry_2015_Recurrent%20neural%20network%20and%20a%20hybrid%20model%20for%20prediction%20of%20stock%20returns(2).pdf}, 
  year = {2015}
}
@article{undefined, 
  author = {}, 
  title = {{Roberts et al.\_2013\_Gaussian processes for time-series modelling.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Roberts%20et%20al._2013_Gaussian%20processes%20for%20time-series%20modelling.pdf}
}
@article{undefined, 
  author = {}, 
  title = {{Rigotti\_2012\_Second Welfare Theorem Preliminaries.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Rigotti_2012_Second%20Welfare%20Theorem%20Preliminaries.pdf}
}
@article{undefined, 
  author = {}, 
  title = {{Rodriguez, Urzua, Reyes\_2016\_Heterogeneous Economic Returns to Post-Secondary Degrees Evidence from Chile.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Rodriguez,%20Urzua,%20Reyes_2016_Heterogeneous%20Economic%20Returns%20to%20Post-Secondary%20Degrees%20Evidence%20from%20Chile.pdf}
}
@article{undefined, 
  author = {}, 
  title = {{Roth, Roth\_2015\_What Have We Learned from Market Design Published by Wiley on behalf of the Royal Economic Society Stable URL httpwww.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Roth,%20Roth_2015_What%20Have%20We%20Learned%20from%20Market%20Design%20Published%20by%20Wiley%20on%20behalf%20of%20the%20Royal%20Economic%20Society%20Stable%20URL%20httpwww.pdf}
}
@article{undefined, 
  author = {}, 
  title = {{Rojemr\_2012\_Proof of Envelope Theorem for constrained optimization problems.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Rojemr_2012_Proof%20of%20Envelope%20Theorem%20for%20constrained%20optimization%20problems.pdf}
}
@article{10.1162/089976600300015015, 
  author = {Gers, Felix A. and Schmidhuber, Jrgen and Cummins, Fred}, 
  title = {{Learning to Forget: Continual Prediction with LSTM}}, 
  issn = {0899-7667}, 
  doi = {10.1162/089976600300015015}, 
  pmid = {11032042}, 
  abstract = {{Long short-term memory (LSTM; Hochreiter \& Schmidhuber, 1997) can solve numerous tasks not solvable by previous learning algorithms for recurrent neural networks (RNNs). We identify a weakness of LSTM networks processing continual input streams that are not a priori segmented into subsequences with explicitly marked ends at which the network's internal state could be reset. Without resets, the state may grow indefinitely and eventually cause the network to break down. Our remedy is a novel, adaptive forget gate that enables an LSTM cell to learn to reset itself at appropriate times, thus releasing internal resources. We review illustrative benchmark problems on which standard LSTM outperforms other RNN algorithms. All algorithms (including LSTM) fail to solve continual versions of these problems. LSTM with forget gates, however, easily solves them, and in an elegant way.}}, 
  pages = {2451--2471}, 
  number = {10}, 
  volume = {12}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Rogers_1990_A%20Critique%20of%20Neoclassical%20Macroeconomics%20-%20Weeks,J.pdf}, 
  year = {2000}
}
@article{undefined, 
  author = {}, 
  title = {{Rubin, Rosenbaum\_2012\_The Central Role of the Propensity Score in Observational Studies for Causal Effects.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Rubin,%20Rosenbaum_2012_The%20Central%20Role%20of%20the%20Propensity%20Score%20in%20Observational%20Studies%20for%20Causal%20Effects.pdf}
}
@article{10.1016/j.jksuci.2015.06.002, 
  author = {Rout, Ajit Kumar and Dash, P.K. and Dash, Rajashree and Bisoi, Ranjeeta}, 
  title = {{Forecasting financial time series using a low complexity recurrent neural network and evolutionary learning approach}}, 
  issn = {1319-1578}, 
  doi = {10.1016/j.jksuci.2015.06.002}, 
  abstract = {{The paper presents a low complexity recurrent Functional Link Artificial Neural Network for predicting the financial time series data like the stock market indices over a time frame varying from 1day ahead to 1month ahead. Although different types of basis functions have been used for low complexity neural networks earlier for stock market prediction, a comparative study is needed to choose the optimal combinations of these for a reasonably accurate forecast. Further several evolutionary learning methods like the Particle Swarm Optimization (PSO) and modified version of its new variant (HMRPSO), and the Differential Evolution (DE) are adopted here to find the optimal weights for the recurrent computationally efficient functional link neural network (RCEFLANN) using a combination of linear and hyperbolic tangent basis functions. The performance of the recurrent computationally efficient FLANN model is compared with that of low complexity neural networks using the Trigonometric, Chebyshev, Laguerre, Legendre, and tangent hyperbolic basis functions in predicting stock prices of Bombay Stock Exchange data and Standard \& Poor’s 500 data sets using different evolutionary methods and has been presented in this paper and the results clearly reveal that the recurrent FLANN model trained with the DE outperforms all other FLANN models similarly trained.}}, 
  pages = {536--552}, 
  number = {4}, 
  volume = {29}, 
  journal = {Journal of King Saud University - Computer and Information Sciences}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Rout%20et%20al._2017_Forecasting%20financial%20time%20series%20using%20a%20low%20complexity%20recurrent%20neural%20network%20and%20evolutionary%20learning%20approach.pdf}, 
  year = {2017}
}
@article{undefined, 
  author = {}, 
  title = {{Rubinstein\_2006\_Rubinstein-2005-Microeconomics-LectureNotesText.pdf.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Rubinstein_2006_Rubinstein-2005-Microeconomics-LectureNotesText.pdf.pdf}
}
@article{undefined, 
  author = {Ruder, Sebastian}, 
  title = {{An overview of gradient descent optimization algorithms}}, 
  eprint = {1609.04747}, 
  abstract = {{Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Ruder_2016_An%20overview%20of%20gradient%20descent%20optimization%20algorithms.pdf}, 
  year = {2016}
}
@article{undefined, 
  author = {Ruder, Sebastian}, 
  title = {{An overview of gradient descent optimization algorithms}}, 
  eprint = {1609.04747}, 
  abstract = {{Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Ruder_2016_An%20overview%20of%20gradient%20descent%20optimization%20algorithms(2).pdf}, 
  year = {2016}
}
@article{10.1016/j.energy.2019.03.081, 
  author = {Sadaei, Hossein Javedani and Silva, Petrônio Cândido de Lima e and Guimarães, Frederico Gadelha and Lee, Muhammad Hisyam}, 
  title = {{Short-term load forecasting by using a combined method of convolutional neural networks and fuzzy time series}}, 
  issn = {0360-5442}, 
  doi = {10.1016/j.energy.2019.03.081}, 
  abstract = {{ We propose a combined method that is based on the fuzzy time series (FTS) and convolutional neural networks (CNN) for short-term load forecasting (STLF). Accordingly, in the proposed method, multivariate time series data which include hourly load data, hourly temperature time series and fuzzified version of load time series, was converted into multi-channel images to be fed to a proposed deep learning CNN model with proper architecture. By using images which have been created from the sequenced values of multivariate time series, the proposed CNN model could determine and extract related important parameters, in an implicit and automatic way, without any need for human interaction and expert knowledge, and all by itself. By following this strategy, it was shown how employing the proposed method is easier than some traditional STLF models. Therefore it could be seen as one of the big difference between the proposed method and some state-of-the-art methodologies of STLF. Moreover, using fuzzy logic had great contribution to control over-fitting by expressing one dimension of time series by a fuzzy space, in a spectrum, and a shadow instead of presenting it with exact numbers. Various experiments on test data-sets support the efficiency of the proposed method.}}, 
  pages = {365--377}, 
  number = {Proc IEEE 75 12 1987}, 
  volume = {175}, 
  journal = {Energy}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Sadaei%20et%20al._2019_Short-term%20load%20forecasting%20by%20using%20a%20combined%20method%20of%20convolutional%20neural%20networks%20and%20fuzzy%20time%20series.pdf}, 
  year = {2019}
}
@article{undefined, 
  author = {Ruder, Sebastian}, 
  title = {{An overview of gradient descent optimization algorithms}}, 
  eprint = {1609.04747}, 
  abstract = {{Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Ruder_2016_An%20overview%20of%20gradient%20descent%20optimization%20algorithms(3).pdf}, 
  year = {2016}
}
@article{10.1016/j.neucom.2018.09.082, 
  author = {Sagheer, Alaa and Kotb, Mostafa}, 
  title = {{Time Series Forecasting of Petroleum Production using Deep LSTM Recurrent Networks}}, 
  issn = {0925-2312}, 
  doi = {10.1016/j.neucom.2018.09.082}, 
  abstract = {{ Time series forecasting (TSF) is the task of predicting future values of a given sequence using historical data. Recently, this task has attracted the attention of researchers in the area of machine learning to address the limitations of traditional forecasting methods, which are time-consuming and full of complexity. With the increasing availability of extensive amounts of historical data along with the need of performing accurate production forecasting, particularly a powerful forecasting technique infers the stochastic dependency between past and future values is highly needed. In this paper, we propose a deep learning approach capable to address the limitations of traditional forecasting approaches and show accurate predictions. The proposed approach is a deep long-short term memory (DLSTM) architecture, as an extension of the traditional recurrent neural network. Genetic algorithm is applied in order to optimally configure DLSTM’s optimum architecture. For evaluation purpose, two case studies from the petroleum industry domain are carried out using the production data of two actual oilfields. Toward a fair evaluation, the performance of the proposed approach is compared with several standard methods, either statistical or soft computing. Using different measurement criteria, the empirical results show that the proposed DLSTM model outperforms other standard approaches.}}, 
  pages = {203--213}, 
  number = {Int. J. Forecast. 22 3 2006}, 
  volume = {323}, 
  journal = {Neurocomputing}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Sagheer,%20Kotb_2019_Time%20series%20forecasting%20of%20petroleum%20production%20using%20deep%20LSTM%20recurrent%20networks.pdf}, 
  year = {2018}
}
@article{undefined, 
  author = {}, 
  title = {{Salkind\_2012\_Principal Components Analysis.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Salkind_2012_Principal%20Components%20Analysis.pdf}
}
@article{10.1016/j.procs.2018.08.153, 
  author = {Salman, Afan Galih and Heryadi, Yaya and Abdurahman, Edi and Suparta, Wayan}, 
  title = {{Single Layer \& Multi-layer Long Short-Term Memory (LSTM) Model with Intermediate Variables for Weather Forecasting}}, 
  issn = {1877-0509}, 
  doi = {10.1016/j.procs.2018.08.153}, 
  abstract = {{ Weather forecasting has gained attention many researchers from various research communities due to its effect to the global human life. The emerging deep learning techniques in the last decade coupled and the wide availability of massive weather observation data have motivated many researches to explore hidden hierarchical pattern in the large volume of weather dataset for weather forecasting. The purposes of this research are to build a robust and adaptive statistical model for forecasting univariate weather variable in Indonesian airport area and to explore the effect of intermediate weather variable related to accuracy prediction using single layer Long Short Memory Model (LSTM) model and multi layers LSTM model. The proposed forecasting model is an extension of LSTM model by adding intermediate variable signal into LSTM memory block. The premise is that two highly related patterns in input dataset will rectify the input patterns so make it easier for the model to learn and recognize the pattern from the training dataset. In an effort to achieve a robust model for learning and recognizing weather pattern, this research will also explore various architectures such as single layer LSTM and Multiple Layer LSTM (4 layers LSTM). The dataset is weather variable data collected by Weather Underground at Hang Nadim Indonesia airport. This research used visibility as predicted data and temperature, pressure, humidity, dew point as intermediates data. The best model of LSTM in this experiment is multiple layers LSTM and the best intermediate data is pressure variable. Using the pressure variable this model has gained the validation accuracy 0.8060 and RMSE 0.0775.}}, 
  pages = {89--98}, 
  number = {IEEE Transactions on 30 2 2000}, 
  volume = {135}, 
  journal = {Procedia Computer Science}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Salman%20et%20al._2018_Single%20Layer%20&%20Multi-layer%20Long%20Short-Term%20Memory%20(LSTM)%20Model%20with%20Intermediate%20Variables%20for%20Weather%20Forecasting.pdf}, 
  year = {2018}
}
@article{10.1016/j.procs.2018.08.153, 
  author = {Salman, Afan Galih and Heryadi, Yaya and Abdurahman, Edi and Suparta, Wayan}, 
  title = {{Single Layer \& Multi-layer Long Short-Term Memory (LSTM) Model with Intermediate Variables for Weather Forecasting}}, 
  issn = {1877-0509}, 
  doi = {10.1016/j.procs.2018.08.153}, 
  abstract = {{ Weather forecasting has gained attention many researchers from various research communities due to its effect to the global human life. The emerging deep learning techniques in the last decade coupled and the wide availability of massive weather observation data have motivated many researches to explore hidden hierarchical pattern in the large volume of weather dataset for weather forecasting. The purposes of this research are to build a robust and adaptive statistical model for forecasting univariate weather variable in Indonesian airport area and to explore the effect of intermediate weather variable related to accuracy prediction using single layer Long Short Memory Model (LSTM) model and multi layers LSTM model. The proposed forecasting model is an extension of LSTM model by adding intermediate variable signal into LSTM memory block. The premise is that two highly related patterns in input dataset will rectify the input patterns so make it easier for the model to learn and recognize the pattern from the training dataset. In an effort to achieve a robust model for learning and recognizing weather pattern, this research will also explore various architectures such as single layer LSTM and Multiple Layer LSTM (4 layers LSTM). The dataset is weather variable data collected by Weather Underground at Hang Nadim Indonesia airport. This research used visibility as predicted data and temperature, pressure, humidity, dew point as intermediates data. The best model of LSTM in this experiment is multiple layers LSTM and the best intermediate data is pressure variable. Using the pressure variable this model has gained the validation accuracy 0.8060 and RMSE 0.0775.}}, 
  pages = {89--98}, 
  number = {IEEE Transactions on 30 2 2000}, 
  volume = {135}, 
  journal = {Procedia Computer Science}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Salman%20et%20al.%20-%202018%20-%20Single%20Layer%20&%20Multi-layer%20Long%20Short-Term%20Memory%20(LSTM)%20Model%20with%20Intermediate%20Variables%20for%20Weather%20Forecastin.pdf}, 
  year = {2018}
}
@article{undefined, 
  author = {Sambasivan, Rajiv and Das, Sourish}, 
  title = {{A Statistical Machine Learning Approach to Yield Curve Forecasting}}, 
  eprint = {1703.01536}, 
  abstract = {{Yield curve forecasting is an important problem in finance. In this work we explore the use of Gaussian Processes in conjunction with a dynamic modeling strategy, much like the Kalman Filter, to model the yield curve. Gaussian Processes have been successfully applied to model functional data in a variety of applications. A Gaussian Process is used to model the yield curve. The hyper-parameters of the Gaussian Process model are updated as the algorithm receives yield curve data. Yield curve data is typically available as a time series with a frequency of one day. We compare existing methods to forecast the yield curve with the proposed method. The results of this study showed that while a competing method (a multivariate time series method) performed well in forecasting the yields at the short term structure region of the yield curve, Gaussian Processes perform well in the medium and long term structure regions of the yield curve. Accuracy in the long term structure region of the yield curve has important practical implications. The Gaussian Process framework yields uncertainty and probability estimates directly in contrast to other competing methods. Analysts are frequently interested in this information. In this study the proposed method has been applied to yield curve forecasting, however it can be applied to model high frequency time series data or data streams in other domains.}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Sambasivan,%20Das_2018_A%20statistical%20machine%20learning%20approach%20to%20yield%20curve%20forecasting.pdf}, 
  year = {2017}
}
@article{10.1257/jep.28.1.209, 
  author = {Schwabish, Jonathan A}, 
  title = {{An Economist's Guide to Visualizing Data}}, 
  issn = {0895-3309}, 
  doi = {10.1257/jep.28.1.209}, 
  pages = {209--234}, 
  number = {1}, 
  volume = {28}, 
  journal = {Journal of Economic Perspectives}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Schwabish%20-%202014%20-%20An%20Economist's%20Guide%20to%20Visualizing%20Data.pdf}, 
  year = {2014}
}
@article{10.1109/icacci.2017.8126078, 
  author = {Selvin, Sreelekshmy and Vinayakumar, R and Gopalakrishnan, E.A and Menon, Vijay Krishna and Soman, K.P}, 
  title = {{Stock Price Prediction Using LSTM, Rnn and Cnn-Sliding Window Model}}, 
  doi = {10.1109/icacci.2017.8126078}, 
  abstract = {{Stock market or equity market have a profound impact in today's economy. A rise or fall in the share price has an important role in determining the investor's gain. The existing forecasting methods make use of both linear (AR, MA, ARIMA) and non-linear algorithms (ARCH, GARCH, Neural Networks), but they focus on predicting the stock index movement or price forecasting for a single company using the daily closing price. The proposed method is a model independent approach. Here we are not fitting the data to a specific model, rather we are identifying the latent dynamics existing in the data using deep learning architectures. In this work we use three different deep learning architectures for the price prediction of NSE listed companies and compares their performance. We are applying a sliding window approach for predicting future values on a short term basis. The performance of the models were Quantified using percentage error.}}, 
  pages = {1643--1647}, 
  journal = {2017 International Conference on Advances in Computing, Communications and Informatics (ICACCI)}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Selvin%20et%20al._2017_Stock%20price%20prediction%20using%20LSTM,%20RNN%20and%20CNN-sliding%20window%20model.pdf}, 
  year = {2017}
}
@article{undefined, 
  author = {}, 
  title = {{Shi, Chen, Wang\_2015\_Convolutional LSTM Network.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Shi,%20Chen,%20Wang_2015_Convolutional%20LSTM%20Network.pdf}
}
@article{undefined, 
  author = {}, 
  title = {{Sena, Nagwani\_2016\_A Neural Network Autoregression Model for Forecast Per Capita Disposable Income.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Sena,%20Nagwani_2016_A%20Neural%20Network%20Autoregression%20Model%20for%20Forecast%20Per%20Capita%20Disposable%20Income.pdf}
}
@article{undefined, 
  author = {}, 
  title = {{Siami-Namini, Namin - 2018 - Forecasting Economics and Financial Time Series ARIMA vs. LSTM.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Siami-Namini,%20Namin%20-%202018%20-%20Forecasting%20Economics%20and%20Financial%20Time%20Series%20ARIMA%20vs.%20LSTM.pdf}
}
@article{undefined, 
  author = {}, 
  title = {{Siami-Namini, Namin\_2018\_Forecasting Economics and Financial Time Series ARIMA vs. LSTM.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Siami-Namini,%20Namin_2018_Forecasting%20Economics%20and%20Financial%20Time%20Series%20ARIMA%20vs.%20LSTM.pdf}
}
@article{undefined, 
  author = {}, 
  title = {{Silva\_Unknown\_Causal Inference in Machine Learning Part I Did you have breakfast today.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Silva_Unknown_Causal%20Inference%20in%20Machine%20Learning%20Part%20I%20Did%20you%20have%20breakfast%20today.pdf}
}
@article{undefined, 
  author = {Smith, Leslie N}, 
  title = {{A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, momentum, and weight decay}}, 
  eprint = {1803.09820}, 
  abstract = {{Although deep learning has produced dazzling successes for applications of image, speech, and video processing in the past few years, most trainings are with suboptimal hyper-parameters, requiring unnecessarily long training times. Setting the hyper-parameters remains a black art that requires years of experience to acquire. This report proposes several efficient ways to set the hyper-parameters that significantly reduce training time and improves performance. Specifically, this report shows how to examine the training validation/test loss function for subtle clues of underfitting and overfitting and suggests guidelines for moving toward the optimal balance point. Then it discusses how to increase/decrease the learning rate/momentum to speed up training. Our experiments show that it is crucial to balance every manner of regularization for each dataset and architecture. Weight decay is used as a sample regularizer to show how its optimal value is tightly coupled with the learning rates and momentums. Files to help replicate the results reported here are available.}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Smith_2018_A%20disciplined%20approach%20to%20neural%20network%20hyper-parameters%20Part%201%20--%20learning%20rate,%20batch%20size,%20momentum,%20and%20weight%20decay.pdf}, 
  year = {2018}
}
@article{undefined, 
  author = {}, 
  title = {{Srivastava et al.\_2014\_Dropout A Simple Way to Prevent Neural Networks from Overfittin.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Srivastava%20et%20al._2014_Dropout%20A%20Simple%20Way%20to%20Prevent%20Neural%20Networks%20from%20Overfittin.pdf}
}
@article{10.1016/j.chb.2016.08.014, 
  author = {Sokolov-Mladenović, Svetlana and Milovančević, Milos and Mladenović, Igor and Alizamir, Meysam}, 
  title = {{Economic growth forecasting by artificial neural network with extreme learning machine based on trade, import and export parameters}}, 
  issn = {0747-5632}, 
  doi = {10.1016/j.chb.2016.08.014}, 
  abstract = {{Economic growth may be developed based on trade, imports and exports parameters. The main goal in this study was to predict the economic growth based on trade in services, exports of goods and services, imports of goods and services, trade and merchandise trade on the economic growth. Gross domestic product (GDP) was used as economic growth indicator. The main purpose of this research is to develop and apply the artificial neural network (ANN) with back propagation learning (BP) algorithm and with extreme learning machine (ELM) in order predict GDP growth rate. The aim was to compare the results of BP and ELM prediction accuracy for the GDP growth rate prediction based on the trade data. Based on results, it was demonstrated that ELM can be utilized effectively in applications of GDP growth rate forecasting.}}, 
  pages = {43--45}, 
  volume = {65}, 
  journal = {Computers in Human Behavior}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Sokolov-Mladenović%20et%20al._2016_Economic%20growth%20forecasting%20by%20artificial%20neural%20network%20with%20extreme%20learning%20machine%20based%20on%20trade,%20i.pdf}, 
  year = {2016}
}
@article{10.1080/09645292.2016.1150418, 
  author = {Stevenson, Adam}, 
  title = {{The returns to quality in graduate education}}, 
  issn = {0964-5292}, 
  doi = {10.1080/09645292.2016.1150418}, 
  abstract = {{This paper estimates the monetary return to quality in US graduate education, controlling for cognitive ability and self-selection across award level, program quality, and field-of-study. In most program types, I cannot reject the hypothesis of no returns to either degree completion or program quality. Important exceptions include master's programs in health science, where completion substantially increases earnings, and in MBA and professional degree programs, where program quality has a positive influence on earnings. I explore the job characteristics that predict greater earnings among students with tertiary education, and I estimate the returns to quality in terms of non-monetary job benefits.}}, 
  pages = {1--20}, 
  number = {5}, 
  volume = {24}, 
  journal = {Education Economics}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Stevenson_2016_The%20returns%20to%20quality%20in%20graduate%20education.pdf}, 
  year = {2016}
}
@article{10.1111/caje.12336, 
  author = {Swanson, Norman R. and Xiong, Weiqi}, 
  title = {{Big data analytics in economics: What have we learned so far, and where should we go from here?}}, 
  issn = {0008-4085}, 
  doi = {10.1111/caje.12336}, 
  abstract = {{Research into predictive accuracy testing remains at the forefront of the forecasting field. One reason for this is that rankings of predictive accuracy across alternative models, which under misspecification are loss function dependent, are universally utilized to assess the usefulness of econometric models. A second reason, which corresponds to the objective of this paper, is that researchers are currently focusing considerable attention on so‐called big data and on new (and old) tools that are available for the analysis of this data. One of the objectives in this field is the assessment of whether big data leads to improvement in forecast accuracy. In this survey paper, we discuss some of the latest (and most interesting) methods currently available for analyzing and utilizing big data when the objective is improved prediction. Our discussion includes a summary of various so‐called dimension reduction, shrinkage and machine learning methods as well as a summary of recent tools that are useful for ranking prediction models associated with the implementation of these methods. We also provide a brief empirical illustration of big data in action, in which we show that big data are indeed useful when predicting the term structure of interest rates. L’analyse des données massives en économie : ce qu’on a appris jusqu’à maintenant et quelles directions pour la prochaine étape? Les travaux sur les tests de précision des prévisions demeurent au premier plan dans le monde de la prévision. Une première raison est que les ordonnancements de la précision des prévisions des divers modèles (qui selon le degré de mis‐spécification dépend de la fonction de perte) sont universellement utilisés pour calibrer l’utilité des modèles économétriques. Une seconde raison, qui correspond à l’objectif de ce mémoire, est que les chercheurs concentrent une portion considérable de leur attention sur ce qu’on appelle les données massives, et les outils (anciens et nouveaux) disponibles pour analyser ce type de données. Un des objectifs dans ce champ d’études est d’établir si les données massives mènent à l’amélioration dans la précision des prévisions. Dans cette étude‐synthèse, les auteurs examinent quelques‐unes des méthodes les plus récentes et les plus intéressantes qui sont disponibles pour analyser et utiliser ce genre de données quand l’objectif est d’améliorer les prévisions. La discussion inclut une présentation succincte d’approches en termes de réduction de dimensions, de rétrécissement, et méthodes d’apprentissage machine, ainsi qu’un résumé succinct d’outillages récents qui sont utiles pour ordonnancer les modèles de prévision associés à la mise en application de ces méthodes. On fournit aussi une brève illustration de données massives en action, dans laquelle on montre que l’analyse des données massives est utile pour prévoir la structure par échéance des taux d’intérêt.}}, 
  pages = {695--746}, 
  number = {3}, 
  volume = {51}, 
  journal = {Canadian Journal of Economics/Revue canadienne d'économique}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Swanson,%20Xiong_2018_Big%20data%20analytics%20in%20economics%20What%20have%20we%20learned%20so%20far,%20and%20where%20should%20we%20go%20from%20here.pdf}, 
  year = {2019}
}
@article{undefined, 
  author = {}, 
  title = {{TAN\_2008\_THE FIRST FUNDAMENTAL THEOREM OF WELFARE ECONOMICS KEGON.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/TAN_2008_THE%20FIRST%20FUNDAMENTAL%20THEOREM%20OF%20WELFARE%20ECONOMICS%20KEGON.pdf}
}
@article{10.1109/icdmw.2016.0078, 
  author = {Tang, Yujin and Xu, Jianfeng and Matsumoto, Kazunori and Ono, Chihiro}, 
  title = {{Sequence-To-Sequence Model with Attention for Time Series Classification}}, 
  doi = {10.1109/icdmw.2016.0078}, 
  abstract = {{Encouraged by recent waves of successful applications of deep learning, some researchers have demonstrated the effectiveness of applying convolutional neural networks (CNN) to time series classification problems. However, CNN and other traditional methods require the input data to be of the same dimension which prevents its direct application on data of various lengths and multi-channel time series with different sampling rates across channels. Long short-term memory (LSTM), another tool in the deep learning arsenal and with its design nature, is more appropriate for problems involving time series such as speech recognition and language translation. In this paper, we propose a novel model incorporating a sequence-to-sequence model that consists two LSTMs, one encoder and one decoder. The encoder LSTM accepts input time series of arbitrary lengths, extracts information from the raw data and based on which the decoder LSTM constructs fixed length sequences that can be regarded as discriminatory features. For better utilization of the raw data, we also introduce the attention mechanism into our model so that the feature generation process can peek at the raw data and focus its attention on the part of the raw data that is most relevant to the feature under construction. We call our model S2SwA, as the short for Sequence-to-Sequence with Attention. We test S2SwA on both uni-channel and multi-channel time series datasets and show that our model is competitive with the state-of-the-art in real world tasks such as human activity recognition.}}, 
  pages = {503--510}, 
  journal = {2016 IEEE 16th International Conference on Data Mining Workshops (ICDMW)}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Tang%20et%20al._2017_Sequence-To-Sequence%20Model%20with%20Attention%20for%20Time%20Series%20Classification.pdf}, 
  year = {2016}
}
@article{10.1111/j.1468-2354.2009.00572.x, 
  author = {Sönmez, Tayfun and Ünver, M. Utku}, 
  title = {{COURSE BIDDING AT BUSINESS SCHOOLS*}}, 
  issn = {1468-2354}, 
  doi = {10.1111/j.1468-2354.2009.00572.x}, 
  abstract = {{Mechanisms that rely on course bidding are widely used at business schools in order to allocate seats at oversubscribed courses. Bids play two key roles under these mechanisms: to infer student preferences and to determine who have bigger claims on course seats. We show that these two roles may easily conflict, and preferences induced from bids may significantly differ from the true preferences. Therefore, these mechanisms, which are promoted as market mechanisms, do not necessarily yield market outcomes. We introduce a Pareto-dominant market mechanism that can be implemented by asking students for their preferences in addition to their bids over courses.}}, 
  pages = {99--123}, 
  number = {1}, 
  volume = {51}, 
  journal = {International Economic Review}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Tayfun,%20Utku_2010_COURSE%20BIDDING%20AT%20BUSINESS%20SCHOOLS.pdf}, 
  year = {2010}
}
@article{undefined, 
  author = {}, 
  title = {{Thiesing, Vornberger - Unknown - Forecasting Sales Using Neural Networks.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Thiesing,%20Vornberger%20-%20Unknown%20-%20Forecasting%20Sales%20Using%20Neural%20Networks.pdf}
}
@article{undefined, 
  author = {}, 
  title = {{Tesfatsion\_2017\_Modeling economic systems as locally-constructive sequential games.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Tesfatsion_2017_Modeling%20economic%20systems%20as%20locally-constructive%20sequential%20games.pdf}
}
@article{10.1016/j.fcij.2017.05.001, 
  author = {Tealab, Ahmed and Hefny, Hesham and Badr, Amr}, 
  title = {{Forecasting of nonlinear time series using ANN}}, 
  issn = {2314-7288}, 
  doi = {10.1016/j.fcij.2017.05.001}, 
  abstract = {{When forecasting time series, it is important to classify them according linearity behavior that the linear time series remains at the forefront of academic and applied research, it has often been found that simple linear time series models usually leave certain aspects of economic and financial data unexplained. The dynamic behavior of most of the time series in our real life with its autoregressive and inherited moving average terms issue the challenge to forecast nonlinear times series that contain inherited moving average terms using computational intelligence methodologies such as neural networks. It is rare to find studies that concentrate on forecasting nonlinear times series that contain moving average terms. In this study, we demonstrate that the common neural networks are not efficient for recognizing the behavior of nonlinear or dynamic time series which has moving average terms and hence low forecasting capability. This leads to the importance of formulating new models of neural networks such as Deep Learning neural networks with or without hybrid methodologies such as Fuzzy Logic.}}, 
  pages = {39--47}, 
  number = {1}, 
  volume = {2}, 
  journal = {Future Computing and Informatics Journal}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Tealab,%20Hefny,%20Badr_2017_Forecasting%20of%20nonlinear%20time%20series%20using%20ANN.pdf}, 
  year = {2017}
}
@article{10.1109/tnn.2005.860885, 
  author = {Tsai, Jinn-Tsong and Chou, Jyh-Horng and Liu, Tung-Kuan}, 
  title = {{Tuning the Structure and Parameters of a Neural Network by Using Hybrid Taguchi-Genetic Algorithm}}, 
  issn = {1045-9227}, 
  doi = {10.1109/tnn.2005.860885}, 
  pmid = {16526477}, 
  abstract = {{In this paper, a hybrid Taguchi-genetic algorithm (HTGA) is applied to solve the problem of tuning both network structure and parameters of a feedforward neural network. The HTGA approach is a method of combining the traditional genetic algorithm (TGA), which has a powerful global exploration capability, with the Taguchi method, which can exploit the optimum offspring. The Taguchi method is inserted between crossover and mutation operations of a TGA. Then, the systematic reasoning ability of the Taguchi method is incorporated in the crossover operations to select the better genes to achieve crossover, and consequently enhance the genetic algorithms. Therefore, the HTGA approach can be more robust, statistically sound, and quickly convergent. First, the authors evaluate the performance of the presented HTGA approach by studying some global numerical optimization problems. Then, the presented HTGA approach is effectively applied to solve three examples on forecasting the sunspot numbers, tuning the associative memory, and solving the XOR problem. The numbers of hidden nodes and the links of the feedforward neural network are chosen by increasing them from small numbers until the learning performance is good enough. As a result, a partially connected feedforward neural network can be obtained after tuning. This implies that the cost of implementation of the neural network can be reduced. In these studied problems of tuning both network structure and parameters of a feedforward neural network, there are many parameters and numerous local optima so that these studied problems are challenging enough for evaluating the performances of any proposed GA-based approaches. The computational experiments show that the presented HTGA approach can obtain better results than the existing method reported recently in the literature.}}, 
  pages = {69--80}, 
  number = {1}, 
  volume = {17}, 
  journal = {IEEE Transactions on Neural Networks}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Tsai,%20Chou,%20Liu_2006_Tuning%20the%20Structure%20and%20Parameters%20of%20a%20Neural%20Network%20by%20Using%20Hybrid%20Taguchi-Genetic%20Algorithm.pdf}, 
  year = {2006}
}
@article{10.1016/j.physa.2018.08.047, 
  author = {Tümer, Abdullah Erdal and Akkuş, Aytekin}, 
  title = {{Forecasting Gross Domestic Product per capita using artificial neural networks with non-economical parameters}}, 
  issn = {0378-4371}, 
  doi = {10.1016/j.physa.2018.08.047}, 
  abstract = {{ Gross Domestic Product per capita is one of the most important indicators of social welfare. All countries try to increase their Gross Domestic Product per capita to contribute to their population’s happiness and well-being, as well as strengthen their nation’s standing in international relations. Economic growth is affected by economic parameters such as trade, import, and export. However, Gross Domestic Product may also be affected by non-economic factors. Therefore, for a country to increase its Gross Domestic Product per capita, it’s important to employ the correct strategy. The aim of this study is to investigate the predictability of Gross Domestic Product per capita based on non-economic data by using artificial neural network with feed forward back-propagation learning algorithm. For this purpose, neural network models have been developed with different architectures. Education level, number of published academic paper per capita, number of researchers per employed, percentage of Research and Development expenditure in the Gross Domestic Product and number of patents per capita are used as input data in the models. The input data has been collected from variety of resources such as Organisation for Economic Co-operation and Development. A comparison between the model results and actual data give a high correlation coefficient (R 2 = 0 .96) and show that the model is able to predict the Gross Domestic Product per capita from non-economic parameters.}}, 
  pages = {468--473}, 
  number = {Int. J. Soc. Econ. 33 2006}, 
  volume = {512}, 
  journal = {Physica A: Statistical Mechanics and its Applications}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Tümer,%20Akkuş_2018_Forecasting%20Gross%20Domestic%20Product%20per%20Capita%20Using%20Artificial%20Neural%20Networks%20with%20Non-Economical%20Parameters.pdf}, 
  year = {2018}
}
@article{undefined, 
  author = {}, 
  title = {{Turner\_2014\_Gaussian processes approximations for time series.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Turner_2014_Gaussian%20processes%20approximations%20for%20time%20series.pdf}
}
@article{undefined, 
  author = {}, 
  title = {{Unknown\_Unknown\_ENVELOPE THEOREMS FOR ARBITRARY CHOICE SETS.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Unknown_Unknown_ENVELOPE%20THEOREMS%20FOR%20ARBITRARY%20CHOICE%20SETS.pdf}
}
@article{10.1016/j.jhydrol.2012.11.017, 
  author = {Valipour, Mohammad and Banihabib, Mohammad Ebrahim and Behbahani, Seyyed Mahmood Reza}, 
  title = {{Comparison of the ARMA, ARIMA, and the autoregressive artificial neural network models in forecasting the monthly inflow of Dez dam reservoir}}, 
  issn = {0022-1694}, 
  doi = {10.1016/j.jhydrol.2012.11.017}, 
  abstract = {{The goal of the present research is forecasting the inflow of Dez dam reservoir by using Auto Regressive Moving Average (ARMA) and Auto Regressive Integrated Moving Average (ARIMA) models while increasing the number of parameters in order to increase the forecast accuracy to four parameters and comparing them with the static and dynamic artificial neural networks. In this research, monthly discharges from 1960 to 2007 were used. The statistics related to first 42years were used to train the models and the 5 past years were used to forecast. In ARMA and ARIMA models, the polynomial was derived respectively with four and six parameters to forecast the inflow. In the artificial neural network, the radial and sigmoid activity functions were used with several different neurons in the hidden layers. By comparing root mean square error (RMSE) and mean bias error (MBE), dynamic artificial neural network model with sigmoid activity function and 17 neurons in the hidden layer was chosen as the best model for forecasting inflow of the Dez dam reservoir. Inflow of the dam reservoir in the 12 past months shows that ARIMA model had a less error compared with the ARMA model. Static and Dynamic autoregressive artificial neural networks with activity sigmoid function can forecast the inflow to the dam reservoirs from the past 60months.}}, 
  pages = {433--441}, 
  volume = {476}, 
  journal = {Journal of Hydrology}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Valipour,%20Banihabib,%20Behbahani_2013_Comparison%20of%20the%20ARMA,%20ARIMA,%20and%20the%20autoregressive%20artificial%20neural%20network%20models%20in%20forecastin.pdf}, 
  year = {2013}
}
@article{undefined, 
  author = {}, 
  title = {{Vulić, Moens\_2015\_Monolingual and Cross-Lingual Information Retrieval Models Based on (Bilingual) Word Embeddings.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Vulić,%20Moens_2015_Monolingual%20and%20Cross-Lingual%20Information%20Retrieval%20Models%20Based%20on%20(Bilingual)%20Word%20Embeddings.pdf}
}
@article{10.1257/jep.28.2.3, 
  author = {Varian, Hal R}, 
  title = {{Big Data: New Tricks for Econometrics}}, 
  issn = {0895-3309}, 
  doi = {10.1257/jep.28.2.3}, 
  pages = {3--28}, 
  number = {2}, 
  volume = {28}, 
  journal = {Journal of Economic Perspectives}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Varian_2014_Big%20Data%20New%20Tricks%20for%20Econometrics.pdf}, 
  year = {2014}
}
@article{10.1016/j.labeco.2018.05.005, 
  author = {Walker, Ian and Zhu, Yu}, 
  title = {{University Selectivity and the Relative Returns to Higher Education: Evidence from the UK}}, 
  issn = {0927-5371}, 
  doi = {10.1016/j.labeco.2018.05.005}, 
  abstract = {{ We study the wage outcomes of university graduates by course (i.e. by subject and institution) using the UK Labour Force Surveys (LFS). We that the selectivity of undergraduate degree programmes plays an important role in explaining the variation in the relative graduate wages. In fact, we find that much of the variation in relative wages across courses is due to the quality of students selected. Once we allow for course selectivity in our analysis we find that our estimates of the effects of attending the most prestigious HEIs is around 10 percentage points lower than otherwise; the effects of attending the middle ranking HEIs is around 5 percentage points lower; and that of attending these lowest ranking HEIs is unaffected. We go on to consider selection (on observables) into subjects and institutions using the Inverse Probability Weighted Regression Adjusted (IPWRA) method to estimate multiple treatment effects.}}, 
  pages = {230--249}, 
  number = {Annual Review of Economics 4 2012}, 
  volume = {53}, 
  journal = {Labour Economics}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Walker,%20Zhu_2018_University%20selectivity%20and%20the%20relative%20returns%20to%20higher%20education%20Evidence%20from%20the%20UK.pdf}, 
  year = {2018}
}
@article{undefined, 
  author = {}, 
  title = {{Weiss\_2008\_An Introduction to Set Theory.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Weiss_2008_An%20Introduction%20to%20Set%20Theory.pdf}
}
@article{undefined, 
  author = {}, 
  title = {{Wei - 2008 - Editorial Comment.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Wei%20-%202008%20-%20Editorial%20Comment.pdf}
}
@article{undefined, 
  author = {}, 
  title = {{Willis, Rosen\_1979\_Education and Self-Selection.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Willis,%20Rosen_1979_Education%20and%20Self-Selection.pdf}
}
@article{undefined, 
  author = {Wilson, Andrew Gordon and Hu, Zhiting and Salakhutdinov, Ruslan and Xing, Eric P}, 
  title = {{Deep Kernel Learning}}, 
  eprint = {1511.02222}, 
  abstract = {{We introduce scalable deep kernels, which combine the structural properties of deep learning architectures with the non-parametric flexibility of kernel methods. Specifically, we transform the inputs of a spectral mixture base kernel with a deep architecture, using local kernel interpolation, inducing points, and structure exploiting (Kronecker and Toeplitz) algebra for a scalable kernel representation. These closed-form kernels can be used as drop-in replacements for standard kernels, with benefits in expressive power and scalability. We jointly learn the properties of these kernels through the marginal likelihood of a Gaussian process. Inference and learning cost \$O(n)\$ for \$n\$ training points, and predictions cost \$O(1)\$ per test point. On a large and diverse collection of applications, including a dataset with 2 million examples, we show improved performance over scalable Gaussian processes with flexible kernel learning models, and stand-alone deep architectures.}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Wilson%20et%20al._2015_Deep%20Kernel%20Learning.pdf}, 
  year = {2015}
}
@article{undefined, 
  author = {}, 
  title = {{Wilson\_2012\_Manifolds The Definition of a Manifold and First Examples.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Wilson_2012_Manifolds%20The%20Definition%20of%20a%20Manifold%20and%20First%20Examples.pdf}
}
@article{10.1177/0081175012452652, 
  author = {Xie, Yu and Brand, Jennie E. and Jann, Ben}, 
  title = {{Estimating Heterogeneous Treatment Effects with Observational Data}}, 
  issn = {0081-1750}, 
  doi = {10.1177/0081175012452652}, 
  pmid = {23482633}, 
  abstract = {{Individuals differ not only in their background characteristics but also in how they respond to a particular treatment, intervention, or stimulation. In particular, treatment effects may vary systematically by the propensity for treatment. In this paper, we discuss a practical approach to studying heterogeneous treatment effects as a function of the treatment propensity, under the same assumption commonly underlying regression analysis: ignorability. We describe one parametric method and two nonparametric methods for estimating interactions between treatment and the propensity for treatment. For the first method, we begin by estimating propensity scores for the probability of treatment given a set of observed covariates for each unit and construct balanced propensity score strata; we then estimate propensity score stratum-specific average treatment effects and evaluate a trend across them. For the second method, we match control units to treated units based on the propensity score and transform the data into treatment-control comparisons at the most elementary level at which such comparisons can be constructed; we then estimate treatment effects as a function of the propensity score by fitting a nonparametric model as a smoothing device. For the third method, we first estimate nonparametric regressions of the outcome variable as a function of the propensity score separately for treated units and for control units and then take the difference between the two nonparametric regressions. We illustrate the application of these methods with an empirical example of the effects of college attendance on women’s fertility.}}, 
  pages = {314--347}, 
  number = {1}, 
  volume = {42}, 
  journal = {Sociological Methodology}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Xie,%20Brand,%20Jann_2012_Estimating%20Heterogeneous%20Treatment%20Effects%20with%20Observational%20Data.pdf}, 
  year = {2012}
}
@article{undefined, 
  author = {Yarotsky, Dmitry}, 
  title = {{Universal approximations of invariant maps by neural networks}}, 
  eprint = {1804.10306}, 
  abstract = {{We describe generalizations of the universal approximation theorem for neural networks to maps invariant or equivariant with respect to linear representations of groups. Our goal is to establish network-like computational models that are both invariant/equivariant and provably complete in the sense of their ability to approximate any continuous invariant/equivariant map. Our contribution is three-fold. First, in the general case of compact groups we propose a construction of a complete invariant/equivariant network using an intermediate polynomial layer. We invoke classical theorems of Hilbert and Weyl to justify and simplify this construction; in particular, we describe an explicit complete ansatz for approximation of permutation-invariant maps. Second, we consider groups of translations and prove several versions of the universal approximation theorem for convolutional networks in the limit of continuous signals on euclidean spaces. Finally, we consider 2D signal transformations equivariant with respect to the group SE(2) of rigid euclidean motions. In this case we introduce the "charge--conserving convnet" -- a convnet-like computational model based on the decomposition of the feature space into isotypic representations of SO(2). We prove this model to be a universal approximator for continuous SE(2)--equivariant signal transformations.}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Yarotsky_2018_Universal%20approximations%20of%20invariant%20maps%20by%20neural%20networks.pdf}, 
  year = {2018}
}
@article{10.1016/j.asoc.2006.01.003, 
  author = {Yadav, R.N. and Kalra, P.K. and John, J.}, 
  title = {{Time series prediction with single multiplicative neuron model}}, 
  issn = {1568-4946}, 
  doi = {10.1016/j.asoc.2006.01.003}, 
  abstract = {{Single neuron models are typical functional replica of the biological neuron that are derived using their individual and group responses in networks. In recent past, a lot of work in this area has produced advanced neuron models for both analog and binary data patterns. Popular among these are the higher-order neurons, fuzzy neurons and other polynomial neurons. In this paper, we propose a new neuron model based on a polynomial architecture. Instead of considering all the higher-order terms, a simple aggregation function is used. The aggregation function is considered as a product of linear functions in different dimensions of the space. The functional mapping capability of the proposed neuron model is demonstrated through some well known time series prediction problems and is compared with the standard multilayer neural network.}}, 
  pages = {1157--1163}, 
  number = {4}, 
  volume = {7}, 
  journal = {Applied Soft Computing}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Yadav,%20Kalra,%20John_2007_Time%20series%20prediction%20with%20single%20multiplicative%20neuron%20model.pdf}, 
  year = {2007}
}
@article{undefined, 
  author = {}, 
  title = {{Zandt\_2002\_An Introduction to Monotone Comparative Statics.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Zandt_2002_An%20Introduction%20to%20Monotone%20Comparative%20Statics.pdf}
}
@article{undefined, 
  author = {Yi, Subin and Ju, Janghoon and Yoon, Man-Ki and Choi, Jaesik}, 
  title = {{Grouped Convolutional Neural Networks for Multivariate Time Series}}, 
  eprint = {1703.09938}, 
  abstract = {{Analyzing multivariate time series data is important for many applications such as automated control, fault diagnosis and anomaly detection. One of the key challenges is to learn latent features automatically from dynamically changing multivariate input. In visual recognition tasks, convolutional neural networks (CNNs) have been successful to learn generalized feature extractors with shared parameters over the spatial domain. However, when high-dimensional multivariate time series is given, designing an appropriate CNN model structure becomes challenging because the kernels may need to be extended through the full dimension of the input volume. To address this issue, we present two structure learning algorithms for deep CNN models. Our algorithms exploit the covariance structure over multiple time series to partition input volume into groups. The first algorithm learns the group CNN structures explicitly by clustering individual input sequences. The second algorithm learns the group CNN structures implicitly from the error backpropagation. In experiments with two real-world datasets, we demonstrate that our group CNNs outperform existing CNN based regression methods.}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Yi%20et%20al.%20-%202017%20-%20Grouped%20Convolutional%20Neural%20Networks%20for%20Multivariate%20Time%20Series.pdf}, 
  year = {2017}
}
@article{undefined, 
  author = {}, 
  title = {{Zhang et al.\_2019\_Short-term forecasting and uncertainty analysis of wind turbine power based on long short-term memory network and Gaus.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Zhang%20et%20al._2019_Short-term%20forecasting%20and%20uncertainty%20analysis%20of%20wind%20turbine%20power%20based%20on%20long%20short-term%20memory%20network%20and%20Gaus.pdf}
}
@article{10.1016/s0169-2070(97)00044-7, 
  author = {Zhang, Guoqiang and Patuwo, B Eddy and Hu, Michael Y}, 
  title = {{Forecasting with artificial neural networks:}}, 
  issn = {0169-2070}, 
  doi = {10.1016/s0169-2070(97)00044-7}, 
  abstract = {{Interest in using artificial neural networks (ANNs) for forecasting has led to a tremendous surge in research activities in the past decade. While ANNs provide a great deal of promise, they also embody much uncertainty. Researchers to date are still not certain about the effect of key factors on forecasting performance of ANNs. This paper presents a state-of-the-art survey of ANN applications in forecasting. Our purpose is to provide (1) a synthesis of published research in this area, (2) insights on ANN modeling issues, and (3) the future research directions.}}, 
  pages = {35--62}, 
  number = {1}, 
  volume = {14}, 
  journal = {International Journal of Forecasting}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Zhang,%20Patuwo,%20Hu_1998_Forecasting%20with%20artificial%20neural%20networks%20The%20state%20of%20the%20art.pdf}, 
  year = {1998}
}
@article{10.1049/iet-its.2016.0208, 
  author = {Zhao, Zheng and Chen, Weihai and Wu, Xingming and Chen, Peter C Y and Liu, Jingmeng}, 
  title = {{LSTM network: a deep learning approach for short-term traffic forecast}}, 
  issn = {1751-956X}, 
  doi = {10.1049/iet-its.2016.0208}, 
  pages = {68--75}, 
  number = {2}, 
  volume = {11}, 
  journal = {IET Intelligent Transport Systems}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Zhao%20et%20al._2017_LSTM%20network%20a%20deep%20learning%20approach%20for%20short-term%20traffic%20forecast.pdf}, 
  year = {2017}
}
@article{10.1016/j.eswa.2016.09.027, 
  author = {Zhong, Xiao and Enke, David}, 
  title = {{Forecasting daily stock market return using dimensionality reduction}}, 
  issn = {0957-4174}, 
  doi = {10.1016/j.eswa.2016.09.027}, 
  abstract = {{In financial markets, it is both important and challenging to forecast the daily direction of the stock market return. Among the few studies that focus on predicting daily stock market returns, the data mining procedures utilized are either incomplete or inefficient, especially when a large amount of features are involved. This paper presents a complete and efficient data mining process to forecast the daily direction of the S\&P 500 Index ETF (SPY) return based on 60 financial and economic features. Three mature dimensionality reduction techniques, including principal component analysis (PCA), fuzzy robust principal component analysis (FRPCA), and kernel-based principal component analysis (KPCA) are applied to the whole data set to simplify and rearrange the original data structure. Corresponding to different levels of the dimensionality reduction, twelve new data sets are generated from the entire cleaned data using each of the three different dimensionality reduction methods. Artificial neural networks (ANNs) are then used with the thirty-six transformed data sets for classification to forecast the daily direction of future market returns. Moreover, the three different dimensionality reduction methods are compared with respect to the natural data set. A group of hypothesis tests are then performed over the classification and simulation results to show that combining the ANNs with the PCA gives slightly higher classification accuracy than the other two combinations, and that the trading strategies guided by the comprehensive classification mining procedures based on PCA and ANNs gain significantly higher risk-adjusted profits than the comparison benchmarks, while also being slightly higher than those strategies guided by the forecasts based on the FRPCA and KPCA models.}}, 
  pages = {126--139}, 
  volume = {67}, 
  journal = {Expert Systems with Applications}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Zhong,%20Enke%20-%202017%20-%20Forecasting%20daily%20stock%20market%20return%20using%20dimensionality%20reduction%204.pdf}, 
  year = {2017}
}
@article{10.1007/s00500-017-2624-5, 
  author = {Zhang, Bing and Wu, Jhen-Long and Chang, Pei-Chann}, 
  title = {{A multiple time series-based recurrent neural network for short-term load forecasting}}, 
  issn = {1432-7643}, 
  doi = {10.1007/s00500-017-2624-5}, 
  abstract = {{Electricity, an indispensable resource in daily life and industrial production, is hard to store, so accurate short-term load forecasting (STLF) plays a vital role in resource allocation, capital budgeting of power companies, energy deployment and government control. In recent decades, the strong dependency relationships of time series have been considered in many researches, but the discrete information has not proven to be very useful in their experiments. In general, while discrete information is weak, it can provide macro trends compared to the micro trends of continuous information. In this research, we aim to combine macro and micro information by continuous and discrete time series to generate multiple time series (MTS). The MTS comprise four information sequences: short-term, cycle, long short-term and cross-long short-term. These MTS are used to build a STLF system using a recurrent neural network (RNN) model that can learn sequential information between continuous and discrete series. Therefore, the RNN model with MTS information can improve the forecasting performance for short-term load forecasting. The experimental results show that our proposed forecasting system outperforms the state-of-the-art approach.}}, 
  pages = {4099--4112}, 
  number = {12}, 
  volume = {22}, 
  journal = {Soft Computing}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Zhang,%20Wu,%20Chang_2018_A%20multiple%20time%20series-based%20recurrent%20neural%20network%20for%20short-term%20load%20forecasting.pdf}, 
  year = {2018}
}
@article{10.1257/aer.20160696, 
  author = {Acemoglu, Daron and Restrepo, Pascual}, 
  title = {{The Race between Man and Machine: Implications of Technology for Growth, Factor Shares, and Employment}}, 
  issn = {0002-8282}, 
  doi = {10.1257/aer.20160696}, 
  pages = {1488--1542}, 
  number = {6}, 
  volume = {108}, 
  journal = {American Economic Review}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Acemoglu_2017_The%20Race%20Between%20Man%20and%20Machine%20Implications%20of%20Technology%20for%20Growth,%20Factor%20Shares%20and%20Employment.pdf}, 
  year = {2018}
}
@article{10.1016/j.econlet.2011.05.028, 
  author = {Zhu, Rong}, 
  title = {{Individual heterogeneity in returns to education in urban China during 1995–2002}}, 
  issn = {0165-1765}, 
  doi = {10.1016/j.econlet.2011.05.028}, 
  abstract = {{This paper analyzes the individual-level differences in returns to education in urban China from 1995 to 2002. We have uncovered substantial heterogeneity in schooling coefficients, and we find that the extent of heterogeneity diminishes over time.}}, 
  pages = {84--87}, 
  number = {1}, 
  volume = {113}, 
  journal = {Economics Letters}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Zhu_2011_Individual%20heterogeneity%20in%20returns%20to%20education%20in%20urban%20China%20during%201995-2002.pdf}, 
  year = {2011}
}
@article{undefined, 
  author = {Dumoulin, Vincent and Visin, Francesco}, 
  title = {{A guide to convolution arithmetic for deep learning}}, 
  eprint = {1603.07285}, 
  abstract = {{We introduce a guide to help deep learning practitioners understand and manipulate convolutional neural network architectures. The guide clarifies the relationship between various properties (input shape, kernel shape, zero padding, strides and output shape) of convolutional, pooling and transposed convolutional layers, as well as the relationship between convolutional and transposed convolutional layers. Relationships are derived for various cases, and are illustrated in order to make them intuitive.}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Dumoulin-2016-Unknown.pdf}, 
  year = {2016}
}
@article{10.1109/tnnls.2017.2709324, 
  author = {Godfrey, Luke B and Gashler, Michael S}, 
  title = {{Neural Decomposition of Time-Series Data for Effective Generalization}}, 
  issn = {2162-237X}, 
  doi = {10.1109/tnnls.2017.2709324}, 
  pmid = {28650827}, 
  eprint = {1705.09137}, 
  abstract = {{We present a neural network technique for the analysis and extrapolation of time-series data called neural decomposition (ND). Units with a sinusoidal activation function are used to perform a Fourier-like decomposition of training samples into a sum of sinusoids, augmented by units with nonperiodic activation functions to capture linear trends and other nonperiodic components. We show how careful weight initialization can be combined with regularization to form a simple model that generalizes well. Our method generalizes effectively on the Mackey-Glass series, a data set of unemployment rates as reported by the U.S. Department of Labor Statistics, a time-series of monthly international airline passengers, the monthly ozone concentration in downtown Los Angeles, and an unevenly sampled time series of oxygen isotope measurements from a cave in north India. We find that ND outperforms popular time-series forecasting techniques, including long short-term memory network, echo-state networks, autoregressive integrated moving average (ARIMA), seasonal ARIMA, support vector regression with a radial basis function, and Gashler and Ashmore's model.}}, 
  pages = {1--13}, 
  number = {7}, 
  volume = {29}, 
  journal = {IEEE Transactions on Neural Networks and Learning Systems}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Godfrey-2017-Ieee%20T%20Neur%20Net%20Lear.pdf}, 
  year = {2017}
}
@article{10.1016/j.dss.2009.02.001, 
  author = {Lu, Chi-Jie and Lee, Tian-Shyug and Chiu, Chih-Chou}, 
  title = {{Financial time series forecasting using independent component analysis and support vector regression}}, 
  issn = {0167-9236}, 
  doi = {10.1016/j.dss.2009.02.001}, 
  abstract = {{As financial time series are inherently noisy and non-stationary, it is regarded as one of the most challenging applications of time series forecasting. Due to the advantages of generalization capability in obtaining a unique solution, support vector regression (SVR) has also been successfully applied in financial time series forecasting. In the modeling of financial time series using SVR, one of the key problems is the inherent high noise. Thus, detecting and removing the noise are important but difficult tasks when building an SVR forecasting model. To alleviate the influence of noise, a two-stage modeling approach using independent component analysis (ICA) and support vector regression is proposed in financial time series forecasting. ICA is a novel statistical signal processing technique that was originally proposed to find the latent source signals from observed mixture signals without having any prior knowledge of the mixing mechanism. The proposed approach first uses ICA to the forecasting variables for generating the independent components (ICs). After identifying and removing the ICs containing the noise, the rest of the ICs are then used to reconstruct the forecasting variables which contain less noise and served as the input variables of the SVR forecasting model. In order to evaluate the performance of the proposed approach, the Nikkei 225 opening index and TAIEX closing index are used as illustrative examples. Experimental results show that the proposed model outperforms the SVR model with non-filtered forecasting variables and a random walk model.}}, 
  pages = {115--125}, 
  number = {2}, 
  volume = {47}, 
  journal = {Decision Support Systems}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Lu-2009-Decis%20Support%20Syst.pdf}, 
  year = {2009}
}
@article{undefined, 
  author = {Doshi-Velez, Finale and Kim, Been}, 
  title = {{Towards A Rigorous Science of Interpretable Machine Learning}}, 
  eprint = {1702.08608}, 
  abstract = {{As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Doshi-Velez-2017-Unknown.pdf}, 
  year = {2017}
}
@article{undefined, 
  author = {Shachter, Ross D.}, 
  title = {{Bayes-Ball:
The
Rational
Pastime
(for
Determining
Irrelev
ance
and
Requisite
Information
in Belief
Net
works
and
Infl
uence
Diagrams)}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Shachter-1998-Unknown.pdf}, 
  year = {1998}
}
@article{undefined, 
  author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew}, 
  title = {{Going Deeper with Convolutions}}, 
  eprint = {1409.4842}, 
  abstract = {{We propose a deep convolutional neural network architecture codenamed "Inception", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Szegedy-2014-Unknown.pdf}, 
  year = {2014}
}
@article{undefined, 
  author = {Oord, Aaron van den and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray}, 
  title = {{WaveNet: A Generative Model for Raw Audio}}, 
  eprint = {1609.03499}, 
  abstract = {{This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Oord-2016-Unknown.pdf}, 
  year = {2016}
}
@phdthesis{undefined, 
  title = {{Conditional time series forecasting with convolutional neural networks}}, 
  author = {Borovykh, Anastasia and Bohte, Sander and Oosterlee, Cornelis W.}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Borovykh-2018-Unknown.pdf}, 
  year = {2018}
}
@inproceedings{undefined, 
  author = {Yu, Fisher and Koltun, Vladlen}, 
  title = {{Multi-scale Context Aggregation by Dilated convolutions}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Unknown-Unknown-Unknown.pdf}, 
  year = {2016}
}
@article{undefined, 
  author = {Maclaurin, Dougal and Duvenaud, David and Adams, Ryan P}, 
  title = {{Gradient-based Hyperparameter Optimization through Reversible Learning}}, 
  eprint = {1502.03492}, 
  abstract = {{Tuning hyperparameters of learning algorithms is hard because gradients are usually unavailable. We compute exact gradients of cross-validation performance with respect to all hyperparameters by chaining derivatives backwards through the entire training procedure. These gradients allow us to optimize thousands of hyperparameters, including step-size and momentum schedules, weight initialization distributions, richly parameterized regularization schemes, and neural network architectures. We compute hyperparameter gradients by exactly reversing the dynamics of stochastic gradient descent with momentum.}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Maclaurin-2015-Unknown.pdf}, 
  year = {2015}
}
@article{Xiu2019, 
  author = {Ke, Zheng Tracy and Kelly, Bryan T. and Xiu, Dacheng}, 
  title = {{Predicting Returns with Text Data}}, 
  doi = {10.2139/ssrn.3489226}, 
  abstract = {{We introduce a new text-mining methodology that extracts sentiment information from news articles to predict asset returns. Unlike more common sentiment scores used for stock return prediction (e.g., those sold by commercial vendors or built with dictionary-based methods), our supervised learning framework constructs a sentiment score that is specifically adapted to the problem of return prediction. Our method proceeds in three steps: 1) isolating a list of sentiment terms via predictive screening, 2) assigning sentiment weights to these words via topic modeling, and 3) aggregating terms into an article-level sentiment score via penalized likelihood. We derive theoretical guarantees on the accuracy of estimates from our model with minimal assumptions. In our empirical analysis, we text-mine one of the most actively monitored streams of news articles in the financial system — the Dow Jones Newswires — and show that our supervised sentiment model excels at extracting return-predictive signals in this context.}}, 
  journal = {SSRN Electronic Journal}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Ke-2019-Ssrn%20Electron%20J.pdf}, 
  year = {2019}
}
@article{10.1007/978-3-319-68612-7, 
  title = {{Lecture Notes in Computer Science}}, 
  issn = {0302-9743}, 
  doi = {10.1007/978-3-319-68612-7}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Unknown-2017-Unknown.pdf}, 
  year = {2017}
}
@article{10.1016/s0925-2312(01)00702-0, 
  author = {Zhang, G.Peter}, 
  title = {{Time series forecasting using a hybrid ARIMA and neural network model}}, 
  issn = {0925-2312}, 
  doi = {10.1016/s0925-2312(01)00702-0}, 
  abstract = {{Autoregressive integrated moving average (ARIMA) is one of the popular linear models in time series forecasting during the past three decades. Recent research activities in forecasting with artificial neural networks (ANNs) suggest that ANNs can be a promising alternative to the traditional linear methods. ARIMA models and ANNs are often compared with mixed conclusions in terms of the superiority in forecasting performance. In this paper, a hybrid methodology that combines both ARIMA and ANN models is proposed to take advantage of the unique strength of ARIMA and ANN models in linear and nonlinear modeling. Experimental results with real data sets indicate that the combined model can be an effective way to improve forecasting accuracy achieved by either of the models used separately.}}, 
  pages = {159--175}, 
  volume = {50}, 
  journal = {Neurocomputing}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Zhang-2003-Neurocomputing.pdf}, 
  year = {2003}
}
@article{10.1007/0-387-36276-2, 
  author = {Shumway, Robert H. and Stoffer, David S.}, 
  title = {{Time Series Analysis and Its Applications, With R Examples}}, 
  doi = {10.1007/0-387-36276-2}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Shumway-2006-Unknown.pdf}, 
  year = {2006}
}
@misc{NBER2020, 
  author = {Research, National Bureau of Economic}, 
  title = {{ US Business Cycle Expansions and Contractions}}, 
  url = {http://www.nber.org/cycles/cyclesmain.html}, 
  urldate = {2020-02-02}
}
@article{Smirnov1939, 
  author = {Smirnov, Nikolai}, 
  title = {{On the estimation of the discrepancy between empirical curves of distribution for two independent samples}}, 
  pages = {3--16}, 
  volume = {2}, 
  journal = {Bulletin Moscow University}, 
  year = {1939}
}
@article{Hodges1957, 
  author = {Hodges, J L}, 
  title = {{The significance probability of the smirnov two-sample test}}, 
  issn = {0004-2080}, 
  doi = {10.1007/bf02589501}, 
  pages = {469--486}, 
  number = {5}, 
  volume = {3}, 
  journal = {Arkiv för Matematik}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Hodges-1958-Arkiv%20För%20Matematik.pdf}, 
  year = {1958}
}
@article{Hess1981, 
  author = {Gibbons, Michael R and Hess, Patrick}, 
  title = {{Day of the Week Effects and Asset Returns}}, 
  issn = {0021-9398}, 
  doi = {10.1086/296147}, 
  abstract = {{A traditional distributional assumption regarding the returns on a financial asset specifies that the expected returns are identical for all days of the week. Contrary to this plausible assumption, this paper discovers that the expected returns on common stocks and treasury bills are not constant across days of the week. The most notable evidence is for Monday's returns where the mean is unusually low or even negative. Several explanations of the results are investigated, but none proves satisfactory. Aside from documenting significant day of the week effects, the implications of the results for tests of market efficiency are examined. While market-adjusted returns continue to exhibit day of the week effects, these effects are no longer concentrated on Monday}}, 
  pages = {579}, 
  number = {4}, 
  volume = {54}, 
  journal = {The Journal of Business}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Unknown-Unknown-Unknown_6.pdf}, 
  year = {1981}
}
@article{undefined, 
  author = {Ioffe, Sergey and Szegedy, Christian}, 
  title = {{Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}}, 
  eprint = {1502.03167}, 
  abstract = {{Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Ioffe-2015-Unknown.pdf}, 
  year = {2015}
}
@article{undefined, 
  author = {Alexopoulos, Michelle and Cohen, Jon}, 
  title = {{The Effects of Computer 
Technologies on the Canadian 
Economy: Evidence from New 
Direct Measures}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Alexopoulos-Unknown-Unknown.pdf}
}
@article{undefined, 
  author = {Akhtar, Shumi and Faff, Robert and Oliver, Barry and Subrahmanyam, Avanidhar}, 
  title = {{Reprint of: Stock salience and the asymmetric market effect of consumer
sentiment news}}, 
  journal = {Journal of Banking \& Finance}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Akhtar-2013-Journal%20of%20Banking%20&%20Finance.pdf}, 
  year = {2013}
}
@article{undefined, 
  author = {Esuli, Andrea and Sebastiani, Fabrizio}, 
  title = {{SENTIWORDNET: A Publicly Available Lexical Resourcefor Opinion Mining}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Esuli-Unknown-Unknown.pdf}
}
@inproceedings{undefined, 
  author = {Baccianella, Stefano and Esuli, Andrea and Sebastiani, Fabrizio}, 
  title = {{SENTIWORDNET 3.0 An enhanced lexical resource for sentiment analysis and opinion mining.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Unknown-Unknown-Unknown_1.pdf}, 
  year = {2010}
}
@article{10.1016/s0378-4371(02)00985-8, 
  author = {Alvarez-Ramirez, Jose and Cisneros, Myriam and Ibarra-Valdez, Carlos and Soriano, Angel}, 
  title = {{Multifractal Hurst analysis of crude oil prices}}, 
  issn = {0378-4371}, 
  doi = {10.1016/s0378-4371(02)00985-8}, 
  abstract = {{Daily records of international crude oil prices are studied using multifractal analysis methods. Rescaled range Hurst analysis provides evidence that the crude oil market is a persistent process with long-run memory effects. On the other hand, height–height correlation analysis reveals evidence of multifractal structures in the sense that the crude oil dynamics displays mixing of (rough) Hurst exponents. The existence of two characteristic time scales in the order of weeks and quarters is discovered and the corresponding prices dynamics are extracted using moving-average-based filtering. These results seem to demonstrate that the crude oil market is consistent with the random-walk assumption only at time scales of the order of days to weeks. A plausible oil price formation mechanism is discussed in terms of the market dynamics at three different time scales.}}, 
  pages = {651--670}, 
  number = {3-4}, 
  volume = {313}, 
  journal = {Physica A: Statistical Mechanics and its Applications}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Alvarez-Ramirez-2002-Phys%20Statistical%20Mech%20Appl.pdf}, 
  year = {2002}
}
@article{10.1111/j.1540-6261.2006.00885.x, 
  author = {BAKER, MALCOLM and WURGLER, JEFFREY}, 
  title = {{Investor Sentiment and the Cross-Section of Stock Returns}}, 
  issn = {0022-1082}, 
  doi = {10.1111/j.1540-6261.2006.00885.x}, 
  abstract = {{We study how investor sentiment affects the cross-section of stock returns. We predict that a wave of investor sentiment has larger effects on securities whose valuations are highly subjective and difficult to arbitrage. Consistent with this prediction, we find that when beginning-of-period proxies for sentiment are low, subsequent returns are relatively high for small stocks, young stocks, high volatility stocks, unprofitable stocks, non-dividend-paying stocks, extreme growth stocks, and distressed stocks. When sentiment is high, on the other hand, these categories of stock earn relatively low subsequent returns.}}, 
  pages = {1645--1680}, 
  number = {4}, 
  volume = {61}, 
  journal = {The Journal of Finance}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/BAKER-2006-J%20Finance.pdf}, 
  year = {2006}
}
@article{10.1080/07350015.2014.949342, 
  author = {Baumeister, Christiane and Kilian, Lutz}, 
  title = {{Forecasting the Real Price of Oil in a Changing World: A Forecast Combination Approach}}, 
  issn = {0735-0015}, 
  doi = {10.1080/07350015.2014.949342}, 
  pages = {338--351}, 
  number = {3}, 
  volume = {33}, 
  journal = {Journal of Business \& Economic Statistics}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Baumeister-2015-J%20Bus%20Econ%20Stat.pdf}, 
  year = {2015}
}
@article{10.2139/ssrn.3232721, 
  author = {Bianchi, Daniele and Büchner, Matthias and Tamoni, Andrea}, 
  title = {{Bond Risk Premia with Machine Learning}}, 
  doi = {10.2139/ssrn.3232721}, 
  abstract = {{We propose, compare, and evaluate a variety of machine learning methods for bond return predictability in the context of regression-based forecasting and contribute to a growing literature that aims to understand the usefulness of machine learning in empirical asset pricing. The main results show that non-linear methods can be highly useful for the out-of-sample prediction of bond excess returns compared to benchmarking data compression techniques such as linear principal component regressions. Also, the empirical evidence show that macroeconomic information has substantial incremental out-of-sample forecasting power for bond excess returns across maturities, especially when complex non-linear features are introduced via ensembled deep neural networks.}}, 
  journal = {SSRN Electronic Journal}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Bianchi-2018-Ssrn%20Electron%20J.pdf}, 
  year = {2018}
}
@article{undefined, 
  author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.}, 
  title = {{Latent Dirichlet Allocation}}, 
  journal = {Journal of Machine Learning Research}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Unknown-Unknown-Unknown_2.pdf}, 
  year = {2003}
}
@article{10.1016/j.jeconom.2005.01.027, 
  author = {Boivin, Jean and Ng, Serena}, 
  title = {{Are more data always better for factor analysis?}}, 
  issn = {0304-4076}, 
  doi = {10.1016/j.jeconom.2005.01.027}, 
  abstract = {{Factors estimated from large macroeconomic panels are being used in an increasing number of applications. However, little is known about how the size and the composition of the data affect the factor estimates. In this paper, we question whether it is possible to use more series to extract the factors, and yet the resulting factors are less useful for forecasting, and the answer is yes. Such a problem tends to arise when the idiosyncratic errors are cross-correlated. It can also arise if forecasting power is provided by a factor that is dominant in a small dataset but is a dominated factor in a larger dataset. In a real time forecasting exercise, we find that factors extracted from as few as 40 pre-screened series often yield satisfactory or even better results than using all 147 series. Weighting the data by their properties when constructing the factors also lead to improved forecasts. Our simulation analysis is unique in that special attention is paid to cross-correlated idiosyncratic errors, and we also allow the factors to have stronger loadings on some groups of series than others. It thus allows us to better understand the properties of the principal components estimator in empirical applications.}}, 
  pages = {169--194}, 
  number = {1}, 
  volume = {132}, 
  journal = {Journal of Econometrics}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Boivin-2006-J%20Econometrics.pdf}, 
  year = {2006}
}
@article{10.2139/ssrn.3446225, 
  author = {Bybee, Leland and Kelly, Bryan T and Manela, Asaf and Xiu, Dacheng}, 
  title = {{The Structure of Economic News}}, 
  doi = {10.2139/ssrn.3446225}, 
  abstract = {{We propose an approach to measuring the state of the economy via textual analysis of business news. From the full text content of 800,000 Wall Street Journal articles for 1984–2017, we estimate a topic model that summarizes business news as easily interpretable topical themes and quantifies the proportion of news attention allocated to each theme at each point in time. We then use our news attention estimates as inputs into statistical models of numerical economic time series. We demonstrate that these text-based inputs accurately track a wide range of economic activity measures and that they have incremental forecasting power for macroeconomic outcomes, above and beyond standard numerical predictors. Finally, we use our model to retrieve the news-based narratives that underly “shocks” in numerical economic data.}}, 
  journal = {SSRN Electronic Journal}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Bybee-2019-Ssrn%20Electron%20J.pdf}, 
  year = {2019}
}
@article{10.1016/j.jempfin.2019.01.007, 
  author = {Brandt, Michael W and Gao, Lin}, 
  title = {{Macro fundamentals or geopolitical events? A textual analysis of news events for crude oil}}, 
  issn = {0927-5398}, 
  doi = {10.1016/j.jempfin.2019.01.007}, 
  abstract = {{ News about macroeconomic fundamentals and geopolitical events affect crude oil markets differently. Using sentiment scores for a broad set of global news of different types, we find that news related to macro fundamentals have an impact on the oil price in the short run and significantly predict oil returns in the long run. Geopolitical news have a much stronger immediate impact but exhibit no predictability. Moreover, geopolitical news generate more uncertainty and greater trading volume, consistent with a disagreement explanation, while macroeconomic news are associated with subsequent lower trading volume. Finally, we find that news sentiment tracks the statistical releases quite well and can partially predict the future realizations of the economic data.}}, 
  pages = {64--94}, 
  number = {J. Finance 59 3 2004}, 
  volume = {51}, 
  journal = {Journal of Empirical Finance}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Brandt-2019-J%20Empir%20Financ.pdf}, 
  year = {2019}
}
@article{undefined, 
  author = {Chen, Luyang and Pelger, Markus and Zhu, Jason}, 
  title = {{Deep Learning in Asset Pricing}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Unknown-Unknown-Unknown_3.pdf}, 
  year = {2019}
}
@article{Bermingham2015, 
  author = {Deeney, Peter and Cummins, Mark and Dowling, Michael and Bermingham, Adam}, 
  title = {{Sentiment in oil markets}}, 
  issn = {1057-5219}, 
  doi = {10.1016/j.irfa.2015.01.005}, 
  abstract = {{Sentiment is shown to influence both West Texas Intermediate (WTI) and Brent futures prices during the period 2002–2013. This is demonstrated while controlling for stock indices, exchange rates, financial costs, inventory and supply levels as well as OPEC activity. Sentiment indices are developed for WTI and Brent crude oils using a suite of financial proxies similar to those used in equity research where the influence of sentiment has already been established. Given the novel nature of this study, multiple hypothesis testing techniques are used to ensure that these conclusions are statistically robust.}}, 
  pages = {179--185}, 
  volume = {39}, 
  journal = {International Review of Financial Analysis}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Deeney-2015-Int%20Rev%20Financ%20Anal.pdf}, 
  year = {2015}
}
@article{10.2139/ssrn.2705559, 
  author = {Choi, Yong Seok and Doshi, Hitesh and Jacobs, Kris and Turnbull, Stuart M}, 
  title = {{No-Arbitrage Modeling of Structured Products with Economic Determinants}}, 
  doi = {10.2139/ssrn.2705559}, 
  abstract = {{We introduce a top-down no-arbitrage model for pricing structured products. The losses are described by Cox processes whose intensities depend on economic variables. The model provides economic insight into the impact of structured products on the risk exposure of financial institutions and systemic risk. We estimate the model using CDO data and find that spreads decrease with higher interest rates, and increase with volatility and leverage. Volatility is the primary determinant of variation in tranche spreads, but leverage and interest rates are more closely associated with rare credit events. Model-implied risk premiums and the probabilities of tranche losses increase substantially during the financial crisis.}}, 
  journal = {SSRN Electronic Journal}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Choi-2015-Ssrn%20Electron%20J.pdf}, 
  year = {2015}
}
@article{10.1257/jep.30.4.171, 
  author = {Donaldson, Dave and Storeygard, Adam}, 
  title = {{The View from Above: Applications of Satellite Data in Economics}}, 
  issn = {0895-3309}, 
  doi = {10.1257/jep.30.4.171}, 
  pages = {171--198}, 
  number = {4}, 
  volume = {30}, 
  journal = {Journal of Economic Perspectives}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Donaldson-2016-J%20Econ%20Perspect.pdf}, 
  year = {2016}
}
@article{undefined, 
  author = {̈uttel, Dirk Eddelb and McCurdy, Thomas H.}, 
  title = {{The Impact of News on Foreign Exchange Rates:
Evidence from High Frequency Data}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/̈uttel-1998-Unknown.pdf}, 
  year = {1998}
}
@article{undefined, 
  author = {Diebold, Francis X. and Li, Canlin}, 
  title = {{Forecasting The Term Structure of Government Bond Yields}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Unknown-Unknown-Unknown_4.pdf}, 
  year = {2003}
}
@misc{undefined, 
  author = {Ensor, Katherine B. and Han, Yu and Ostdiek, Barbara and Turnbull, Stuart M.}, 
  title = {{Dynamic Jump Intensities and News Arrival in Oil Futures Markets}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Ensor-2019-Unknown.pdf}, 
  year = {2019}
}
@article{10.1016/j.ejor.2017.11.054, 
  author = {Fischer, Thomas and Krauss, Christopher}, 
  title = {{Deep learning with long short-term memory networks for financial market predictions}}, 
  issn = {0377-2217}, 
  doi = {10.1016/j.ejor.2017.11.054}, 
  abstract = {{ Long short-term memory (LSTM) networks are a state-of-the-art technique for sequence learning. They are less commonly applied to financial time series predictions, yet inherently suitable for this domain. We deploy LSTM networks for predicting out-of-sample directional movements for the constituent stocks of the S\&P 500 from 1992 until 2015. With daily returns of 0.46 percent and a Sharpe ratio of 5.8 prior to transaction costs, we find LSTM networks to outperform memory-free classification methods, i.e., a random forest (RAF), a deep neural net (DNN), and a logistic regression classifier (LOG). The outperformance relative to the general market is very clear from 1992 to 2009, but as of 2010, excess returns seem to have been arbitraged away with LSTM profitability fluctuating around zero after transaction costs. We further unveil sources of profitability, thereby shedding light into the black box of artificial neural networks. Specifically, we find one common pattern among the stocks selected for trading – they exhibit high volatility and a short-term reversal return profile. Leveraging these findings, we are able to formalize a rules-based short-term reversal strategy that yields 0.23 percent prior to transaction costs. Further regression analysis unveils low exposure of the LSTM returns to common sources of systematic risk – also compared to the three benchmark models.}}, 
  pages = {654--669}, 
  number = {Expert Systems with Applications 36 3 2009}, 
  volume = {270}, 
  journal = {European Journal of Operational Research}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Fischer-2017-Eur%20J%20Oper%20Res.pdf}, 
  year = {2017}
}
@phdthesis{undefined, 
  title = {{Empirical Asset Pricing via Machine Learning}}, 
  author = {Gu, Shihao and Kelly, Bryan and Xiu, Dacheng}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Gu-2021-Unknown.pdf}, 
  year = {2021}
}
@phdthesis{undefined, 
  title = {{Autoencoder Asset Pricing Models}}, 
  author = {Gu, Shihao and Kelly, Bryan and Xiu, Dacheng}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Gu-2030-Unknown.pdf}, 
  year = {2030}
}
@techreport{undefined, 
  author = {Hafez, Peter and Lautizi, Francesco and Guerrero
-
Colón, Jose A. and Fran
cisco
Gomez and Gomez, Maria and Matas, Ricard}, 
  title = {{Machine Learning \& Event 
Detection for Trading 
Energy Futures}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Hafez-Unknown-Unknown_1.pdf}
}
@techreport{undefined, 
  author = {Hafez, Peter Ager}, 
  title = {{Construction of Market Sentiment Indices 
Using News Sentiment}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Hafez-Unknown-Unknown.pdf}
}
@article{undefined, 
  author = {hamilton, James D.}, 
  title = {{Understanding crude oil prices}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/hamilton-2008-Unknown.pdf}, 
  year = {2008}
}
@article{undefined, 
  author = {Hamilton, James D. and Wu, Jing Cynthia}, 
  title = {{RISK PREMIA IN CRUDE OIL FUTURES PRICES}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Hamilton-2013-Unknown.pdf}, 
  year = {2013}
}
@misc{Xiu2014, 
  author = {Ke, Zheng Tracy and Kelly, Brian and Xiu, Dacheng}, 
  title = {{Predicting Returns with Text Data}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Ke-2014-Unknown.pdf}, 
  year = {2014}
}
@article{10.1016/j.eswa.2014.08.004, 
  author = {Nassirtoussi, Arman Khadjeh and Aghabozorgi, Saeed and Wah, Teh Ying and Ngo, David Chek Ling}, 
  title = {{Text mining of news-headlines for FOREX market prediction: A Multi-layer Dimension Reduction Algorithm with semantics and sentiment}}, 
  issn = {0957-4174}, 
  doi = {10.1016/j.eswa.2014.08.004}, 
  abstract = {{In this paper a novel approach is proposed to predict intraday directional-movements of a currency-pair in the foreign exchange market based on the text of breaking financial news-headlines. The motivation behind this work is twofold: First, although market-prediction through text-mining is shown to be a promising area of work in the literature, the text-mining approaches utilized in it at this stage are not much beyond basic ones as it is still an emerging field. This work is an effort to put more emphasis on the text-mining methods and tackle some specific aspects thereof that are weak in previous works, namely: the problem of high dimensionality as well as the problem of ignoring sentiment and semantics in dealing with textual language. This research assumes that addressing these aspects of text-mining have an impact on the quality of the achieved results. The proposed system proves this assumption to be right. The second part of the motivation is to research a specific market, namely, the foreign exchange market, which seems not to have been researched in the previous works based on predictive text-mining. Therefore, results of this work also successfully demonstrate a predictive relationship between this specific market-type and the textual data of news. Besides the above two main components of the motivation, there are other specific aspects that make the setup of the proposed system and the conducted experiment unique, for example, the use of news article-headlines only and not news article-bodies, which enables usage of short pieces of text rather than long ones; or the use of general financial breaking news without any further filtration.In order to accomplish the above, this work produces a multi-layer algorithm that tackles each of the mentioned aspects of the text-mining problem at a designated layer. The first layer is termed the Semantic Abstraction Layer and addresses the problem of co-reference in text mining that is contributing to sparsity. Co-reference occurs when two or more words in a text corpus refer to the same concept. This work produces a custom approach by the name of Heuristic-Hypernyms Feature-Selection which creates a way to recognize words with the same parent-word to be regarded as one entity. As a result, prediction accuracy increases significantly at this layer which is attributed to appropriate noise-reduction from the feature-space.The second layer is termed Sentiment Integration Layer, which integrates sentiment analysis capability into the algorithm by proposing a sentiment weight by the name of SumScore that reflects investors’ sentiment. Additionally, this layer reduces the dimensions by eliminating those that are of zero value in terms of sentiment and thereby improves prediction accuracy.The third layer encompasses a dynamic model creation algorithm, termed Synchronous Targeted Feature Reduction (STFR). It is suitable for the challenge at hand whereby the mining of a stream of text is concerned. It updates the models with the most recent information available and, more importantly, it ensures that the dimensions are reduced to the absolute minimum.The algorithm and each of its layers are extensively evaluated using real market data and news content across multiple years and have proven to be solid and superior to any other comparable solution. The proposed techniques implemented in the system, result in significantly high directional-accuracies of up to 83.33\%.On top of a well-rounded multifaceted algorithm, this work contributes a much needed research framework for this context with a test-bed of data that must make future research endeavors more convenient. The produced algorithm is scalable and its modular design allows improvement in each of its layers in future research. This paper provides ample details to reproduce the entire system and the conducted experiments.}}, 
  pages = {306--324}, 
  number = {1}, 
  volume = {42}, 
  journal = {Expert Systems with Applications}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Nassirtoussi-2015-Expert%20Syst%20Appl.pdf}, 
  year = {2015}
}
@article{undefined, 
  author = {Khaidem, Luckyson and Saha, Snehanshu and Dey, Sudeepa Roy}, 
  title = {{Predicting the direction of stock market prices using random forest}}, 
  eprint = {1605.00003}, 
  abstract = {{Predicting trends in stock market prices has been an area of interest for researchers for many years due to its complex and dynamic nature. Intrinsic volatility in stock market across the globe makes the task of prediction challenging. Forecasting and diffusion modeling, although effective can't be the panacea to the diverse range of problems encountered in prediction, short-term or otherwise. Market risk, strongly correlated with forecasting errors, needs to be minimized to ensure minimal risk in investment. The authors propose to minimize forecasting error by treating the forecasting problem as a classification problem, a popular suite of algorithms in Machine learning. In this paper, we propose a novel way to minimize the risk of investment in stock market by predicting the returns of a stock using a class of powerful machine learning algorithms known as ensemble learning. Some of the technical indicators such as Relative Strength Index (RSI), stochastic oscillator etc are used as inputs to train our model. The learning model used is an ensemble of multiple decision trees. The algorithm is shown to outperform existing algo- rithms found in the literature. Out of Bag (OOB) error estimates have been found to be encouraging. Key Words: Random Forest Classifier, stock price forecasting, Exponential smoothing, feature extraction, OOB error and convergence.}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Khaidem-2016-Unknown.pdf}, 
  year = {2016}
}
@article{10.1016/j.knosys.2014.04.022, 
  author = {Li, Xiaodong and Xie, Haoran and Chen, Li and Wang, Jianping and Deng, Xiaotie}, 
  title = {{News impact on stock price return via sentiment analysis}}, 
  issn = {0950-7051}, 
  doi = {10.1016/j.knosys.2014.04.022}, 
  abstract = {{Financial news articles are believed to have impacts on stock price return. Previous works model news pieces in bag-of-words space, which analyzes the latent relationship between word statistical patterns and stock price movements. However, news sentiment, which is an important ring on the chain of mapping from the word patterns to the price movements, is rarely touched. In this paper, we first implement a generic stock price prediction framework, and plug in six different models with different analyzing approaches. To take one step further, we use Harvard psychological dictionary and Loughran–McDonald financial sentiment dictionary to construct a sentiment space. Textual news articles are then quantitatively measured and projected onto the sentiment space. Instance labeling method is rigorously discussed and tested. We evaluate the models’ prediction accuracy and empirically compare their performance at different market classification levels. Experiments are conducted on five years historical Hong Kong Stock Exchange prices and news articles. Results show that (1) at individual stock, sector and index levels, the models with sentiment analysis outperform the bag-of-words model in both validation set and independent testing set; (2) the models which use sentiment polarity cannot provide useful predictions; (3) there is a minor difference between the models using two different sentiment dictionaries.}}, 
  pages = {14--23}, 
  volume = {69}, 
  journal = {Knowledge-Based Systems}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Li-2014-Knowl-based%20Syst.pdf}, 
  year = {2014}
}
@article{10.1016/j.ejor.2016.10.031, 
  author = {Krauss, Christopher and Do, Xuan Anh and Huck, Nicolas}, 
  title = {{Deep neural networks, gradient-boosted trees, random forests: Statistical arbitrage on the S\&P 500}}, 
  issn = {0377-2217}, 
  doi = {10.1016/j.ejor.2016.10.031}, 
  abstract = {{In recent years, machine learning research has gained momentum: new developments in the field of deep learning allow for multiple levels of abstraction and are starting to supersede well-known and powerful tree-based techniques mainly operating on the original feature space. All these methods can be applied to various fields, including finance. This paper implements and analyzes the effectiveness of deep neural networks (DNN), gradient-boosted-trees (GBT), random forests (RAF), and several ensembles of these methods in the context of statistical arbitrage. Each model is trained on lagged returns of all stocks in the S\&P 500, after elimination of survivor bias. From 1992 to 2015, daily one-day-ahead trading signals are generated based on the probability forecast of a stock to outperform the general market. The highest k probabilities are converted into long and the lowest k probabilities into short positions, thus censoring the less certain middle part of the ranking. Empirical findings are promising. A simple, equal-weighted ensemble (ENS1) consisting of one deep neural network, one gradient-boosted tree, and one random forest produces out-of-sample returns exceeding 0.45 percent per day for k=10, prior to transaction costs. Irrespective of the fact that profits are declining in recent years, our findings pose a severe challenge to the semi-strong form of market efficiency.}}, 
  pages = {689--702}, 
  number = {2}, 
  volume = {259}, 
  journal = {European Journal of Operational Research}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Krauss-2017-Eur%20J%20Oper%20Res.pdf}, 
  year = {2017}
}
@book{undefined, 
  author = {Prado, López de}, 
  title = {{Advances in Financial Machine Learning}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Prado-2018-Unknown.pdf}, 
  year = {2018}
}
@article{undefined, 
  author = {Mudinas, Andrius and Zhang, Dell and Levene, Mark}, 
  title = {{Market Trend Prediction using Sentiment Analysis: Lessons Learned and Paths Forward}}, 
  eprint = {1903.05440}, 
  abstract = {{Financial market forecasting is one of the most attractive practical applications of sentiment analysis. In this paper, we investigate the potential of using sentiment \textbackslashemph\{attitudes\} (positive vs negative) and also sentiment \textbackslashemph\{emotions\} (joy, sadness, etc.) extracted from financial news or tweets to help predict stock price movements. Our extensive experiments using the \textbackslashemph\{Granger-causality\} test have revealed that (i) in general sentiment attitudes do not seem to Granger-cause stock price changes; and (ii) while on some specific occasions sentiment emotions do seem to Granger-cause stock price changes, the exhibited pattern is not universal and must be looked at on a case by case basis. Furthermore, it has been observed that at least for certain stocks, integrating sentiment emotions as additional features into the machine learning based market trend prediction model could improve its accuracy.}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Mudinas-2019-Unknown.pdf}, 
  year = {2019}
}
@article{undefined, 
  author = {Nelson, Charles R. and Siegel, Andrew F.}, 
  title = {{Parsimonious Modeling of Yield Curves}}, 
  journal = {The Journal of Business}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Nelson-1987-The%20Journal%20of%20Business.pdf}, 
  year = {1987}
}
@article{undefined, 
  author = {Peramunetilleke, Desh and Wong, Raymond K.}, 
  title = {{Currency Exchange Rate Forecasting from News Headlines}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Peramunetilleke-2001-Unknown.pdf}, 
  year = {2001}
}
@article{10.1016/j.qref.2010.02.007, 
  author = {Roache, Shaun K and Rossi, Marco}, 
  title = {{The effects of economic news on commodity prices}}, 
  issn = {1062-9769}, 
  doi = {10.1016/j.qref.2010.02.007}, 
  abstract = {{We assess how commodity prices respond to macroeconomic news and show that commodities have been relatively insensitive to such news over daily frequencies between 1997 and 2009 compared to other financial assets and major exchange rates. Where commodity prices are influenced by news, there is a pro-cyclical bias and these sensitivities have risen as commodities have become increasingly financialized. However, models based on news still do a relatively poor job of forecasting commodity prices at daily frequencies. We also find some asymmetries in how commodity prices respond to news, most notably for gold, which alone among commodities acts as a safe-haven when “bad” economic news emerges.}}, 
  pages = {377--385}, 
  number = {3}, 
  volume = {50}, 
  journal = {The Quarterly Review of Economics and Finance}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Roache-2010-Q%20Rev%20Econ%20Finance.pdf}, 
  year = {2010}
}
@article{10.1016/j.eswa.2014.12.003, 
  author = {Rather, Akhter Mohiuddin and Agarwal, Arun and Sastry, V.N.}, 
  title = {{Recurrent neural network and a hybrid model for prediction of stock returns}}, 
  issn = {0957-4174}, 
  doi = {10.1016/j.eswa.2014.12.003}, 
  abstract = {{In this paper, we propose a robust and novel hybrid model for prediction of stock returns. The proposed model is constituted of two linear models: autoregressive moving average model, exponential smoothing model and a non-linear model: recurrent neural network. Training data for recurrent neural network is generated by a new regression model. Recurrent neural network produces satisfactory predictions as compared to linear models. With the goal to further improve the accuracy of predictions, the proposed hybrid prediction model merges predictions obtained from these three prediction based models. An optimization model is introduced which generates optimal weights for proposed model; the model is solved using genetic algorithms. The results confirm about the accuracy of the prediction performance of recurrent neural network. As expected, an outstanding prediction performance has been obtained from proposed hybrid prediction model as it outperforms recurrent neural network. The proposed model is certainly expected to be a promising approach in the field of prediction based models where data is non-linear, whose patterns are difficult to be captured by traditional models.}}, 
  pages = {3234--3241}, 
  number = {6}, 
  volume = {42}, 
  journal = {Expert Systems with Applications}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Rather-2015-Expert%20Syst%20Appl.pdf}, 
  year = {2015}
}
@article{undefined, 
  author = {Sambasivan, Rajiv and Das, Sourish}, 
  title = {{A Statistical Machine Learning Approach to Yield Curve Forecasting}}, 
  eprint = {1703.01536}, 
  abstract = {{Yield curve forecasting is an important problem in finance. In this work we explore the use of Gaussian Processes in conjunction with a dynamic modeling strategy, much like the Kalman Filter, to model the yield curve. Gaussian Processes have been successfully applied to model functional data in a variety of applications. A Gaussian Process is used to model the yield curve. The hyper-parameters of the Gaussian Process model are updated as the algorithm receives yield curve data. Yield curve data is typically available as a time series with a frequency of one day. We compare existing methods to forecast the yield curve with the proposed method. The results of this study showed that while a competing method (a multivariate time series method) performed well in forecasting the yields at the short term structure region of the yield curve, Gaussian Processes perform well in the medium and long term structure regions of the yield curve. Accuracy in the long term structure region of the yield curve has important practical implications. The Gaussian Process framework yields uncertainty and probability estimates directly in contrast to other competing methods. Analysts are frequently interested in this information. In this study the proposed method has been applied to yield curve forecasting, however it can be applied to model high frequency time series data or data streams in other domains.}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Sambasivan-2017-Unknown.pdf}, 
  year = {2017}
}
@article{smales2014, 
  author = {Smales, Lee A}, 
  title = {{News sentiment in the gold futures market}}, 
  issn = {0378-4266}, 
  doi = {10.1016/j.jbankfin.2014.09.006}, 
  abstract = {{This article utilises commodity specific news sentiment data provided by Thomson Reuters News Analytics to examine the relationship between news sentiment and returns in the gold futures market over the period 2003–2012. There is an asymmetric response to news releases with negative news sentiment invoking a greater contemporaneous response in returns of gold futures. There is evidence to support the supposition that net trader positions significantly impact the identified sentiment relationship with the effect greatest when traders are holding positions contrary to their natural position; this may be explained by constraints imposed on traders in terms of credit availability, exchange imposed limits, or inventory required for physical settlement. Recession, and associated changes in credit costs, impact the size of net positions and the news sentiment/return relationship.}}, 
  pages = {275--286}, 
  volume = {49}, 
  journal = {Journal of Banking \& Finance}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Smales-2014-J%20Bank%20Financ.pdf}, 
  year = {2014}
}
@article{10.1111/j.1540-6261.2007.01232.x, 
  author = {TETLOCK, PAUL C}, 
  title = {{Giving Content to Investor Sentiment: The Role of Media in the Stock Market}}, 
  issn = {0022-1082}, 
  doi = {10.1111/j.1540-6261.2007.01232.x}, 
  abstract = {{I quantitatively measure the interactions between the media and the stock market using daily content from a popular Wall Street Journal column. I find that high media pessimism predicts downward pressure on market prices followed by a reversion to fundamentals, and unusually high or low pessimism predicts high market trading volume. These and similar results are consistent with theoretical models of noise and liquidity traders, and are inconsistent with theories of media content as a proxy for new information about fundamental asset values, as a proxy for market volatility, or as a sideshow with no relationship to asset markets.}}, 
  pages = {1139--1168}, 
  number = {3}, 
  volume = {62}, 
  journal = {The Journal of Finance}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/TETLOCK-2007-J%20Finance.pdf}, 
  year = {2007}
}
@article{10.1016/j.resourpol.2018.05.012, 
  author = {Yang, Cai and Gong, Xu and Zhang, Hongwei}, 
  title = {{Volatility forecasting of crude oil futures: The role of investor sentiment and leverage effect}}, 
  issn = {0301-4207}, 
  doi = {10.1016/j.resourpol.2018.05.012}, 
  abstract = {{ This paper explores the role of investor sentiment and leverage effect on the predictability of crude oil futures market volatility over daily, weekly and monthly horizons. Based on the existing five HAR-type models, we develop three kinds of new HAR-type models by incorporating investor sentiment and/or leverage effect in the corresponding original HAR-type models to examine this issue. We find that the investor sentiment and leverage effect have significant effects on volatility forecasting. In most cases, the leverage effect contains more in-sample and out-of-sample information than the investor sentiment, however, the investor sentiment seems to have more out-of-sample information when forecasting the long-term (i.e., 1-month) future volatility. Furthermore, taking into consideration both the investor sentiment and leverage effect in the original HAR-type models produces better in-sample fitting power and out-of-sample forecasting performance than the corresponding other three kinds of HAR-type models. The results suggest that investor sentiment and leverage effect should be taken into consideration when forecasting the volatility of crude oil futures market.}}, 
  pages = {548--563}, 
  number = {Int. Econ. Rev. 39 1998}, 
  volume = {61}, 
  journal = {Resources Policy}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Yang-2019-Resour%20Policy.pdf}, 
  year = {2019}
}
@misc{undefined, 
  author = {Velay, Marc and Daniel, Fabrice}, 
  title = {{Using NLP on news headlines to predict index trends}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Velay-2018-Unknown.pdf}, 
  year = {2018}
}
@misc{undefined, 
  author = {Turnbull, Stuart M. and Habahbeh, Lawrence}, 
  title = {{A Framework to Analyze the Financial Effects of Climate Change}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Unknown-Unknown-Unknown_5.pdf}, 
  year = {2019}
}
@article{10.1016/j.physa.2014.06.064, 
  author = {Yuan, Ying and Zhuang, Xin-tian and Jin, Xiu and Huang, Wei-qiang}, 
  title = {{Stable distribution and long-range correlation of Brent crude oil market}}, 
  issn = {0378-4371}, 
  doi = {10.1016/j.physa.2014.06.064}, 
  abstract = {{An empirical study of stable distribution and long-range correlation in Brent crude oil market was presented. First, it is found that the empirical distribution of Brent crude oil returns can be fitted well by a stable distribution, which is significantly different from a normal distribution. Second, the detrended fluctuation analysis for the Brent crude oil returns shows that there are long-range correlation in returns. It implies that there are patterns or trends in returns that persist over time. Third, the detrended fluctuation analysis for the Brent crude oil returns shows that after the financial crisis 2008, the Brent crude oil market becomes more persistence. It implies that the financial crisis 2008 could increase the frequency and strength of the interdependence and correlations between the financial time series. All of these findings may be used to improve the current fractal theories.}}, 
  pages = {173--179}, 
  volume = {413}, 
  journal = {Physica A: Statistical Mechanics and its Applications}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Yuan-2014-Phys%20Statistical%20Mech%20Appl.pdf}, 
  year = {2014}
}
@misc{undefined, 
  author = {Alexopoulos, Michelle}, 
  title = {{Read all about it!! What happens following a technology shock?}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Alexopoulos-2004-Unknown.pdf}, 
  year = {2004}
}
@article{undefined, 
  author = {Zhao, David and Rinaldo, Alessandro and Brookins, Christopher}, 
  title = {{Cryptocurrency Price Prediction and Trading Strategies Using Support Vector Machines}}, 
  eprint = {1911.11819}, 
  abstract = {{Few assets in financial history have been as notoriously volatile as cryptocurrencies. While the long term outlook for this asset class remains unclear, we are successful in making short term price predictions for several major crypto assets. Using historical data from July 2015 to November 2019, we develop a large number of technical indicators to capture patterns in the cryptocurrency market. We then test various classification methods to forecast short-term future price movements based on these indicators. On both PPV and NPV metrics, our classifiers do well in identifying up and down market moves over the next 1 hour. Beyond evaluating classification accuracy, we also develop a strategy for translating 1-hour-ahead class predictions into trading decisions, along with a backtester that simulates trading in a realistic environment. We find that support vector machines yield the most profitable trading strategies, which outperform the market on average for Bitcoin, Ethereum and Litecoin over the past 22 months, since January 2018.}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Papers%20Library/Zhao-2019-Unknown.pdf}, 
  year = {2019}
}
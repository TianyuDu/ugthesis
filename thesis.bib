@article{10.1007/s10614-006-9041-7, 
  author = {Aminian, Farzan and Suarez, E. Dante and Aminian, Mehran and Walz, Daniel T.}, 
  title = {{Forecasting Economic Data with Neural Networks}}, 
  issn = {0927-7099}, 
  doi = {10.1007/s10614-006-9041-7}, 
  abstract = {{Studies in recent years have attempted to forecast macroeconomic phenomena with neural networks reporting mixed results. This work represents an investigation of this problem using U.S. Real Gross Domestic Production and Industrial Production as case studies. This work is based on a coefficient of determination which accurately measures the ability of linear or nonlinear models to forecast economic data. The significance of our work is twofold: (1) It confirms recent work that neural networks significantly outperform linear regression due to nonlinearities inherent in the data sets, and (2) it provides a systematic approach that guarantees to find the maximum correlation between input(s) and output of interest.}}, 
  pages = {71--88}, 
  number = {1}, 
  volume = {28}, 
  journal = {Computational Economics}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Aminian%20et%20al._2006_Forecasting%20economic%20data%20with%20neural%20networks.pdf}, 
  year = {2006}
}
@article{undefined, 
  author = {}, 
  title = {{Adya, Collopy\_2002\_How effective are neural networks at forecasting and prediction A review and evaluation.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Adya,%20Collopy_2002_How%20effective%20are%20neural%20networks%20at%20forecasting%20and%20prediction%20A%20review%20and%20evaluation.pdf}
}
@article{undefined, 
  author = {}, 
  title = {{Aggarwal\_2018\_Neural Networks and Deep Learning A Textbook.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Aggarwal_2018_Neural%20Networks%20and%20Deep%20Learning%20A%20Textbook.pdf}
}
@article{undefined, 
  author = {}, 
  title = {{Aguirregabiria - 2009 - Some Notes on Sample Selection Models.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Aguirregabiria%20-%202009%20-%20Some%20Notes%20on%20Sample%20Selection%20Models.pdf}
}
@article{undefined, 
  author = {}, 
  title = {{Aguirregabiria\_2009\_Some Notes on Sample Selection Models.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Aguirregabiria_2009_Some%20Notes%20on%20Sample%20Selection%20Models.pdf}
}
@article{undefined, 
  author = {Ahuja, Chaitanya and Morency, Louis-Philippe}, 
  title = {{Lattice Recurrent Unit: Improving Convergence and Statistical Efficiency for Sequence Modeling}}, 
  eprint = {1710.02254}, 
  abstract = {{Recurrent neural networks have shown remarkable success in modeling sequences. However low resource situations still adversely affect the generalizability of these models. We introduce a new family of models, called Lattice Recurrent Units (LRU), to address the challenge of learning deep multi-layer recurrent models with limited resources. LRU models achieve this goal by creating distinct (but coupled) flow of information inside the units: a first flow along time dimension and a second flow along depth dimension. It also offers a symmetry in how information can flow horizontally and vertically. We analyze the effects of decoupling three different components of our LRU model: Reset Gate, Update Gate and Projected State. We evaluate this family on new LRU models on computational convergence rates and statistical efficiency. Our experiments are performed on four publicly-available datasets, comparing with Grid-LSTM and Recurrent Highway networks. Our results show that LRU has better empirical computational convergence rates and statistical efficiency values, along with learning more accurate language models.}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Ahuja,%20Morency_2017_Lattice%20Recurrent%20Unit%20Improving%20Convergence%20and%20Statistical%20Efficiency%20for%20Sequence%20Modeling.pdf}, 
  year = {2017}
}
@article{10.1093/restud/rdv052, 
  author = {Alaoui, Larbi and Penta, Antonio}, 
  title = {{Endogenous Depth of Reasoning}}, 
  issn = {0034-6527}, 
  doi = {10.1093/restud/rdv052}, 
  pages = {1297--1333}, 
  number = {4}, 
  volume = {83}, 
  journal = {The Review of Economic Studies}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Alaoui,%20Penta_2016_Endogenous%20depth%20of%20reasoning.pdf}, 
  year = {2015}
}
@article{10.1515/jaiscr-2018-0009, 
  author = {Akdeniz, Esra and Egrioglu, Erol and Bas, Eren and Yolcu, Ufuk}, 
  title = {{An ARMA Type Pi-Sigma Artificial Neural Network for Nonlinear Time Series Forecasting}}, 
  doi = {10.1515/jaiscr-2018-0009}, 
  abstract = {{Real-life time series have complex and non-linear structures. Artificial Neural Networks have been frequently used in the literature to analyze non-linear time series. High order artificial neural networks, in view of other artificial neural network types, are more adaptable to the data because of their expandable model order. In this paper, a new recurrent architecture for Pi-Sigma artificial neural networks is proposed. A learning algorithm based on particle swarm optimization is also used as a tool for the training of the proposed neural network. The proposed new high order artificial neural network is applied to three real life time series data and also a simulation study is performed for Istanbul Stock Exchange data set.}}, 
  pages = {121--132}, 
  number = {2}, 
  volume = {8}, 
  journal = {Journal of Artificial Intelligence and Soft Computing Research}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Akdeniz%20et%20al._2018_An%20ARMA%20Type%20Pi-Sigma%20Artificial%20Neural%20Network%20for%20Nonlinear%20Time%20Series%20Forecasting.pdf}, 
  year = {2018}
}
@article{undefined, 
  author = {}, 
  title = {{Ando et al.\_2002\_Presence of autoantibody against attr Val30Met after sequential liver transplantation.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Ando%20et%20al._2002_Presence%20of%20autoantibody%20against%20attr%20Val30Met%20after%20sequential%20liver%20transplantation.pdf}
}
@article{undefined, 
  author = {}, 
  title = {{Arellano\_2008\_Binary Models with Endogenous Explanatory Variables Class Notes.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Arellano_2008_Binary%20Models%20with%20Endogenous%20Explanatory%20Variables%20Class%20Notes.pdf}
}
@article{undefined, 
  author = {}, 
  title = {{Angrist, Krueger\_1991\_Does compulsory school attendance affect schooling and earnings.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Angrist,%20Krueger_1991_Does%20compulsory%20school%20attendance%20affect%20schooling%20and%20earnings.pdf}
}
@article{undefined, 
  author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E}, 
  title = {{Layer Normalization}}, 
  eprint = {1607.06450}, 
  abstract = {{Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Ba,%20Kiros,%20Hinton%20-%202016%20-%20Layer%20Normalization.pdf}, 
  year = {2016}
}
@article{undefined, 
  author = {}, 
  title = {{Arellano\_2008\_Tobit and Selection Models Class Notes.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Arellano_2008_Tobit%20and%20Selection%20Models%20Class%20Notes.pdf}
}
@article{undefined, 
  author = {}, 
  title = {{Bank of England\_2019\_Machine learning in UK financial services.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Bank%20of%20England_2019_Machine%20learning%20in%20UK%20financial%20services.pdf}
}
@article{undefined, 
  author = {}, 
  title = {{Baum et al.\_2012\_Binary choice models with endogenous regressors.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Baum%20et%20al._2012_Binary%20choice%20models%20with%20endogenous%20regressors.pdf}
}
@article{10.1080/07350015.2011.648859, 
  author = {Baumeister, Christiane and Kilian, Lutz}, 
  title = {{Real-Time Forecasts of the Real Price of Oil}}, 
  issn = {0735-0015}, 
  doi = {10.1080/07350015.2011.648859}, 
  pages = {326--336}, 
  number = {2}, 
  volume = {30}, 
  journal = {Journal of Business \& Economic Statistics}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Baumeister,%20Kilian_2012_Real-time%20forecasts%20of%20the%20real%20price%20of%20oil.pdf}, 
  year = {2012}
}
@article{10.1080/07350015.2014.949342, 
  author = {Baumeister, Christiane and Kilian, Lutz}, 
  title = {{Forecasting the Real Price of Oil in a Changing World: A Forecast Combination Approach}}, 
  issn = {0735-0015}, 
  doi = {10.1080/07350015.2014.949342}, 
  pages = {338--351}, 
  number = {3}, 
  volume = {33}, 
  journal = {Journal of Business \& Economic Statistics}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Baumeister,%20Kilian_2015_Forecasting%20the%20Real%20Price%20of%20Oil%20in%20a%20Changing%20World%20A%20Forecast%20Combination%20Approach.pdf}, 
  year = {2015}
}
@article{Kilian2016, 
  author = {Baumeister, Christiane and Kilian, Lutz}, 
  title = {{Forty Years of Oil Price Fluctuations: Why the Price of Oil May Still Surprise Us}}, 
  issn = {0895-3309}, 
  doi = {10.1257/jep.30.1.139}, 
  pages = {139--160}, 
  number = {1}, 
  volume = {30}, 
  journal = {Journal of Economic Perspectives}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Baumeister,%20Kilian_2016_Forty%20years%20of%20oil%20price%20fluctuations%20Why%20the%20price%20of%20oil%20may%20still%20surprise%20us.pdf}, 
  year = {2016}
}
@article{undefined, 
  author = {}, 
  title = {{Bengio, Simard, Frasconi\_1994\_Learning long-term dependencies with gradient descent is difficult.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Bengio,%20Simard,%20Frasconi_1994_Learning%20long-term%20dependencies%20with%20gradient%20descent%20is%20difficult.pdf}
}
@article{10.1007/978-3-319-74380-6, 
  author = {Beran, Jan}, 
  title = {{Mathematical Foundations of Time Series Analysis, A Concise Introduction}}, 
  doi = {10.1007/978-3-319-74380-6}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Beran_2018_Mathematical%20foundations%20of%20time%20series%20analysis%20A%20concise%20introduction.pdf}, 
  year = {2017}
}
@article{10.1007/978-3-319-70338-1, 
  author = {Bianchi, Filippo Maria and Maiorino, Enrico and Kampffmeyer, Michael C. and Rizzi, Antonello and Jenssen, Robert}, 
  title = {{Recurrent Neural Networks for Short-Term Load Forecasting, An Overview and Comparative Analysis}}, 
  doi = {10.1007/978-3-319-70338-1}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Bianchi,%20Kampffmeyer%20-%20Unknown%20-%20Recurrent%20Neural%20Networks%20for%20Short-Term%20Load%20Forecasting%20An%20Overview%20and%20Comparative%20Analysis.pdf}, 
  year = {2017}
}
@article{undefined, 
  author = {}, 
  title = {{Bergstra et al.\_2011\_Algorithms for hyper-parameter optimization.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Bergstra%20et%20al._2011_Algorithms%20for%20hyper-parameter%20optimization.pdf}
}
@article{10.1257/jep.28.2.3, 
  author = {Varian, Hal R}, 
  title = {{Big Data: New Tricks for Econometrics}}, 
  issn = {0895-3309}, 
  doi = {10.1257/jep.28.2.3}, 
  pages = {3--28}, 
  number = {2}, 
  volume = {28}, 
  journal = {Journal of Economic Perspectives}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Big_Data_New_Tricks_for_Econo%202.pdf}, 
  year = {2014}
}
@article{10.1080/0003684052000343679, 
  author = {*, Jane M. Binner and Bissoondeeal, Rakesh K. and Elger, Thomas and Gazely, Alicia M. and Mullineux, Andrew W.}, 
  title = {{A comparison of linear forecasting models and neural networks: an application to Euro inflation and Euro Divisia}}, 
  issn = {0003-6846}, 
  doi = {10.1080/0003684052000343679}, 
  abstract = {{Linear models reach their limitations in applications with nonlinearities in the data. In this paper new empirical evidence is provided on the relative Euro inflation forecasting performance of linear and non-linear models. The well established and widely used univariate ARIMA and multivariate VAR models are used as linear forecasting models whereas neural networks (NN) are used as non-linear forecasting models. It is endeavoured to keep the level of subjectivity in the NN building process to a minimum in an attempt to exploit the full potentials of the NN. It is also investigated whether the historically poor performance of the theoretically superior measure of the monetary services flow, Divisia, relative to the traditional Simple Sum measure could be attributed to a certain extent to the evaluation of these indices within a linear framework. Results obtained suggest that non-linear models provide better within-sample and out-of-sample forecasts and linear models are simply a subset of them. The Divisia index also outperforms the Simple Sum index when evaluated in a non-linear framework.}}, 
  pages = {665--680}, 
  number = {6}, 
  volume = {37}, 
  journal = {Applied Economics}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Binner%20et%20al.%20-%202005%20-%20A%20comparison%20of%20linear%20forecasting%20models%20and%20neural%20networks%20An%20application%20to%20Euro%20inflation%20and%20Euro%20Divisia.pdf}, 
  year = {2005}
}
@article{10.1080/0003684052000343679, 
  author = {*, Jane M. Binner and Bissoondeeal, Rakesh K. and Elger, Thomas and Gazely, Alicia M. and Mullineux, Andrew W.}, 
  title = {{A comparison of linear forecasting models and neural networks: an application to Euro inflation and Euro Divisia}}, 
  issn = {0003-6846}, 
  doi = {10.1080/0003684052000343679}, 
  abstract = {{Linear models reach their limitations in applications with nonlinearities in the data. In this paper new empirical evidence is provided on the relative Euro inflation forecasting performance of linear and non-linear models. The well established and widely used univariate ARIMA and multivariate VAR models are used as linear forecasting models whereas neural networks (NN) are used as non-linear forecasting models. It is endeavoured to keep the level of subjectivity in the NN building process to a minimum in an attempt to exploit the full potentials of the NN. It is also investigated whether the historically poor performance of the theoretically superior measure of the monetary services flow, Divisia, relative to the traditional Simple Sum measure could be attributed to a certain extent to the evaluation of these indices within a linear framework. Results obtained suggest that non-linear models provide better within-sample and out-of-sample forecasts and linear models are simply a subset of them. The Divisia index also outperforms the Simple Sum index when evaluated in a non-linear framework.}}, 
  pages = {665--680}, 
  number = {6}, 
  volume = {37}, 
  journal = {Applied Economics}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Binner%20et%20al._2005_A%20comparison%20of%20linear%20forecasting%20models%20and%20neural%20networks%20An%20application%20to%20Euro%20inflation%20and%20Euro%20Divisia.pdf}, 
  year = {2005}
}
@article{10.1111/j.1475-5890.1999.tb00001.x, 
  author = {Blundell, Richard and Dearden, Lorraine and Meghir, Costas and Sianesi, Barbara}, 
  title = {{Human Capital Investment: The Returns from Education and Training to the Individual, the Firm and the Economy}}, 
  issn = {1475-5890}, 
  doi = {10.1111/j.1475-5890.1999.tb00001.x}, 
  abstract = {{This paper provides a non-technical review of the evidence on the returns to education and training for the individual, the firm and the economy at large. It begins by reviewing the empirical work that has attempted to estimate the true causal effect of education and training on individual earnings, focusing on the recent literature that has attempted to control for potential biases in the estimated returns to education and training. It then moves on to review the literature that has looked at the returns from human capital investments to employers. Lack of suitable data and methodological difficulties have resulted in a paucity of studies that have carried out sound empirical work on this issue. In the final part of the review, we look at the work that has tried to assess the contribution of human capital to national economic growth at the macroeconomic level. This work has generally involved using either a ‘growth accounting’ theoretical framework or ‘new growth’ theories. Although the empirical macroeconomic evidence that accompanies this work does not generally allow one to distinguish between the two approaches, there is a substantial body of evidence on the contribution of education to economic growth.}}, 
  pages = {1--23}, 
  number = {1}, 
  volume = {20}, 
  journal = {Fiscal Studies}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Blundell%20et%20al._1999_Human%20Capital%20Investment%20The%20Returns%20from%20Education%20and%20Training%20to%20the%20Individual,%20the%20Firm%20and%20the%20Economy.pdf}, 
  year = {1999}
}
@article{undefined, 
  author = {}, 
  title = {{Bontempi, Taieb, Borgne - 2003 - Machine Learning Strategies for Time Series Forecasting.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Bontempi,%20Taieb,%20Borgne%20-%202003%20-%20Machine%20Learning%20Strategies%20for%20Time%20Series%20Forecasting.pdf}
}
@article{undefined, 
  author = {}, 
  title = {{Black, Kolesnikova, Taylor\_2007\_Earnings Functions When Wages and Prices Vary by Location.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Black,%20Kolesnikova,%20Taylor_2007_Earnings%20Functions%20When%20Wages%20and%20Prices%20Vary%20by%20Location.pdf}
}
@article{undefined, 
  author = {}, 
  title = {{Borwein\_2010\_A Very Complicated Proof of the Minimax Theorem.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Borwein_2010_A%20Very%20Complicated%20Proof%20of%20the%20Minimax%20Theorem.pdf}
}
@article{10.1016/j.ijepes.2018.11.022, 
  author = {Bracale, Antonio and Carpinelli, Guido and Falco, Pasquale De and Hong, Tao}, 
  title = {{Short-term industrial reactive power forecasting}}, 
  issn = {0142-0615}, 
  doi = {10.1016/j.ijepes.2018.11.022}, 
  abstract = {{ Reactive power forecasting is essential for managing energy systems of factories and industrial plants. However, the scientific community has devoted scant attention to industrial load forecasting, and even less to reactive power forecasting. Many challenges in developing a short-term reactive power forecasting system for factories have rarely been studied. Industrial loads may depend on many factors, such as scheduled processes and work shifts, which are uncommon or unnecessary in classical load forecasting models. Moreover, the features of reactive power are significantly different from active power, so some commonly used variables in classical load forecasting models may become meaningless for forecasting reactive power. In this paper, we develop several models to forecast industrial reactive power. These models are constructed based on two forecasting techniques (e.g., multiple linear regression and support vector regression) and two variable selection methods (e.g., cross validation and least absolute shrinkage and selection operator). In the numerical applications based on real data collected from an Italian factory at both aggregate and individual load levels, the proposed models outperform four benchmark models in short forecast horizons.}}, 
  pages = {177--185}, 
  number = {Int J Forecast 32 3 2016}, 
  volume = {107}, 
  journal = {International Journal of Electrical Power \& Energy Systems}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Bracale%20et%20al._2019_Short-term%20industrial%20reactive%20power%20forecasting.pdf}, 
  year = {2019}
}
@article{10.1080/07474938.2015.1035163, 
  author = {Kock, Anders Bredahl and Teräsvirta, Timo}, 
  title = {{Forecasting Macroeconomic Variables Using Neural Network Models and Three Automated Model Selection Techniques}}, 
  issn = {0747-4938}, 
  doi = {10.1080/07474938.2015.1035163}, 
  abstract = {{When forecasting with neural network models one faces several problems, all of which influence the accuracy of the forecasts. First, neural networks are often hard to estimate due to their highly nonlinear structure. To alleviate the problem, White (2006) presented a solution (QuickNet) that converts the specification and nonlinear estimation problem into a linear model selection and estimation problem. We shall compare its performance to that of two other procedures building on the linearization idea: the Marginal Bridge Estimator and Autometrics. Second, one must decide whether forecasting should be carried out recursively or directly. This choice is investigated in this work. The economic time series used in this study are the consumer price indices for the G7 and the Scandinavian countries. In addition, a number of simulations are carried out and results reported in the article.}}, 
  pages = {1753--1779}, 
  number = {8-10}, 
  volume = {35}, 
  journal = {Econometric Reviews}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Bredahl%20Kock,%20Teräsvirta_2016_Forecasting%20Macroeconomic%20Variables%20Using%20Neural%20Network%20Models%20and%20Three%20Automated%20Model%20Selection%20Techn.pdf}, 
  year = {2015}
}
@article{undefined, 
  author = {}, 
  title = {{Brand, Xie\_2010\_Who Benefits Most from College.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Brand,%20Xie_2010_Who%20Benefits%20Most%20from%20College.pdf}
}
@article{10.1007/978-3-319-69014-8, 
  author = {Basuchoudhary, Atin and Bang, James T. and Sen, Tinni}, 
  title = {{Machine-learning Techniques in Economics, New Tools for Predicting Economic Growth}}, 
  doi = {10.1007/978-3-319-69014-8}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Briefs,%20Economics%20-%20Unknown%20-%20Atin%20Basuchoudhary%20James%20T.%20Bang%20Tinni%20Sen.pdf}, 
  year = {2017}
}
@article{undefined, 
  author = {}, 
  title = {{Brockwell, Davis\_1991\_Time Series Theroy and Methods.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Brockwell,%20Davis_1991_Time%20Series%20Theroy%20and%20Methods.pdf}
}
@article{undefined, 
  author = {}, 
  title = {{Busseti, Osband, Wong\_2012\_Deep Learning for Time Series Modeling CS 229 Final Project Report.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Busseti,%20Osband,%20Wong_2012_Deep%20Learning%20for%20Time%20Series%20Modeling%20CS%20229%20Final%20Project%20Report.pdf}
}
@article{10.2139/ssrn.3446225, 
  author = {Bybee, Leland and Kelly, Bryan T. and Manela, Asaf and Xiu, Dacheng}, 
  title = {{The Structure of Economic News}}, 
  doi = {10.2139/ssrn.3446225}, 
  abstract = {{We propose an approach to measuring the state of the economy via textual analysis of business news. From the full text content of 800,000 Wall Street Journal articles for 1984–2017, we estimate a topic model that summarizes business news as easily interpretable topical themes and quantifies the proportion of news attention allocated to each theme at each point in time. We then use our news attention estimates as inputs into statistical models of numerical economic time series. We demonstrate that these text-based inputs accurately track a wide range of economic activity measures and that they have incremental forecasting power for macroeconomic outcomes, above and beyond standard numerical predictors. Finally, we use our model to retrieve the news-based narratives that underly “shocks” in numerical economic data.}}, 
  journal = {SSRN Electronic Journal}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Bybee%20et%20al._2019_The%20Structure%20of%20Economic%20News.pdf}, 
  year = {2019}
}
@article{undefined, 
  author = {}, 
  title = {{Camerer, Ho, Chong\_2004\_A cognitive hierarchy model of games.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Camerer,%20Ho,%20Chong_2004_A%20cognitive%20hierarchy%20model%20of%20games.pdf}
}
@article{10.1016/j.physa.2018.11.061, 
  author = {Cao, Jian and Li, Zhi and Li, Jian}, 
  title = {{Financial time series forecasting model based on CEEMDAN and LSTM}}, 
  issn = {0378-4371}, 
  doi = {10.1016/j.physa.2018.11.061}, 
  abstract = {{ In order to improve the accuracy of the stock market prices forecasting, two hybrid forecasting models are proposed in this paper which combine the two kinds of empirical mode decomposition (EMD) with the long short-term memory (LSTM). The financial time series is a kind of non-linear and non-stationary random signal, which can be decomposed into several intrinsic mode functions of different time scales by the original EMD and the complete ensemble empirical mode decomposition with adaptive noise (CEEMDAN). To ensure the effect of historical data onto the prediction result, the LSTM prediction models are established for all each characteristic series from EMD and CEEMDAN deposition. The final prediction results are obtained by reconstructing each prediction series. The forecasting performance of the proposed models is verified by linear regression analysis of the major global stock market indices. Compared with single LSTM model, support vector machine (SVM), multi-layer perceptron (MLP) and other hybrid models, the experimental results show that the proposed models display a better performance in one-step-ahead forecasting of financial time series.}}, 
  pages = {127--139}, 
  number = {J. Econ. Lit. 41 2 2003}, 
  volume = {519}, 
  journal = {Physica A: Statistical Mechanics and its Applications}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Cao,%20Li,%20Li_2019_Financial%20time%20series%20forecasting%20model%20based%20on%20CEEMDAN%20and%20LSTM.pdf}, 
  year = {2018}
}
@article{undefined, 
  author = {}, 
  title = {{Cameron, Trivedi\_2006\_16 . Tobit and Selection.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Cameron,%20Trivedi_2006_16%20.%20Tobit%20and%20Selection.pdf}
}
@article{10.1016/j.physa.2018.11.061, 
  author = {Cao, Jian and Li, Zhi and Li, Jian}, 
  title = {{Financial time series forecasting model based on CEEMDAN and LSTM}}, 
  issn = {0378-4371}, 
  doi = {10.1016/j.physa.2018.11.061}, 
  abstract = {{ In order to improve the accuracy of the stock market prices forecasting, two hybrid forecasting models are proposed in this paper which combine the two kinds of empirical mode decomposition (EMD) with the long short-term memory (LSTM). The financial time series is a kind of non-linear and non-stationary random signal, which can be decomposed into several intrinsic mode functions of different time scales by the original EMD and the complete ensemble empirical mode decomposition with adaptive noise (CEEMDAN). To ensure the effect of historical data onto the prediction result, the LSTM prediction models are established for all each characteristic series from EMD and CEEMDAN deposition. The final prediction results are obtained by reconstructing each prediction series. The forecasting performance of the proposed models is verified by linear regression analysis of the major global stock market indices. Compared with single LSTM model, support vector machine (SVM), multi-layer perceptron (MLP) and other hybrid models, the experimental results show that the proposed models display a better performance in one-step-ahead forecasting of financial time series.}}, 
  pages = {127--139}, 
  number = {J. Econ. Lit. 41 2 2003}, 
  volume = {519}, 
  journal = {Physica A: Statistical Mechanics and its Applications}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Cao,%20Li,%20Li_2019_Financial%20time%20series%20forecasting%20model%20based%20on%20CEEMDAN%20and%20LSTM(2).pdf}, 
  year = {2018}
}
@article{10.1007/978-3-319-75304-1, 
  author = {Caterini, Anthony L. and Chang, Dong Eui}, 
  title = {{Deep Neural Networks in a Mathematical Framework}}, 
  doi = {10.1007/978-3-319-75304-1}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Caterini,%20Eui%20-%20Unknown%20-%20Deep%20Neural%20Networks%20in%20a%20Mathematical%20Framework.pdf}, 
  year = {2018}
}
@article{undefined, 
  author = {}, 
  title = {{Carleton Athey, Luca\_2018\_Economists (and Economics) in Tech Companies.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Carleton%20Athey,%20Luca_2018_Economists%20(and%20Economics)%20in%20Tech%20Companies.pdf}
}
@article{10.1038/s41598-018-24271-9, 
  author = {Che, Zhengping and Purushotham, Sanjay and Cho, Kyunghyun and Sontag, David and Liu, Yan}, 
  title = {{Recurrent Neural Networks for Multivariate Time Series with Missing Values}}, 
  doi = {10.1038/s41598-018-24271-9}, 
  pmid = {29666385}, 
  abstract = {{Multivariate time series data in practical applications, such as health care, geoscience, and biology, are characterized by a variety of missing values. In time series prediction and other related tasks, it has been noted that missing values and their missing patterns are often correlated with the target labels, a.k.a., informative missingness. There is very limited work on exploiting the missing patterns for effective imputation and improving prediction performance. In this paper, we develop novel deep learning models, namely GRU-D, as one of the early attempts. GRU-D is based on Gated Recurrent Unit (GRU), a state-of-the-art recurrent neural network. It takes two representations of missing patterns, i.e., masking and time interval, and effectively incorporates them into a deep model architecture so that it not only captures the long-term temporal dependencies in time series, but also utilizes the missing patterns to achieve better prediction results. Experiments of time series classification tasks on real-world clinical datasets (MIMIC-III, PhysioNet) and synthetic datasets demonstrate that our models achieve state-of-the-art performance and provide useful insights for better understanding and utilization of missing values in time series analysis.}}, 
  pages = {6085}, 
  number = {1}, 
  volume = {8}, 
  journal = {Scientific Reports}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Che%20et%20al.%20-%202018%20-%20Recurrent%20Neural%20Networks%20for%20Multivariate%20Time%20Series%20with%20Missing%20Values.pdf}, 
  year = {2018}
}
@article{10.1016/s0893-6080(05)80092-9, 
  author = {Chakraborty, Kanad and Mehrotra, Kishan and Mohan, Chilukuri K. and Ranka, Sanjay}, 
  title = {{Forecasting the behavior of multivariate time series using neural networks}}, 
  issn = {0893-6080}, 
  doi = {10.1016/s0893-6080(05)80092-9}, 
  abstract = {{This paper presents a neural network approach to multivariate time-series analysis. Real world observations of flour prices in three cities have been used as a benchmark in our experiments. Feedforward connectionist networks have been designed to model flour prices over the period from August 1972 to November 1980 for the cities of Buffalo, Minneapolis, and Kansas City. Remarkable success has been achieved in training the networks to learn the price curve for each of these cities and in making accurate price predictions. Our results show that the neural network approach is a leading contender with the statistical modeling approaches.}}, 
  pages = {961--970}, 
  number = {6}, 
  volume = {5}, 
  journal = {Neural Networks}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Chakraborty%20et%20al._1992_Forecasting%20the%20behavior%20of%20multivariate%20time%20series%20using%20neural%20networks.pdf}, 
  year = {1992}
}
@article{undefined, 
  author = {Cho, Kyunghyun and Merrienboer, Bart van and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua}, 
  title = {{Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation}}, 
  eprint = {1406.1078}, 
  abstract = {{In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Cho%20et%20al._2014_Learning%20Phrase%20Representations%20using%20RNN%20Encoder-Decoder%20for%20Statistical%20Machine%20Translation.pdf}, 
  year = {2014}
}
@article{undefined, 
  author = {}, 
  title = {{Chen, Ranciere - Unknown - Financial Information and Macroeconomic Forecasts 4.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Chen,%20Ranciere%20-%20Unknown%20-%20Financial%20Information%20and%20Macroeconomic%20Forecasts%204.pdf}
}
@article{undefined, 
  author = {Claesen, Marc and Moor, Bart De}, 
  title = {{Hyperparameter Search in Machine Learning}}, 
  eprint = {1502.02127}, 
  abstract = {{We introduce the hyperparameter search problem in the field of machine learning and discuss its main challenges from an optimization perspective. Machine learning methods attempt to build models that capture some element of interest based on given data. Most common learning algorithms feature a set of hyperparameters that must be determined before training commences. The choice of hyperparameters can significantly affect the resulting model's performance, but determining good values can be complex; hence a disciplined, theoretically sound search strategy is essential.}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Claesen,%20De%20Moor_2015_Hyperparameter%20Search%20in%20Machine%20Learning.pdf}, 
  year = {2015}
}
@article{undefined, 
  author = {Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua}, 
  title = {{Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling}}, 
  eprint = {1412.3555}, 
  abstract = {{In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Chung%20et%20al._2014_Empirical%20Evaluation%20of%20Gated%20Recurrent%20Neural%20Networks%20on%20Sequence%20Modeling.pdf}, 
  year = {2014}
}
@article{undefined, 
  author = {}, 
  title = {{Chung et al.\_2015\_Gated Feedback Recurrent Neural Networks.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Chung%20et%20al._2015_Gated%20Feedback%20Recurrent%20Neural%20Networks.pdf}
}
@article{10.1016/j.econmod.2013.09.024, 
  author = {Claveria, Oscar and Torra, Salvador}, 
  title = {{Forecasting tourism demand to Catalonia: Neural networks vs. time series models}}, 
  issn = {0264-9993}, 
  doi = {10.1016/j.econmod.2013.09.024}, 
  abstract = {{The increasing interest aroused by more advanced forecasting techniques, together with the requirement for more accurate forecasts of tourism demand at the destination level due to the constant growth of world tourism, has lead us to evaluate the forecasting performance of neural modelling relative to that of time series methods at a regional level. Seasonality and volatility are important features of tourism data, which makes it a particularly favourable context in which to compare the forecasting performance of linear models to that of nonlinear alternative approaches. Pre-processed official statistical data of overnight stays and tourist arrivals from all the different countries of origin to Catalonia from 2001 to 2009 is used in the study. When comparing the forecasting accuracy of the different techniques for different time horizons, autoregressive integrated moving average models outperform self-exciting threshold autoregressions and artificial neural network models, especially for shorter horizons. These results suggest that the there is a trade-off between the degree of pre-processing and the accuracy of the forecasts obtained with neural networks, which are more suitable in the presence of nonlinearity in the data. In spite of the significant differences between countries, which can be explained by different patterns of consumer behaviour, we also find that forecasts of tourist arrivals are more accurate than forecasts of overnight stays.}}, 
  pages = {220--228}, 
  volume = {36}, 
  journal = {Economic Modelling}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Claveria,%20Torra_2014_Forecasting%20tourism%20demand%20to%20Catalonia%20Neural%20networks%20vs.%20time%20series%20models.pdf}, 
  year = {2014}
}
@article{10.1016/j.cie.2007.06.005, 
  author = {Co, Henry C. and Boosarawongse, Rujirek}, 
  title = {{Forecasting Thailand’s rice export: Statistical techniques vs. artificial neural networks}}, 
  issn = {0360-8352}, 
  doi = {10.1016/j.cie.2007.06.005}, 
  abstract = {{Forecasting the international trade of rice is difficult because demand and supply are affected by many unpredictable factors (e.g., trade barriers and subsidies, agricultural and environmental factors, meteorological factors, biophysical factors, changing demographics, etc.) that interact in a complex manner. This paper compares the performance of artificial neural networks (ANNs) with exponential smoothing and ARIMA models in forecasting rice exports from Thailand. To ascertain that the models can reproduce acceptable results on unseen future, we evaluated various aggregate measures of forecast error (MAE, MSE, MAPE, and RMSE) during the validation process of the models. The results reveal that while the Holt–Winters and the Box–Jenkins models showed satisfactory goodness of fit, the models did not perform as well in predicting unseen data during validation. On the other hand, the ANNs performed relatively well as they were able to track the dynamic non-linear trend and seasonality, and the interactions between them.}}, 
  pages = {610--627}, 
  number = {4}, 
  volume = {53}, 
  journal = {Computers \& Industrial Engineering}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Co%20-%202007%20-%20Forecasting%20Thailand%20’%20s%20rice%20export%20Statistical%20techniques%20vs%20.%20artificial%20neural%20networks.pdf}, 
  year = {2007}
}
@article{10.1016/j.ijforecast.2003.10.004, 
  author = {Clements, Michael P. and Franses, Philip Hans and Swanson, Norman R.}, 
  title = {{Forecasting economic and financial time-series with non-linear models}}, 
  issn = {0169-2070}, 
  doi = {10.1016/j.ijforecast.2003.10.004}, 
  abstract = {{In this paper we discuss the current state-of-the-art in estimating, evaluating, and selecting among non-linear forecasting models for economic and financial time series. We review theoretical and empirical issues, including predictive density, interval and point evaluation and model selection, loss functions, data-mining, and aggregation. In addition, we argue that although the evidence in favor of constructing forecasts using non-linear models is rather sparse, there is reason to be optimistic. However, much remains to be done. Finally, we outline a variety of topics for future research, and discuss a number of areas which have received considerable attention in the recent literature, but where many questions remain.}}, 
  pages = {169--183}, 
  number = {2}, 
  volume = {20}, 
  journal = {International Journal of Forecasting}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Clements,%20Hans,%20Swanson%20-%202004%20-%20Forecasting%20economic%20and%20financial%20time-series%20with%20non-linear%20models%205.pdf}, 
  year = {2004}
}
@article{undefined, 
  author = {}, 
  title = {{Connor, Atlas\_2002\_Recurrent neural networks and time series prediction.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Connor,%20Atlas_2002_Recurrent%20neural%20networks%20and%20time%20series%20prediction.pdf}
}
@article{undefined, 
  author = {}, 
  title = {{Connor, Atlas, Martin\_1992\_Recurrent Networks and NARMA Modeling.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Connor,%20Atlas,%20Martin_1992_Recurrent%20Networks%20and%20NARMA%20Modeling.pdf}
}
@article{undefined, 
  author = {}, 
  title = {{Connor, Atlas, Martin - 1992 - Recurrent Networks and NARMA Modeling.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Connor,%20Atlas,%20Martin%20-%201992%20-%20Recurrent%20Networks%20and%20NARMA%20Modeling.pdf}
}
@article{10.18651/rwp2017-11, 
  author = {Cook, Thomas and Hall, Aaron Smalter}, 
  title = {{Macroeconomic Indicator Forecasting with Deep Neural Networks}}, 
  issn = {1936-5330}, 
  doi = {10.18651/rwp2017-11}, 
  journal = {The Federal Reserve Bank of Kansas City Research Working Papers}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Cook%20et%20al.%20-%202017%20-%20Macroeconomic%20Indicator%20Forecasting%20with%20Deep%20Neural%20Networks.pdf}, 
  year = {2017}
}
@article{undefined, 
  author = {}, 
  title = {{Dale, Krueger\_2002\_Estimating the payoff to attending a more selective college An application of selection on observables and unobservab.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Dale,%20Krueger_2002_Estimating%20the%20payoff%20to%20attending%20a%20more%20selective%20college%20An%20application%20of%20selection%20on%20observables%20and%20unobservab.pdf}
}
@article{10.1007/bf02551274, 
  author = {Cybenko, G.}, 
  title = {{Approximation by superpositions of a sigmoidal function}}, 
  issn = {0932-4194}, 
  doi = {10.1007/bf02551274}, 
  abstract = {{In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.}}, 
  pages = {303--314}, 
  number = {4}, 
  volume = {2}, 
  journal = {Mathematics of Control, Signals and Systems}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Cybenkot_1989_Approximation%20by%20Superpositions%20of%20a%20Sigmoidal%20Function.pdf}, 
  year = {1989}
}
@article{10.1007/978-0-387-71720-3, 
  author = {Yu, Lean and Wang, Shouyang and Lai, Kin Keung}, 
  title = {{Foreign-Exchange-Rate Forecasting With Artificial Neural Networks}}, 
  doi = {10.1007/978-0-387-71720-3}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Dalgleish%20et%20al.%20-%202007%20-%20No%20Title.pdf}, 
  year = {2007}
}
@article{10.1353/jhr.2014.0015, 
  author = {Dale, Stacy B. and Krueger, Alan B.}, 
  title = {{Estimating the Effects of College Characteristics over the Career Using Administrative Earnings Data}}, 
  issn = {1548-8004}, 
  doi = {10.1353/jhr.2014.0015}, 
  abstract = {{We estimate the labor market effect of attending a highly selective college, using the College and Beyond Survey linked to Social Security Administration data. We extend earlier work by estimating effects for students that entered college in 1976 over a longer time horizon (from 1983 through 2007) and for a more recent cohort (1989). For both cohorts, the effects of college characteristics on earnings are sizeable (and similar in magnitude) in standard regression models. In selectionadjusted models, these effects generally fall to close to zero; however, these effects remain large for certain subgroups, such as for black and Hispanic students.}}, 
  pages = {323--358}, 
  number = {2}, 
  volume = {49}, 
  journal = {Journal of Human Resources}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Dale,%20Krueger_2014_Estimating%20the%20Effects%20of%20College%20Characteristics%20over%20the%20Career%20Using%20Administrative%20Earnings%20Data.pdf}, 
  year = {2014}
}
@article{undefined, 
  author = {}, 
  title = {{Dietterich - 2002 - (impo)(George recom)(Sequenced prediction)Machine learning for sequential data A review.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Dietterich%20-%202002%20-%20(impo)(George%20recom)(Sequenced%20prediction)Machine%20learning%20for%20sequential%20data%20A%20review.pdf}
}
@article{undefined, 
  author = {}, 
  title = {{Denmark\_2017\_Education at a Glance OECD Indicators is the authoritative source for information on the state of education around the worl.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Denmark_2017_Education%20at%20a%20Glance%20OECD%20Indicators%20is%20the%20authoritative%20source%20for%20information%20on%20the%20state%20of%20education%20around%20the%20worl.pdf}
}
@article{undefined, 
  author = {}, 
  title = {{David Albouy\_1994\_The Difference in Difference Estimator.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/David%20Albouy_1994_The%20Difference%20in%20Difference%20Estimator.pdf}
}
@article{10.1007/3-540-70659-3_2, 
  author = {Dietterich, Thomas G.}, 
  title = {{Structural, Syntactic, and Statistical Pattern Recognition, Joint IAPR International Workshops SSPR 2002 and SPR 2002 Windsor, Ontario, Canada, August 6–9, 2002 Proceedings}}, 
  doi = {10.1007/3-540-70659-3\_2}, 
  abstract = {{Statistical learning problems in many fields involve sequential data. This paper formalizes the principal learning tasks and describes the methods that have been developed within the machine learning research community for addressing these problems. These methods include sliding window methods, recurrent sliding windows, hidden Markov models, conditional random fields, and graph transformer networks. The paper also discusses some open research issues.}}, 
  pages = {15--30}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Dietterich%20-%202002%20-%20Machine%20Learning%20for%20Sequential%20Data%20A%20Review.pdf}, 
  year = {2002}
}
@article{undefined, 
  author = {}, 
  title = {{Duchi, Hazan, Singer\_2011\_Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Duchi,%20Hazan,%20Singer_2011_Adaptive%20Subgradient%20Methods%20for%20Online%20Learning%20and%20Stochastic%20Optimization.pdf}
}
@article{undefined, 
  author = {}, 
  title = {{Du, Pandey, Xing\_2017\_Modeling approaches for time series forecasting and anomaly detection.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Du,%20Pandey,%20Xing_2017_Modeling%20approaches%20for%20time%20series%20forecasting%20and%20anomaly%20detection.pdf}
}
@article{undefined, 
  author = {}, 
  title = {{Dziechciarz\_2015\_Measurement of Rate of Return in Education. Research Directions.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Dziechciarz_2015_Measurement%20of%20Rate%20of%20Return%20in%20Education.%20Research%20Directions.pdf}
}
@article{10.1007/s11063-014-9342-0, 
  author = {Egrioglu, Erol and Yolcu, Ufuk and Aladag, Cagdas Hakan and Bas, Eren}, 
  title = {{Recurrent Multiplicative Neuron Model Artificial Neural Network for Non-linear Time Series Forecasting}}, 
  issn = {1370-4621}, 
  doi = {10.1007/s11063-014-9342-0}, 
  abstract = {{Artificial neural networks (ANN) have been widely used in recent years to model non-linear time series since ANN approach is a responsive method and does not require some assumptions such as normality or linearity. An important problem with using ANN for time series forecasting is to determine the number of neurons in hidden layer. There have been some approaches in the literature to deal with the problem of determining the number of neurons in hidden layer. A new ANN model was suggested which is called multiplicative neuron model (MNM) in the literature. MNM has only one neuron in hidden layer. Therefore, the problem of determining the number of neurons in hidden layer is automatically solved when MNM is employed. Also, MNM can produce accurate forecasts for non-linear time series. ANN models utilized for non-linear time series have generally autoregressive structures since lagged variables of time series are generally inputs of these models. On the other hand, it is a well-known fact that better forecasts for real life time series can be obtained from models whose inputs are lagged variables of error. In this study, a new recurrent multiplicative neuron neural network model is firstly proposed. In the proposed method, lagged variables of error are included in the model. Also, the problem of determining the number of neurons in hidden layer is avoided when the proposed method is used. To train the proposed neural network model, particle swarm optimization algorithm was used. To evaluate the performance of the proposed model, it was applied to a real life time series. Then, results produced by the proposed method were compared to those obtained from other methods. It was observed that the proposed method has superior performance to existing methods.}}, 
  pages = {249--258}, 
  number = {2}, 
  volume = {41}, 
  journal = {Neural Processing Letters}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Egrioglu%20et%20al._2015_Recurrent%20Multiplicative%20Neuron%20Model%20Artificial%20Neural%20Network%20for%20Non-linear%20Time%20Series%20Forecasting.pdf}, 
  year = {2015}
}
@article{undefined, 
  author = {}, 
  title = {{Egerton\_1973\_The University of Chicago Press.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Egerton_1973_The%20University%20of%20Chicago%20Press.pdf}
}
@article{undefined, 
  author = {}, 
  title = {{Emrah Önder - 2013 - Forecasting Macroeconomic Variables using Artificial Neural Network and Traditional Smoothing Techniques 4.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Emrah%20Önder%20-%202013%20-%20Forecasting%20Macroeconomic%20Variables%20using%20Artificial%20Neural%20Network%20and%20Traditional%20Smoothing%20Techniques%204.pdf}
}
@article{undefined, 
  author = {}, 
  title = {{Ensor, Ostdiek, Turnbull\_2019\_Dynamic Jump Intensities and News Arrival in Oil Futures Markets.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Ensor,%20Ostdiek,%20Turnbull_2019_Dynamic%20Jump%20Intensities%20and%20News%20Arrival%20in%20Oil%20Futures%20Markets.pdf}
}
@article{10.1016/s2212-5671(15)01619-6, 
  author = {Falat, Lukas and Pancikova, Lucia}, 
  title = {{Quantitative Modelling in Economics with Advanced Artificial Neural Networks}}, 
  issn = {2212-5671}, 
  doi = {10.1016/s2212-5671(15)01619-6}, 
  abstract = {{In this paper, authors present a new approach in forecasting economic time series - application of artificial neural networks. Authors apply feed forward artificial neural network of the RBF type into the process of forecasting the financial data. Except for the standard RBF, authors also test their own new versions of this neural network combined with other techniques of the ML. These models represent new and more advanced version of the standard neural network. Authors add the evolutionary approach into the ANN and also combine the standard algorithm for adapting weights of the ANN with an unsupervised clustering algorithm called K-means. Finally, all of these methods are compared and contrasted with standard (statistical) approach on real economic data to show the potential of using artificial neural network in modelling economic variables.}}, 
  pages = {194--201}, 
  volume = {34}, 
  journal = {Procedia Economics and Finance}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Falat,%20Pancikova_2015_Quantitative%20Modelling%20in%20Economics%20with%20Advanced%20Artificial%20Neural%20Networks.pdf}, 
  year = {2015}
}
@article{undefined, 
  author = {}, 
  title = {{Fair, Shiller\_1990\_Comparing Information in Forecasts from Econometric Models.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Fair,%20Shiller_1990_Comparing%20Information%20in%20Forecasts%20from%20Econometric%20Models.pdf}
}
@article{10.1016/j.econmod.2014.03.024, 
  author = {Feng, Lihua and Zhang, Jianzhen}, 
  title = {{Application of artificial neural networks in tendency forecasting of economic growth}}, 
  issn = {0264-9993}, 
  doi = {10.1016/j.econmod.2014.03.024}, 
  abstract = {{Economic growth results from the synthesis influence of various known or unknown and certain or uncertain factors. Mapping of the stimuli effects and the input and output estimates of artificial neural networks (ANN) are obtained via combinations of nonlinear functions. This approach offers the advantages of self-learning, self-organization, self-adaptation, and fault tolerance, as well as the potential for use in forecasting applications of economic growth. Furthermore, the ANN technology allows the use of multiple variables in both the input and output layers. This capability is very important for economic growth calculations because economic development is often a function of many influential variables. Herein, a forecasting system of economic growth with related application has been proposed, based on ANN. The results show that the Zhejiang proportions of tertiary industry in China for 1995, 1996, and 1997 were 32.305\%, 32.174\%, and 32.114\%, respectively, and the comparative errors eann were 0.64\%, −0.08\%, and −0.27\%, respectively, indicating that the forecast result of ANN was better than that of the GM(1.1) model. This method offered better performance and efficiency.}}, 
  pages = {76--80}, 
  volume = {40}, 
  journal = {Economic Modelling}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Feng,%20Zhang_2014_Application%20of%20artificial%20neural%20networks%20in%20tendency%20forecasting%20of%20economic%20growth.pdf}, 
  year = {2014}
}
@article{10.1023/a:1012074215150, 
  author = {Frank, R. J. and Davey, N. and Hunt, S. P.}, 
  title = {{Time Series Prediction and Neural Networks}}, 
  issn = {0921-0296}, 
  doi = {10.1023/a:1012074215150}, 
  abstract = {{Neural Network approaches to time series prediction are briefly discussed, and the need to find the appropriate sample rate and an appropriately sized input window identified. Relevant theoretical results from dynamic systems theory are briefly introduced, and heuristics for finding the appropriate sampling rate and embedding dimension, and thence window size, are discussed. The method is applied to several time series and the resulting generalisation performance of the trained feed-forward neural network predictors is analysed. It is shown that the heuristics can provide useful information in defining the appropriate network architecture.}}, 
  pages = {91--103}, 
  number = {1-3}, 
  volume = {31}, 
  journal = {Journal of Intelligent and Robotic Systems}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Frank,%20Davey,%20Hunt_2001_Time%20series%20prediction%20and%20neural%20networks.pdf}, 
  year = {2001}
}
@article{10.1016/j.neucom.2015.03.100, 
  author = {Galeshchuk, Svitlana}, 
  title = {{Neural networks performance in exchange rate prediction}}, 
  issn = {0925-2312}, 
  doi = {10.1016/j.neucom.2015.03.100}, 
  abstract = {{Exploration of ANNs for the economic purposes is described and empirically examined with the foreign exchange market data. For the experiments, panel data of the exchange rates (USD/EUR, JPN/USD, USD/GBP) are examined and optimized to be used for time-series predictions with neural networks. In this stage the input selection, in which the processing steps to prepare the raw data to a suitable input for the models are investigated. The best neural network is found with the best forecasting abilities, based on a certain performance measure. A visual graphs on the experiments data set is presented after processing steps, to illustrate that particular results. The out-of-sample results are compared with training ones.}}, 
  pages = {446--452}, 
  volume = {172}, 
  journal = {Neurocomputing}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Galeshchuk_2016_Neural%20networks%20performance%20in%20exchange%20rate%20prediction.pdf}, 
  year = {2016}
}
@article{undefined, 
  author = {Gamboa, John Cristian Borges}, 
  title = {{Deep Learning for Time-Series Analysis}}, 
  eprint = {1701.01887}, 
  abstract = {{In many real-world application, e.g., speech recognition or sleep stage classification, data are captured over the course of time, constituting a Time-Series. Time-Series often contain temporal dependencies that cause two otherwise identical points of time to belong to different classes or predict different behavior. This characteristic generally increases the difficulty of analysing them. Existing techniques often depended on hand-crafted features that were expensive to create and required expert knowledge of the field. With the advent of Deep Learning new models of unsupervised learning of features for Time-series analysis and forecast have been developed. Such new developments are the topic of this paper: a review of the main Deep Learning techniques is presented, and some applications on Time-Series analysis are summaried. The results make it clear that Deep Learning has a lot to contribute to the field.}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Gamboa%20-%202017%20-%20Deep%20Learning%20for%20Time-Series%20Analysis.pdf}, 
  year = {2017}
}
@article{10.1007/978-3-642-24797-2, 
  author = {Graves, Alex}, 
  title = {{Supervised Sequence Labelling with Recurrent Neural Networks}}, 
  doi = {10.1007/978-3-642-24797-2}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Gao,%20Chen%20-%202018%20-%20Spectrum%20sensing%20exploiting%20the%20maximum%20value%20of%20power%20spectrum%20density%20in%20wireless%20sensor%20network.pdf}, 
  year = {2012}
}
@article{undefined, 
  author = {}, 
  title = {{Geoff, Ryan\_2012\_Lecture 16 October 18 Review on duality.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Geoff,%20Ryan_2012_Lecture%2016%20October%2018%20Review%20on%20duality.pdf}
}
@article{10.2139/ssrn.1667442, 
  author = {Giovanis, Eleftherios}, 
  title = {{Applications of Neural Network Radial Basis Function in Economics and Financial Time Series}}, 
  doi = {10.2139/ssrn.1667442}, 
  abstract = {{In this paper we present the Radial Basis Neural Network Function. We examine some simple numerical examples of time-series in economics and finance. The forecasting performance is significant superior, especially in financial time-series, to traditional econometric modeling indicating that artificial intelligence procedure are more appropriate. Some MATLAB routines are presented for further application research.}}, 
  journal = {SSRN Electronic Journal}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Giovanis_Unknown_Applications%20of%20Neural%20Network%20Radial%20Basis%20Function%20in%20Economics%20and%20Financial%20Time%20Series.pdf}, 
  year = {2010}
}
@article{10.1109/tnnls.2017.2709324, 
  author = {Godfrey, Luke B and Gashler, Michael S}, 
  title = {{Neural Decomposition of Time-Series Data for Effective Generalization}}, 
  doi = {10.1109/tnnls.2017.2709324}, 
  pmid = {28650827}, 
  eprint = {1705.09137}, 
  abstract = {{We present a neural network technique for the analysis and extrapolation of time-series data called Neural Decomposition (ND). Units with a sinusoidal activation function are used to perform a Fourier-like decomposition of training samples into a sum of sinusoids, augmented by units with nonperiodic activation functions to capture linear trends and other nonperiodic components. We show how careful weight initialization can be combined with regularization to form a simple model that generalizes well. Our method generalizes effectively on the Mackey-Glass series, a dataset of unemployment rates as reported by the U.S. Department of Labor Statistics, a time-series of monthly international airline passengers, the monthly ozone concentration in downtown Los Angeles, and an unevenly sampled time-series of oxygen isotope measurements from a cave in north India. We find that ND outperforms popular time-series forecasting techniques including LSTM, echo state networks, ARIMA, SARIMA, SVR with a radial basis function, and Gashler and Ashmore's model.}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Godfrey,%20Gashler_2018_Neural%20decomposition%20of%20time-series%20data%20for%20effective%20generalization.pdf}, 
  year = {2017}
}
@article{10.1177/0891242403256447, 
  author = {Goetz, Stephan J. and Rupasingha, Anil}, 
  title = {{The Returns on Higher Education: Estimates for the 48 Contiguous States}}, 
  issn = {0891-2424}, 
  doi = {10.1177/0891242403256447}, 
  abstract = {{Governors of a number of states are seeking to expand the proportion of workers with college degrees so that their economies do not fall behind in the new economy. How- ever, because of pronounced differences in the compositions and levels of development of state economies, returns on higher education are far from equal across the states. We estimate the average return on a college degree in each state in terms of its effect on per capita income, holding constant other factors that also influence levels of per capita income.}}, 
  pages = {337--351}, 
  number = {4}, 
  volume = {17}, 
  journal = {Economic Development Quarterly}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Goetz,%20Rupasingha_2003_The%20returns%20on%20higher%20education%20Estimates%20for%20the%2048%20contiguous%20states.pdf}, 
  year = {2003}
}
@article{undefined, 
  author = {}, 
  title = {{Gronau\_1974\_Wage Comparisons--A Selectivity Bias.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Gronau_1974_Wage%20Comparisons--A%20Selectivity%20Bias.pdf}
}
@article{10.1016/j.eswa.2011.02.068, 
  author = {Guresen, Erkam and Kayakutlu, Gulgun and Daim, Tugrul U.}, 
  title = {{Using artificial neural network models in stock market index prediction}}, 
  issn = {0957-4174}, 
  doi = {10.1016/j.eswa.2011.02.068}, 
  abstract = {{Forecasting stock exchange rates is an important financial problem that is receiving increasing attention. During the last few years, a number of neural network models and hybrid models have been proposed for obtaining accurate prediction results, in an attempt to outperform the traditional linear and nonlinear approaches. This paper evaluates the effectiveness of neural network models which are known to be dynamic and effective in stock-market predictions. The models analysed are multi-layer perceptron (MLP), dynamic artificial neural network (DAN2) and the hybrid neural networks which use generalized autoregressive conditional heteroscedasticity (GARCH) to extract new input variables. The comparison for each model is done in two view points: Mean Square Error (MSE) and Mean Absolute Deviate (MAD) using real exchange daily rate values of NASDAQ Stock Exchange index.}}, 
  pages = {10389--10397}, 
  number = {8}, 
  volume = {38}, 
  journal = {Expert Systems with Applications}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Guresen,%20Kayakutlu,%20Daim_2011_Using%20artificial%20neural%20network%20models%20in%20stock%20market%20index%20prediction.pdf}, 
  year = {2011}
}
@article{10.1016/j.econedurev.2004.07.013, 
  author = {Hamermesh, Daniel S. and Parker, Amy}, 
  title = {{Beauty in the classroom: instructors’ pulchritude and putative pedagogical productivity}}, 
  issn = {0272-7757}, 
  doi = {10.1016/j.econedurev.2004.07.013}, 
  abstract = {{Adjusted for many other determinants, beauty affects earnings; but does it lead directly to the differences in productivity that we believe generate earnings differences? We take a large sample of student instructional ratings for a group of university teachers and acquire six independent measures of their beauty, and a number of other descriptors of them and their classes. Instructors who are viewed as better looking receive higher instructional ratings, with the impact of a move from the 10th to the 90th percentile of beauty being substantial. This impact exists within university departments and even within particular courses, and is larger for male than for female instructors. Disentangling whether this outcome represents productivity or discrimination is, as with the issue generally, probably impossible.}}, 
  pages = {369--376}, 
  number = {4}, 
  volume = {24}, 
  journal = {Economics of Education Review}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Hamermesh,%20Parker_2005_Beauty%20in%20the%20classroom%20Instructors'%20pulchritude%20and%20putative%20pedagogical%20productivity.pdf}, 
  year = {2005}
}
@article{10.1134/s0005117913090129, 
  author = {Gusev, K. Yu. and Burkovskii, V. L.}, 
  title = {{A neural network forecasting model for integrated economic indicators}}, 
  issn = {0005-1179}, 
  doi = {10.1134/s0005117913090129}, 
  abstract = {{This paper considers forecasting models for integrated economic indicators. The first model is implemented as a fuzzy system, and the second one represents a fuzzy-neural module. Finally, we study the possibility of using the fuzzy system as a service tool in the fuzzy-neural module.}}, 
  pages = {1567--1572}, 
  number = {9}, 
  volume = {74}, 
  journal = {Automation and Remote Control}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Gusev,%20Burkovskii_2013_A%20neural%20network%20forecasting%20model%20for%20integrated%20economic%20indicators.pdf}, 
  year = {2013}
}
@article{undefined, 
  author = {}, 
  title = {{Han et al.\_2009\_Route Choice under Uncertainty.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Han%20et%20al._2009_Route%20Choice%20under%20Uncertainty.pdf}
}
@article{undefined, 
  author = {}, 
  title = {{Hamilton\_2009\_Understanding Crude Oil Prices(2).pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Hamilton_2009_Understanding%20Crude%20Oil%20Prices(2).pdf}
}
@article{10.1111/1467-6419.00191, 
  author = {Harmon, Colm and Oosterbeek, Hessel and Walker, Ian}, 
  title = {{The Returns to Education: Microeconomics}}, 
  issn = {1467-6419}, 
  doi = {10.1111/1467-6419.00191}, 
  abstract = {{In this paper we focus on education as a private decision to invest in “human capital” and the estimation of the rate of return to that private investment. While the literature is replete with studies that estimate the rate of return using regression methods where the estimated return is obtained as the coefficient on a years of education variable in a log wage equation that contains controls for work experience and other individual characteristics, the issue is surrounded with difficulties. We outline the theoretical arguments underpinning the empirical developments and show that the evidence on private returns to the individual is compelling. Despite some of these issues surrounding the estimation of the return to schooling, our evidence, based on estimates from a variety of datasets and specifications, is that there is an unambiguously positive effect on the earnings of an individual from participation in education. Moreover, the size of the effect seems large relative to the returns on other investments.}}, 
  pages = {115--156}, 
  number = {2}, 
  volume = {17}, 
  journal = {Journal of Economic Surveys}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Harmon,%20Walker_2003_The%20returns%20to%20education%20Microeconomics.pdf}, 
  year = {2003}
}
@article{undefined, 
  author = {Hartford, Jason and Lewis, Greg and Leyton-Brown, Kevin and Taddy, Matt}, 
  title = {{Counterfactual Prediction with Deep Instrumental Variables Networks}}, 
  eprint = {1612.09596}, 
  abstract = {{We are in the middle of a remarkable rise in the use and capability of artificial intelligence. Much of this growth has been fueled by the success of deep learning architectures: models that map from observables to outputs via multiple layers of latent representations. These deep learning algorithms are effective tools for unstructured prediction, and they can be combined in AI systems to solve complex automated reasoning problems. This paper provides a recipe for combining ML algorithms to solve for causal effects in the presence of instrumental variables -- sources of treatment randomization that are conditionally independent from the response. We show that a flexible IV specification resolves into two prediction tasks that can be solved with deep neural nets: a first-stage network for treatment prediction and a second-stage network whose loss function involves integration over the conditional treatment distribution. This Deep IV framework imposes some specific structure on the stochastic gradient descent routine used for training, but it is general enough that we can take advantage of off-the-shelf ML capabilities and avoid extensive algorithm customization. We outline how to obtain out-of-sample causal validation in order to avoid over-fit. We also introduce schemes for both Bayesian and frequentist inference: the former via a novel adaptation of dropout training, and the latter via a data splitting routine.}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Hartford%20et%20al._2016_Counterfactual%20Prediction%20with%20Deep%20Instrumental%20Variables%20Networks.pdf}, 
  year = {2016}
}
@article{undefined, 
  author = {}, 
  title = {{Heckman - 1979 - Sample Selection Bias as a Specification Error.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Heckman%20-%201979%20-%20Sample%20Selection%20Bias%20as%20a%20Specification%20Error.pdf}
}
@article{undefined, 
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian}, 
  title = {{Deep Residual Learning for Image Recognition}}, 
  eprint = {1512.03385}, 
  abstract = {{Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/He%20et%20al._2015_Deep%20Residual%20Learning%20for%20Image%20Recognition.pdf}, 
  year = {2015}
}
@article{undefined, 
  author = {}, 
  title = {{Heckman et al.\_2003\_NBER WORKING PAPER SERIES FIFTY YEARS OF MINCER EARNINGS REGRESSIONS Fifty Years of Mincer Earnings Regressions.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Heckman%20et%20al._2003_NBER%20WORKING%20PAPER%20SERIES%20FIFTY%20YEARS%20OF%20MINCER%20EARNINGS%20REGRESSIONS%20Fifty%20Years%20of%20Mincer%20Earnings%20Regressions.pdf}
}
@article{10.1016/j.econedurev.2011.05.002, 
  author = {Henderson, Daniel J. and Polachek, Solomon W. and Wang, Le}, 
  title = {{Heterogeneity in schooling rates of return}}, 
  issn = {0272-7757}, 
  doi = {10.1016/j.econedurev.2011.05.002}, 
  abstract = {{This paper relaxes the assumption of homogeneous rates of return to schooling by employing nonparametric kernel regression. This approach allows us to examine the differences in rates of return to education both across and within groups. Similar to previous studies we find that on average blacks have higher returns to education than whites, natives have higher returns than immigrants and younger workers have higher returns than older workers. Contrary to previous studies we find that the average gap of the rate of return between white and black workers is larger than previously thought and the gap is smaller between immigrants and natives. We also uncover significant heterogeneity, the extent of which differs both across and within groups. Finally, we uncover the characteristics common amongst those with the smallest and largest returns to education.}}, 
  pages = {1202--1214}, 
  number = {6}, 
  volume = {30}, 
  journal = {Economics of Education Review}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Henderson,%20Polachek,%20Wang_2011_Heterogeneity%20in%20schooling%20rates%20of%20return.pdf}, 
  year = {2011}
}
@article{undefined, 
  author = {}, 
  title = {{Heckman\_1979\_Sample Selection Bias as a Specification Error.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Heckman_1979_Sample%20Selection%20Bias%20as%20a%20Specification%20Error.pdf}
}
@article{undefined, 
  author = {}, 
  title = {{Heckman, Tobias, Vytlacil\_2006\_Four Parameters of Interest in the Evaluation of Social Programs.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Heckman,%20Tobias,%20Vytlacil_2006_Four%20Parameters%20of%20Interest%20in%20the%20Evaluation%20of%20Social%20Programs.pdf}
}
@article{undefined, 
  author = {Hinton, Geoffrey E and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R}, 
  title = {{Improving neural networks by preventing co-adaptation of feature detectors}}, 
  eprint = {1207.0580}, 
  abstract = {{When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This "overfitting" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random "dropout" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Hinton%20et%20al._2012_Improving%20neural%20networks%20by%20preventing%20co-adaptation%20of%20feature%20detectors.pdf}, 
  year = {2012}
}
@article{10.1016/s0360-8352(02)00036-0, 
  author = {Ho, S.L and Xie, M and Goh, T.N}, 
  title = {{A comparative study of neural network and Box-Jenkins ARIMA modeling in time series prediction}}, 
  issn = {0360-8352}, 
  doi = {10.1016/s0360-8352(02)00036-0}, 
  abstract = {{This paper aims to investigate suitable time series models for repairable system failure analysis. A comparative study of the Box-Jenkins autoregressive integrated moving average (ARIMA) models and the artificial neural network models in predicting failures are carried out. The neural network architectures evaluated are the multi-layer feed-forward network and the recurrent network. Simulation results on a set of compressor failures showed that in modeling the stochastic nature of reliability data, both the ARIMA and the recurrent neural network (RNN) models outperform the feed-forward model; in terms of lower predictive errors and higher percentage of correct reversal detection. However, both models perform better with short term forecasting. The effect of varying the damped feedback weights in the recurrent net is also investigated and it was found that RNN at the optimal weighting factor gives satisfactory performances compared to the ARIMA model.}}, 
  pages = {371--375}, 
  number = {2-4}, 
  volume = {42}, 
  journal = {Computers \& Industrial Engineering}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Ho,%20Xie,%20Goh_2002_A%20comparative%20study%20of%20NN%20and%20BoxJenkins%20ARIMA%20modeling%20in%20time%20series%20prediction.pdf.pdf}, 
  year = {2002}
}
@article{undefined, 
  author = {}, 
  title = {{Hochreiter, Schmidhuber\_1997\_Long short term memory.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Hochreiter,%20Schmidhuber_1997_Long%20short%20term%20memory.pdf}
}
@article{undefined, 
  author = {}, 
  title = {{Hoang\_2013\_The Box-Jenkins Methodology for Time Series Models.pdf}}, 
  local-url = {file://localhost/Users/tianyudu/Documents/Mendeley%20Desktop/Hoang_2013_The%20Box-Jenkins%20Methodology%20for%20Time%20Series%20Models.pdf}
}